
@inproceedings{sarkar_intelligent_2020,
	location = {Marina del Rey, {CA}, {USA}},
	title = {An Intelligent Framework for Prediction of a {UAV}’s Flight Time},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72814-351-4},
	url = {https://ieeexplore.ieee.org/document/9183625/},
	doi = {10.1109/DCOSS49796.2020.00058},
	abstract = {The success of an unmanned aerial vehicle’s ({UAVs}) or drone’s mission is contingent upon planning, command, control, tasking, and communications. Drone-mounted payloads impact {UAV} ﬂight time. Depending upon the payload, ﬂight times may vary. As such, it is important to know beforehand the expected ﬂight time of a drone, in order to ensure a successful ﬂight. Currently, there are no methods or algorithms for calculating or predicting a drone’s ﬂight time that takes into account varying payloads. Manufacturers’ technical manuals usually provide only the best-case predictions of {UAV} ﬂight time; these assume no additional mounted items (e.g., onboard wireless sensor modules, cameras, etc.). In this paper, we describe an empirical study of {UAV} ﬂight events, and propose regression and deep learning ({DL})-based methods to predict accurately the ﬂight time of {UAVs}. Our methods take onto account both the payload weight and energy dissipation from the onboard battery. The payloads used in our study include gimbal-mounted {RGB}/thermal cameras, as well as onboard computers. It is expected that this work will provide important guidance to researchers and planners of {UAVs}.},
	eventtitle = {2020 16th International Conference on Distributed Computing in Sensor Systems ({DCOSS})},
	pages = {328--332},
	booktitle = {2020 16th International Conference on Distributed Computing in Sensor Systems ({DCOSS})},
	publisher = {{IEEE}},
	author = {Sarkar, Sayani and Totaro, Michael W. and Kumar, Ashok},
	urldate = {2024-05-04},
	date = {2020-05},
	langid = {english},
}

@article{tsai_auto-annotated_2021,
	title = {Auto-Annotated Deep Segmentation for Surface Defect Detection},
	volume = {70},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9456, 1557-9662},
	url = {https://ieeexplore.ieee.org/document/9449912/},
	doi = {10.1109/TIM.2021.3087826},
	abstract = {This article presents a deep learning scheme for automatic defect detection in material surfaces. The success of deep learning model training is generally determined by the number of representative training samples and the quality of the annotation. It is extremely tedious and tiresome to annotate defects pixel-by-pixel in an image to train a semantic network model for defect segmentation. In this study, we propose a two-stage deep learning scheme to tackle the pixel-wise defect detection in textured surfaces without manual annotation. The ﬁrst stage of the deep learning scheme uses two cycle-consistent adversarial network ({CycleGAN}) models to automatically synthesize and annotate defect pixels in an image. The synthesized defect images and their corresponding annotated results from the {CycleGAN} models are then used as the input–output pairs for training the U-Net semantic network. The proposed scheme requires only a few real defect samples for the training and completely requires no manual annotation work. It is practical and computationally very efﬁcient for the implementation in manufacturing. Experimental results show that the proposed deep learning scheme can be applied for defect detection in a variety of textured and patterned surfaces, and results in high detection accuracy.},
	pages = {1--10},
	journaltitle = {{IEEE} Transactions on Instrumentation and Measurement},
	shortjournal = {{IEEE} Trans. Instrum. Meas.},
	author = {Tsai, Du-Ming and Fan, Shu-Kai S. and Chou, Yi-Hsiang},
	urldate = {2024-05-02},
	date = {2021},
	langid = {english},
}

@book{ruz_image_2005,
	title = {Image segmentation using fuzzy min-max neural networks for wood defect detection},
	abstract = {In this work a colour image segmentation method for wood surface defect detection is presented. In an automated visual inspection system for wood boards, the image segmentation task aims to obtain a high defect detection rate with a low false positive rate, i.e., clear wood areas identified as defect regions. The proposed method is called {FMMIS} (Fuzzy Min-Max neural network for Image Segmentation). The {FMMIS} method grows boxes from a set of seed pixels, yielding the minimum bounded rectangle ({MBR}) for each defect present in the wood board image. The {FMMIS} method was applied to a set of 900 colour images of radiata pine boards, which included 10 defect categories. The {FMMIS} achieved a defect detection rate of 95
percent on the test set, with only 6 percent of false positives. The area recognition rate ({ARR}) criterion was computed, to measure the segmentation quality, using as a reference the manually placed {MBR} for each defect. The {ARR} achieved 94.4 percent on the test set. The results show significant improvements compared with previous work and that the computational load of {FMMIS} is suitable for real-time segmentation tasks.},
	author = {Ruz, Gonzalo and Estevez, Pablo},
	date = {2005-07-04},
}

@article{funck_image_2003,
	title = {Image segmentation algorithms applied to wood defect detection},
	volume = {41},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169903000498},
	doi = {10.1016/S0168-1699(03)00049-8},
	pages = {157--179},
	number = {1},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Computers and Electronics in Agriculture},
	author = {Funck, J.W and Zhong, Y and Butler, D.A and Brunner, C.C and Forrer, J.B},
	urldate = {2024-05-02},
	date = {2003-12},
	langid = {english},
}

@article{conners_identifying_1983,
	title = {Identifying and Locating Surface Defects in Wood: Part of an Automated Lumber Processing System},
	volume = {{PAMI}-5},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4767446/},
	doi = {10.1109/TPAMI.1983.4767446},
	shorttitle = {Identifying and Locating Surface Defects in Wood},
	abstract = {Continued increases in the cost of materials and labor make it imperative for furniture manufacturers to control costs by improved yield and increased productivity. This paper describes an Automated Lumber Processing System ({ALPS}) that employs computer tomography, optical scanning technology, the calculation of an optimum cutting strategy, and a computer-driven laser cutting device. While certain major hardware components of {ALPS} are already commercially available, a major missing element is the automatic inspection system needed to locate and identify surface defects on boards. This paper reports research aimed at developing such an inspection system. The basic strategy is to divide the digital image of a board into a number of disjoint rectangular regions and classify each independently. This simple procedure has the advantage of allowing an obvious parallel processing implementation. The study shows that measures of tonal and pattern related qualities are needed. The tonal measures are the mean, variance, skewness, and kurtosis of the gray levels. The pattern related measures are those based on cooccurrence matrices. In this initial feasibility study, these combined measures yielded an overall 88.3 percent correct classification on the eight defects most commonly found in lumber. To minimize the number of calculations needed to make the required classifications a sequential classifier is proposed.},
	pages = {573--583},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Conners, Richard W. and Mcmillin, Charles W. and Lin, Kingyao and Vasquez-Espinosa, Ramon E.},
	urldate = {2024-05-02},
	date = {1983-11},
	langid = {english},
}

@article{zhu_multi-source_2024,
	title = {A Multi-Source Data Fusion Network for Wood Surface Broken Defect Segmentation},
	volume = {24},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/5/1635},
	doi = {10.3390/s24051635},
	abstract = {Wood surface broken defects seriously damage the structure of wooden products, these defects have to be detected and eliminated. However, current defect detection methods based on machine vision have difficulty distinguishing the interference, similar to the broken defects, such as stains and mineral lines, and can result in frequent false detections. To address this issue, a multi-source data fusion network based on U-Net is proposed for wood broken defect detection, combining image and depth data, to suppress the interference and achieve complete segmentation of the defects. To efficiently extract various semantic information of defects, an improved {ResNet}34 is designed to, respectively, generate multi-level features of the image and depth data, in which the depthwise separable convolution ({DSC}) and dilated convolution ({DC}) are introduced to decrease the computational expense and feature redundancy. To take full advantages of two types of data, an adaptive interacting fusion module ({AIF}) is designed to adaptively integrate them, thereby generating accurate feature representation of the broken defects. The experiments demonstrate that the multisource data fusion network can effectively improve the detection accuracy of wood broken defects and reduce the false detections of interference, such as stains and mineral lines.},
	pages = {1635},
	number = {5},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Zhu, Yuhang and Xu, Zhezhuang and Lin, Ye and Chen, Dan and Ai, Zhijie and Zhang, Hongchuan},
	urldate = {2024-05-02},
	date = {2024-03-02},
	langid = {english},
}

@incollection{hutchison_automatic_2012,
	location = {Berlin, Heidelberg},
	title = {Automatic Segmentation of Wood Logs by Combining Detection and Segmentation},
	volume = {7431},
	isbn = {978-3-642-33178-7 978-3-642-33179-4},
	url = {http://link.springer.com/10.1007/978-3-642-33179-4_25},
	abstract = {The segmentation of cut surfaces from a stack of wood logs is a challenging task and leads to many problems. Wood logs theoretically have a certain shape and color, which is the main reason to apply object detection methods. But in real world images there are many disturbing factors, such as defects, dirt or non-elliptical logs. In this paper we mainly address the problem of wood and wood log segmentation by combining object detection with a graph-cut segmentation. We introduce an iterative segmentation procedure, which detects the stack of wood, segments foreground and background, and separates the logs. Our novel approach works fully automatically and has no restrictions on the image acquisition other than well visible log cut surfaces. All three steps of our approach are novel and could be applied on similar problems. We implemented and evaluated diﬀerent methods and show that of these approaches, our methods leads to the best results.},
	pages = {252--261},
	booktitle = {Advances in Visual Computing},
	publisher = {Springer Berlin Heidelberg},
	author = {Gutzeit, Enrico and Voskamp, Jörg},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Fowlkes, Charless and Wang, Sen and Choi, Min-Hyung and Mantler, Stephan and Schulze, Jürgen and Acevedo, Daniel and Mueller, Klaus and Papka, Michael},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2024-05-02},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-33179-4_25},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{chang_novel_2018,
	title = {A novel image segmentation approach for wood plate surface defect classification through convex optimization},
	volume = {29},
	issn = {1007-662X, 1993-0607},
	url = {http://link.springer.com/10.1007/s11676-017-0572-7},
	doi = {10.1007/s11676-017-0572-7},
	pages = {1789--1795},
	number = {6},
	journaltitle = {Journal of Forestry Research},
	shortjournal = {J. For. Res.},
	author = {Chang, Zhanyuan and Cao, Jun and Zhang, Yizhuo},
	urldate = {2024-05-02},
	date = {2018-11},
	langid = {english},
}

@article{noauthor_linux-grundlagenfur_nodate,
	title = {Linux-Grundlagen//für Anwender und Administratoren},
	langid = {german},
}

@misc{cui_adaptive_2023,
	title = {Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection},
	url = {http://arxiv.org/abs/2308.05426},
	abstract = {Foundation models, such as {OpenAI}’s {GPT}-3 and {GPT}-4, Meta’s {LLaMA}, and Google’s {PaLM}2, have revolutionized the ﬁeld of artiﬁcial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model ({SAM}), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although {SAM} excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model ({SSOM}), an innovative approach that adaptively ﬁne-tunes {SAM} for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across ﬁve challenging {RGB} benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.},
	number = {{arXiv}:2308.05426},
	publisher = {{arXiv}},
	author = {Cui, Ruikai and He, Siyuan and Qiu, Shi},
	urldate = {2024-04-29},
	date = {2023-08-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.05426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_segment_2024,
	title = {Segment Anything Model is a Good Teacher for Local Feature Learning},
	url = {http://arxiv.org/abs/2309.16992},
	abstract = {Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Datadriven local feature learning methods need to rely on pixel-level correspondence for training, ignoring the semantic information on which humans rely to describe image pixels. However, it is not feasible to enhance generic scene keypoints detection and description using traditional semantic segmentation models because they can only recognize a limited number of coarse-grained object classes. In this paper, we propose {SAMFeat} to introduce {SAM} (segment anything model), a foundation model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation ({PSRD}), which distillates feature relations with categoryagnostic semantic information learned by the {SAM} encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning Based on Semantic Grouping ({WSC}), which utilizes semantic groupings derived from {SAM} as weakly supervised signals, to optimize the metric space of local descriptors. Third, we design an Edge Attention Guidance ({EAG}) to further improve the accuracy of local feature detection and description by prompting the network to pay more attention to the edge region guided by {SAM}. {SAMFeat}’s performance on various tasks such as image matching on {HPatches}, and long-term visual localization on Aachen Day-Night showcases its superiority over previous local features. The release code is available at https://github.com/vignywang/{SAMFeat}.},
	number = {{arXiv}:2309.16992},
	publisher = {{arXiv}},
	author = {Wu, Jingqian and Xu, Rongtao and Wood-Doughty, Zach and Wang, Changwei and Xu, Shibiao and Lam, Edmund},
	urldate = {2024-04-29},
	date = {2024-03-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.16992 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wang_segment_2023,
	title = {Segment anything also Detect anything},
	abstract = {The field of natural language processing ({NLP}) has been revolutionised by the emergence of large language models ({LLMs}), which have demonstrated impressive capabilities in zero-shot and few-shot tasks, as well as more complex tasks such as mathematical problem-solving and commonsense reasoning, due to their massive corpus and intensive training computation.The emergence of computer vision macromodels ({SAMs}) is also transforming computer vision ({CV}) tasks. In this paper, we propose the use of {SAM} vision macromodels to guide semi-automated annotation of data in the domain of specific object detection. Also focusing on visual image data augmentation, we propose the High Fine Grain Fill-in Augmentation ({HFGFA}) method, which can generate false images with higher fineness and greatly improve data imbalance and small object problems. Through early experimental validation, such an approach can improve model generalisation and model generalisation capabilities. Finally, we focus on open world object detection, where the advent of {SAM} will greatly advance research related to open world object detection.},
	author = {Wang, Rongsheng and Duan, Yaofei and Li, {YuKun}},
	date = {2023-04-11},
	langid = {english},
}

@article{xu_multidimensional_2024,
	title = {Multidimensional Exploration of Segment Anything Model for Weakly Supervised Video Salient Object Detection},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1051-8215, 1558-2205},
	url = {https://ieeexplore.ieee.org/document/10443051/},
	doi = {10.1109/TCSVT.2024.3368053},
	abstract = {Fully supervised video salient object detection ({VSOD}) has made considerable breakthroughs using costly and time-consuming pixel-wise annotations. Recently, to achieve a trade-off between the annotation burden and the model performance, scribble-based {VSOD} tasks have attracted increasing attention. However, learning the complete object structure and precise boundary details from sparse scribble annotations remains challenging. In this paper, we propose a series of strategies to effectively explore valid information from the recently proposed segmentation foundation model “Segment Anything Model ({SAM})” in various perspectives to address these challenges. Specifically, due to the limited performance of {SAM} on videos, we propose a {SAM}-guided label enhancement method instead of directly using the results of {SAM}, which can introduce edge information while reducing the interference of erroneous information. Moreover, we propose a {SAM}-driven spatiotemporal network guided by general semantic features from the {SAM} encoder to help the model be aware of global connections. Additionally, we propose a {SAM}-based global-aware loss, which further considers the affinity constraint between predicted results and foreground labels or background labels from a global perspective, guiding the model to perceive the complete salient objects. Experimental results demonstrate that our method outperforms state-of-the-art weakly supervised {VSOD} methods and is comparable to fully supervised {VSOD} methods.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	shortjournal = {{IEEE} Trans. Circuits Syst. Video Technol.},
	author = {Xu, Binwei and Jiang, Qiuping and Zhao, Xing and Lu, Chenyang and Liang, Haoran and Liang, Ronghua},
	urldate = {2024-04-29},
	date = {2024},
	langid = {english},
}

@inproceedings{chen_sam-adapter_2023,
	location = {Paris, France},
	title = {{SAM}-Adapter: Adapting Segment Anything in Underperformed Scenes},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350307443},
	url = {https://ieeexplore.ieee.org/document/10350709/},
	doi = {10.1109/ICCVW60793.2023.00361},
	shorttitle = {{SAM}-Adapter},
	abstract = {The emergence of large models, also known as foundation models, has brought signiﬁcant advancements to {AI} research. One such model is Segment Anything ({SAM}), which is designed for image segmentation tasks. However, as with other foundation models, our experimental ﬁndings suggest that {SAM} may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouﬂaged object detection (concealed object detection). This study ﬁrst paves the way for applying the large pre-trained image segmentation model {SAM} to these downstream tasks, even in situations where {SAM} performs poorly. Rather than ﬁnetuning the {SAM} network, we propose {SAM}-Adapter, which incorporates domain-speciﬁc information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-speciﬁc knowledge with general knowledge learnt by the large model, {SAM}-Adapter can signiﬁcantly elevate the performance of {SAM} in challenging tasks as shown in extensive experiments. We can even outperform task-speciﬁc network models and achieve stateof-the-art performance in the task we tested: camouﬂaged object detection, shadow detection. Our code of adapting {SAM} in downstream applications have been released publicly at https://github.com/tianrun-chen/{SAM}-{AdapterPyTorch}/ and has beneﬁted many researchers. We believe our work opens up opportunities for utilizing {SAM} in downstream tasks, with potential applications in various ﬁelds, including medical image processing, agriculture, remote sensing, and more.},
	eventtitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision Workshops ({ICCVW})},
	pages = {3359--3367},
	booktitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision Workshops ({ICCVW})},
	publisher = {{IEEE}},
	author = {Chen, Tianrun and Zhu, Lanyun and Ding, Chaotao and Cao, Runlong and Wang, Yan and Zhang, Shangzhan and Li, Zejian and Sun, Lingyun and Zang, Ying and Mao, Papa},
	urldate = {2024-04-29},
	date = {2023-10-02},
	langid = {english},
}

@article{giannakis_flexible_2024,
	title = {A flexible deep learning crater detection scheme using Segment Anything Model ({SAM})},
	volume = {408},
	issn = {00191035},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0019103523003755},
	doi = {10.1016/j.icarus.2023.115797},
	abstract = {Craters are one of the most important morphological features in planetary exploration. To that extent, detecting, mapping and counting craters is a mainstream process in planetary science, done primarily manually, which is a very laborious, time-consuming and inconsistent process. Recently, machine learning ({ML}) and computer vision have been successfully applied for both detecting craters and estimating their size. Existing {ML} models for automated crater detection have been trained in specific types of data e.g. digital elevation model ({DEM}), images and associated metadata from orbiters such as the Lunar Reconnaissance Orbiter Camera ({LROC}) etc. Due to that, each of the resulting {ML} schemes is applicable and reliable only to the type of data used during the training process. Data from different sources, angles and setups can compromise the reliability of these {ML} schemes. In this paper we present a flexible crater detection scheme that is based on the recently proposed Segment Anything Model ({SAM}) from {META} {AI}. {SAM} is a promptable segmentation system with zero-shot generalisation to unfamiliar objects and images without the need for additional training. Using {SAM}, without additional training and fine-tuning, we can successfully identify crater-looking objects in various types of data (e,g, raw satellite images Level-1 and 2 products, {DEMs} etc.) for different setups (e.g. Lunar, Mars) and different capturing angles. Moreover, using shape indexes, we only keep the segmentation masks of crater-like features. These masks are subsequently fitted with a circle or an ellipse, recovering both the location and the size/geometry of the detected craters.},
	pages = {115797},
	journaltitle = {Icarus},
	shortjournal = {Icarus},
	author = {Giannakis, Iraklis and Bhardwaj, Anshuman and Sam, Lydia and Leontidis, Georgios},
	urldate = {2024-04-29},
	date = {2024-01},
	langid = {english},
}

@incollection{jia_detect_2023,
	location = {Singapore},
	title = {Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization},
	volume = {14463},
	isbn = {978-981-9985-64-7 978-981-9985-65-4},
	url = {https://link.springer.com/10.1007/978-981-99-8565-4_18},
	shorttitle = {Detect Any Deepfakes},
	abstract = {The rapid advancements in computer vision have stimulated remarkable progress in face forgery techniques, capturing the dedicated attention of researchers committed to detecting forgeries and precisely localizing manipulated areas. Nonetheless, with limited ﬁnegrained pixel-wise supervision labels, deepfake detection models perform unsatisfactorily on precise forgery detection and localization. To address this challenge, we introduce the well-trained vision segmentation foundation model, i.e., Segment Anything Model ({SAM}) in face forgery detection and localization. Based on {SAM}, we propose the Detect Any Deepfakes ({DADF}) framework with the Multiscale Adapter, which can capture short- and long-range forgery contexts for eﬃcient ﬁne-tuning. Moreover, to better identify forged traces and augment the model’s sensitivity towards forgery regions, Reconstruction Guided Attention ({RGA}) module is proposed. The proposed framework seamlessly integrates endto-end forgery localization and detection optimization. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach for both forgery detection and localization. The codes are available at Link.},
	pages = {180--190},
	booktitle = {Biometric Recognition},
	publisher = {Springer Nature Singapore},
	author = {Lai, Yingxin and Luo, Zhiming and Yu, Zitong},
	editor = {Jia, Wei and Kang, Wenxiong and Pan, Zaiyu and Ben, Xianye and Bian, Zhengfu and Yu, Shiqi and He, Zhaofeng and Wang, Jun},
	urldate = {2024-04-29},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-981-99-8565-4_18},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{yamagiwa_zero-shot_2024,
	location = {Waikoloa, {HI}, {USA}},
	title = {Zero-Shot Edge Detection with {SCESAME}: Spectral Clustering-based Ensemble for Segment Anything Model Estimation},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350370287},
	url = {https://ieeexplore.ieee.org/document/10495706/},
	doi = {10.1109/WACVW60836.2024.00064},
	shorttitle = {Zero-Shot Edge Detection with {SCESAME}},
	abstract = {This paper proposes a novel zero-shot edge detection with {SCESAME}, which stands for Spectral Clusteringbased Ensemble for Segment Anything Model Estimation, based on the recently proposed Segment Anything Model ({SAM}). {SAM} is a foundation model for segmentation tasks, and one of the interesting applications of {SAM} is Automatic Mask Generation ({AMG}), which generates zero-shot segmentation masks of an entire image. {AMG} can be applied to edge detection, but suffers from the problem of overdetecting edges. Edge detection with {SCESAME} overcomes this problem by three steps: (1) eliminating small generated masks, (2) combining masks by spectral clustering, taking into account mask positions and overlaps, and (3) removing artifacts after edge detection. We performed edge detection experiments on two datasets, {BSDS}500 and {NYUDv}2. Although our zero-shot approach is simple, the experimental results on {BSDS}500 showed almost identical performance to human performance and {CNN}-based methods from seven years ago. In the {NYUDv}2 experiments, it performed almost as well as recent {CNN}-based methods. These results indicate that our method effectively enhances the utility of {SAM} and can be a new direction in zero-shot edge detection methods.},
	eventtitle = {2024 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision Workshops ({WACVW})},
	pages = {541--551},
	booktitle = {2024 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision Workshops ({WACVW})},
	publisher = {{IEEE}},
	author = {Yamagiwa, Hiroaki and Takase, Yusuke and Kambe, Hiroyuki and Nakamoto, Ryosuke},
	urldate = {2024-04-29},
	date = {2024-01-01},
	langid = {english},
}

@misc{kirillov_segment_2023,
	title = {Segment Anything},
	url = {http://arxiv.org/abs/2304.02643},
	abstract = {We introduce the Segment Anything ({SA}) project: a new task, model, and dataset for image segmentation. Using our efﬁcient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and ﬁnd that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model ({SAM}) and corresponding dataset ({SA}-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	number = {{arXiv}:2304.02643},
	publisher = {{arXiv}},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	urldate = {2024-04-29},
	date = {2023-04-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{hu_segment_2023,
	title = {Segment Anything in Defect Detection},
	url = {http://arxiv.org/abs/2311.10245},
	abstract = {Defect detection plays a crucial role in infrared nondestructive testing systems, offering non-contact, safe, and efficient inspection capabilities. However, challenges such as low resolution, high noise, and uneven heating in infrared thermal images hinder comprehensive and accurate defect detection. In this study, we propose {DefectSAM}, a novel approach for segmenting defects on highly noisy thermal images based on the widely adopted model, Segment Anything ({SAM}) [20]. Harnessing the power of a meticulously curated dataset generated through laborintensive lab experiments and valuable prompts from experienced experts, {DefectSAM} surpasses existing state-of-theart segmentation algorithms and achieves significant improvements in defect detection rates. Notably, {DefectSAM} excels in detecting weaker and smaller defects on complex and irregular surfaces, reducing the occurrence of missed detections and providing more accurate defect size estimations. Experimental studies conducted on various materials have validated the effectiveness of our solutions in defect detection, which hold significant potential to expedite the evolution of defect detection tools, enabling enhanced inspection capabilities and accuracy in defect identification.},
	number = {{arXiv}:2311.10245},
	publisher = {{arXiv}},
	author = {Hu, Bozhen and Gao, Bin and Tan, Cheng and Wu, Tongle and Li, Stan Z.},
	urldate = {2024-04-29},
	date = {2023-11-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.10245 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{ding_adapting_2024,
	title = {Adapting Segment Anything Model for Change Detection in {VHR} Remote Sensing Images},
	volume = {62},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/10443350/},
	doi = {10.1109/TGRS.2024.3368168},
	abstract = {Vision foundation models ({VFMs}), such as the segment anything model ({SAM}), allow zero-shot or interactive segmentation of visual contents; thus, they are quickly applied in a variety of visual scenes. However, their direct use in many remote sensing ({RS}) applications is often unsatisfactory due to the special imaging properties of {RS} images ({RSIs}). In this work, we aim to utilize the strong visual recognition capabilities of {VFMs} to improve change detection ({CD}) in very high-resolution ({VHR}) {RSIs}. We employ the visual encoder of {FastSAM}, a variant of the {SAM}, to extract visual representations in {RS} scenes. To adapt {FastSAM} to focus on some specific ground objects in {RS} scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to {SAM} features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bitemporal {RSIs}. The resulting method, {SAM}-based {CD} ({SAM}-{CD}), obtains superior accuracy compared with the state-of-the-art ({SOTA}) fully supervised {CD} methods and exhibits a sample-efficient learning ability that is comparable to semisupervised {CD} methods. To the best of our knowledge, this is the first work that adapts {VFMs} to {CD} in {VHR} {RSIs}.},
	pages = {1--11},
	journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
	shortjournal = {{IEEE} Trans. Geosci. Remote Sensing},
	author = {Ding, Lei and Zhu, Kun and Peng, Daifeng and Tang, Hao and Yang, Kuiwu and Bruzzone, Lorenzo},
	urldate = {2024-04-29},
	date = {2024},
	langid = {english},
}

@misc{tang_can_2023,
	title = {Can {SAM} Segment Anything? When {SAM} Meets Camouflaged Object Detection},
	url = {http://arxiv.org/abs/2304.04709},
	shorttitle = {Can {SAM} Segment Anything?},
	abstract = {{SAM} is a segmentation model recently released by Meta {AI} Research and has been gaining attention quickly due to its impressive performance in generic object segmentation. However, its ability to generalize to speciﬁc scenes such as camouﬂaged scenes is still unknown. Camouﬂaged object detection ({COD}) involves identifying objects that are seamlessly integrated into their surroundings and has numerous practical applications in ﬁelds such as medicine, art, and agriculture. In this study, we try to ask if {SAM} can address the {COD} task and evaluate the performance of {SAM} on the {COD} benchmark by employing maximum segmentation evaluation and camouﬂage location evaluation. We also compare {SAM}’s performance with 22 state-of-the-art {COD} methods. Our results indicate that while {SAM} shows promise in generic object segmentation, its performance on the {COD} task is limited. This presents an opportunity for further research to explore how to build a stronger {SAM} that may address the {COD} task. The results of this paper are provided in https://github.com/ luckybird1994/{SAMCOD}.},
	number = {{arXiv}:2304.04709},
	publisher = {{arXiv}},
	author = {Tang, Lv and Xiao, Haoke and Li, Bo},
	urldate = {2024-04-29},
	date = {2023-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.04709 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wong_synthetic_2019,
	title = {Synthetic dataset generation for object-to-model deep learning in industrial applications},
	url = {http://arxiv.org/abs/1909.10976},
	abstract = {The availability of large image data sets has been a crucial factor in the success of deep learning-based classification and detection methods. While data sets for everyday objects are widely available, data for specific industrial use-cases (e.g. identifying packaged products in a warehouse) remains scarce. In such cases, the data sets have to be created from scratch, placing a crucial bottleneck on the deployment of deep learning techniques in industrial applications. We present work carried out in collaboration with a leading {UK} online supermarket, with the aim of creating a computer vision system capable of detecting and identifying unique supermarket products in a warehouse setting. To this end, we demonstrate a framework for using synthetic data to create an end-to-end deep learning pipeline, beginning with real-world objects and culminating in a trained model. Our method is based on the generation of a synthetic dataset from 3D models obtained by applying photogrammetry techniques to real-world objects. Using 100k synthetic images generated from 60 real images per class, an {InceptionV}3 convolutional neural network ({CNN}) was trained, which achieved classification accuracy of 95.8\% on a separately acquired test set of real supermarket product images. The image generation process supports automatic pixel annotation. This eliminates the prohibitively expensive manual annotation typically required for detection tasks. Based on this readily available data, a one-stage {RetinaNet} detector was trained on the synthetic, annotated images to produce a detector that can accurately localize and classify the specimen products in real-time.},
	number = {{arXiv}:1909.10976},
	publisher = {{arXiv}},
	author = {Wong, Matthew Z. and Kunii, Kiyohito and Baylis, Max and Ong, Wai Hong and Kroupa, Pavel and Koller, Swen},
	urldate = {2024-04-25},
	date = {2019-09-24},
	eprinttype = {arxiv},
	eprint = {1909.10976 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{brachmann_scene_2024,
	title = {Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer},
	url = {http://arxiv.org/abs/2404.14351},
	shorttitle = {Scene Coordinate Reconstruction},
	abstract = {We address the task of estimating camera parameters from a set of images depicting a scene. Popular feature-based structure-from-motion ({SfM}) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud. We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction. This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching. We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images. Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images. Our method, {ACE}0 ({ACE} Zero), estimates camera poses to an accuracy comparable to feature-based {SfM}, as demonstrated by novel view synthesis. Project page: https://nianticlabs.github.io/acezero/},
	number = {{arXiv}:2404.14351},
	publisher = {{arXiv}},
	author = {Brachmann, Eric and Wynn, Jamie and Chen, Shuai and Cavallari, Tommaso and Monszpart, Áron and Turmukhambetov, Daniyar and Prisacariu, Victor Adrian},
	urldate = {2024-04-25},
	date = {2024-04-22},
	eprinttype = {arxiv},
	eprint = {2404.14351 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{girshick_fast_2015,
	title = {Fast R-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-{CNN}) for object detection. Fast R-{CNN} builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-{CNN} employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-{CNN} trains the very deep {VGG}16 network 9x faster than R-{CNN}, is 213x faster at test-time, and achieves a higher {mAP} on {PASCAL} {VOC} 2012. Compared to {SPPnet}, Fast R-{CNN} trains {VGG}16 3x faster, tests 10x faster, and is more accurate. Fast R-{CNN} is implemented in Python and C++ (using Caffe) and is available under the open-source {MIT} License at https://github.com/rbgirshick/fast-rcnn.},
	number = {{arXiv}:1504.08083},
	publisher = {{arXiv}},
	author = {Girshick, Ross},
	urldate = {2024-04-23},
	date = {2015-09-27},
	eprinttype = {arxiv},
	eprint = {1504.08083 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhao_object_2019,
	title = {Object Detection With Deep Learning: A Review},
	volume = {30},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8627998/},
	doi = {10.1109/TNNLS.2018.2876865},
	shorttitle = {Object Detection With Deep Learning},
	abstract = {Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classiﬁers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modiﬁcations and useful tricks to improve detection performance further. As distinct speciﬁc detection tasks exhibit different characteristics, we also brieﬂy survey several speciﬁc tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	pages = {3212--3232},
	number = {11},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	urldate = {2024-04-23},
	date = {2019-11},
	langid = {english},
}

@article{zou_object_2023,
	title = {Object Detection in 20 Years: A Survey},
	volume = {111},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/10028728/},
	doi = {10.1109/JPROC.2023.3238524},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today’s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fastmoving research field in the light of technical evolution, spanning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.},
	pages = {257--276},
	number = {3},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2024-04-23},
	date = {2023-03},
	langid = {english},
}

@incollection{vedaldi_end--end_2020,
	location = {Cham},
	title = {End-to-End Object Detection with Transformers},
	volume = {12346},
	isbn = {978-3-030-58451-1 978-3-030-58452-8},
	url = {https://link.springer.com/10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called {DEtection} {TRansformer} or {DETR}, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, {DETR} reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. {DETR} demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster {RCNN} baseline on the challenging {COCO} object detection dataset. Moreover, {DETR} can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	pages = {213--229},
	booktitle = {Computer Vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	urldate = {2024-04-22},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-58452-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{howard_mobilenets_2017,
	title = {{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
	url = {http://arxiv.org/abs/1704.04861},
	shorttitle = {{MobileNets}},
	abstract = {We present a class of efficient models called {MobileNets} for mobile and embedded vision applications. {MobileNets} are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on {ImageNet} classification. We then demonstrate the effectiveness of {MobileNets} across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	number = {{arXiv}:1704.04861},
	publisher = {{arXiv}},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	urldate = {2024-04-17},
	date = {2017-04-16},
	eprinttype = {arxiv},
	eprint = {1704.04861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{tang_transferable_2019,
	location = {Seoul, Korea (South)},
	title = {Transferable Semi-Supervised 3D Object Detection From {RGB}-D Data},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008127/},
	doi = {10.1109/ICCV.2019.00202},
	abstract = {We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semisupervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the {SUN}-{RGBD} and {KITTI} datasets. Furthermore, we show that our Boxto-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {1931--1940},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Tang, Yew Siang and Lee, Gim Hee},
	urldate = {2024-04-15},
	date = {2019-10},
	langid = {english},
}

@inproceedings{yang_learning_2019,
	title = {Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d0aa518d4d3bfc721aa0b8ab4ef32269-Abstract.html},
	abstract = {We propose a novel, conceptually simple and general framework for instance segmentation on 3D point clouds. Our method, called 3D-{BoNet}, follows the simple design philosophy of per-point multilayer perceptrons ({MLPs}). The framework directly regresses 3D bounding boxes for all instances in a point cloud, while simultaneously predicting a point-level mask for each instance. It consists of a backbone network followed by two parallel network branches for 1) bounding box regression and 2) point mask prediction. 3D-{BoNet} is single-stage, anchor-free and end-to-end trainable. Moreover, it is remarkably computationally efficient as, unlike existing approaches, it does not require any post-processing steps such as non-maximum suppression, feature sampling, clustering or voting. Extensive experiments show that our approach surpasses existing work on both {ScanNet} and S3DIS datasets while being approximately 10x more computationally efficient. Comprehensive ablation studies demonstrate the effectiveness of our design.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Bo and Wang, Jianan and Clark, Ronald and Hu, Qingyong and Wang, Sen and Markham, Andrew and Trigoni, Niki},
	urldate = {2024-04-15},
	date = {2019},
}

@inproceedings{wang_bridged_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {Bridged Transformer for Vision and Point Cloud 3D Object Detection},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880216/},
	doi = {10.1109/CVPR52688.2022.01180},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {12104--12113},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Wang, Yikai and Ye, {TengQi} and Cao, Lele and Huang, Wenbing and Sun, Fuchun and He, Fengxiang and Tao, Dacheng},
	urldate = {2024-04-15},
	date = {2022-06},
	langid = {english},
}

@article{li_deep_2021,
	title = {Deep Learning for {LiDAR} Point Clouds in Autonomous Driving: A Review},
	volume = {32},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9173706/},
	doi = {10.1109/TNNLS.2020.3015992},
	shorttitle = {Deep Learning for {LiDAR} Point Clouds in Autonomous Driving},
	abstract = {Recently, the advancement of deep learning ({DL}) in discriminative feature learning from 3-D {LiDAR} data has led to rapid development in the ﬁeld of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3-D point clouds are a challenging and tedious task. In this article, we provide a systematic review of existing compelling {DL} architectures applied in {LiDAR} point clouds, detailing for speciﬁc tasks in autonomous driving, such as segmentation, detection, and classiﬁcation. Although several published research articles focus on speciﬁc topics in computer vision for autonomous vehicles, to date, no general survey on {DL} applied in {LiDAR} point clouds for autonomous vehicles exists. Thus, the goal of this article is to narrow the gap in this topic. More than 140 key contributions in the recent ﬁve years are summarized in this survey, including the milestone 3-D deep architectures, the remarkable {DL} applications in 3-D semantic segmentation, object detection, and classiﬁcation; speciﬁc data sets, evaluation metrics, and the state-of-the-art performance. Finally, we conclude the remaining challenges and future researches.},
	pages = {3412--3432},
	number = {8},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Li, Ying and Ma, Lingfei and Zhong, Zilong and Liu, Fei and Chapman, Michael A. and Cao, Dongpu and Li, Jonathan},
	urldate = {2024-04-12},
	date = {2021-08},
	langid = {english},
}

@article{li_deep_2021-1,
	title = {Deep Learning for {LiDAR} Point Clouds in Autonomous Driving: A Review},
	volume = {32},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9173706/},
	doi = {10.1109/TNNLS.2020.3015992},
	shorttitle = {Deep Learning for {LiDAR} Point Clouds in Autonomous Driving},
	abstract = {Recently, the advancement of deep learning ({DL}) in discriminative feature learning from 3-D {LiDAR} data has led to rapid development in the ﬁeld of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3-D point clouds are a challenging and tedious task. In this article, we provide a systematic review of existing compelling {DL} architectures applied in {LiDAR} point clouds, detailing for speciﬁc tasks in autonomous driving, such as segmentation, detection, and classiﬁcation. Although several published research articles focus on speciﬁc topics in computer vision for autonomous vehicles, to date, no general survey on {DL} applied in {LiDAR} point clouds for autonomous vehicles exists. Thus, the goal of this article is to narrow the gap in this topic. More than 140 key contributions in the recent ﬁve years are summarized in this survey, including the milestone 3-D deep architectures, the remarkable {DL} applications in 3-D semantic segmentation, object detection, and classiﬁcation; speciﬁc data sets, evaluation metrics, and the state-of-the-art performance. Finally, we conclude the remaining challenges and future researches.},
	pages = {3412--3432},
	number = {8},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Li, Ying and Ma, Lingfei and Zhong, Zilong and Liu, Fei and Chapman, Michael A. and Cao, Dongpu and Li, Jonathan},
	urldate = {2024-04-12},
	date = {2021-08},
	langid = {english},
}

@misc{hackel_semantic3dnet_2017,
	title = {Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark},
	url = {http://arxiv.org/abs/1704.03847},
	shorttitle = {Semantic3D.net},
	abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks ({CNNs}) as a work horse, which already show remarkable performance improvements over state-of-the-art. {CNNs} have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
	number = {{arXiv}:1704.03847},
	publisher = {{arXiv}},
	author = {Hackel, Timo and Savinov, Nikolay and Ladicky, Lubor and Wegner, Jan D. and Schindler, Konrad and Pollefeys, Marc},
	urldate = {2024-04-12},
	date = {2017-04-12},
	eprinttype = {arxiv},
	eprint = {1704.03847 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{zhang_review_2019,
	title = {A Review of Deep Learning-Based Semantic Segmentation for Point Cloud},
	volume = {7},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8930503/},
	doi = {10.1109/ACCESS.2019.2958671},
	abstract = {In recent years, the popularity of depth sensors and 3D scanners has led to a rapid development of 3D point clouds. Semantic segmentation of point cloud, as a key step in understanding 3D scenes, has attracted extensive attention of researchers. Recent advances in this topic are dominantly led by deep learning-based methods. In this paper, we provide a survey covering various aspects ranging from indirect segmentation to direct segmentation. Firstly, we review methods of indirect segmentation based on multi-views and voxel grids, as well as direct segmentation methods from different perspectives including point ordering, multi-scale, feature fusion and fusion of graph convolutional neural network ({GCNN}). Then, the common datasets for point cloud segmentation are exposed to help researchers choose which one is the most suitable for their tasks. Following that, we devote a part of the paper to analyze the quantitative results of these methods. Finally, the development trend of point cloud semantic segmentation technology is prospected.},
	pages = {179118--179133},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Zhang, Jiaying and Zhao, Xiaoli and Chen, Zheng and Lu, Zhejun},
	urldate = {2024-04-12},
	date = {2019},
	langid = {english},
}

@misc{wang_mamba-unet_2024,
	title = {Mamba-{UNet}: {UNet}-Like Pure Visual Mamba for Medical Image Segmentation},
	url = {http://arxiv.org/abs/2402.05079},
	shorttitle = {Mamba-{UNet}},
	abstract = {In recent advancements in medical image analysis, Convolutional Neural Networks ({CNN}) and Vision Transformers ({ViT}) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model ({SSM}), we propose Mamba-{UNet}, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability. Mamba-{UNet} adopts a pure Visual Mamba ({VMamba})-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the {VMamba} blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available {ACDC} {MRI} Cardiac segmentation dataset, and Synapse {CT} Abdomen segmentation dataset. The results show that Mamba-{UNet} outperforms several types of {UNet} in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.},
	number = {{arXiv}:2402.05079},
	publisher = {{arXiv}},
	author = {Wang, Ziyang and Zheng, Jian-Qing and Zhang, Yichi and Cui, Ge and Li, Lei},
	urldate = {2024-04-11},
	date = {2024-03-30},
	eprinttype = {arxiv},
	eprint = {2402.05079 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{ma_u-mamba_2024,
	title = {U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/2401.04722},
	shorttitle = {U-Mamba},
	abstract = {Convolutional Neural Networks ({CNNs}) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models ({SSMs}), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid {CNN}-{SSM} block that integrates the local feature extraction power of convolutional layers with the abilities of {SSMs} for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in {CT} and {MR} images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art {CNN}-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at https://wanglab.ai/u-mamba.html.},
	number = {{arXiv}:2401.04722},
	publisher = {{arXiv}},
	author = {Ma, Jun and Li, Feifei and Wang, Bo},
	urldate = {2024-04-11},
	date = {2024-01-09},
	eprinttype = {arxiv},
	eprint = {2401.04722 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{gu_mamba_2023,
	title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
	url = {http://arxiv.org/abs/2312.00752},
	shorttitle = {Mamba},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models ({SSMs}) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the {SSM} parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective {SSMs} into a simplified end-to-end neural network architecture without attention or even {MLP} blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	number = {{arXiv}:2312.00752},
	publisher = {{arXiv}},
	author = {Gu, Albert and Dao, Tri},
	urldate = {2024-04-11},
	date = {2023-12-01},
	eprinttype = {arxiv},
	eprint = {2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{ding_unireplknet_2024,
	title = {{UniRepLKNet}: A Universal Perception Large-Kernel {ConvNet} for Audio, Video, Point Cloud, Time-Series and Image Recognition},
	url = {http://arxiv.org/abs/2311.15599},
	shorttitle = {{UniRepLKNet}},
	abstract = {Large-kernel convolutional neural networks ({ConvNets}) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel {ConvNets} largely follow the design principles of conventional {ConvNets} or transformers, while the architectural design for large-kernel {ConvNets} remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether {ConvNets} also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel {ConvNets}, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel {ConvNet} shows leading performance in image recognition ({ImageNet} accuracy of 88.0\%, {ADE}20K {mIoU} of 55.6\%, and {COCO} box {AP} of 56.4\%), demonstrating better performance and higher speed than the recent powerful competitors. 2) We discover large kernels are the key to unlocking the exceptional performance of {ConvNets} in domains where they were originally not proficient. With certain modality-related preprocessing approaches, the proposed model achieves state-of-the-art performance on time-series forecasting and audio recognition tasks even without modality-specific customization to the architecture. All the code and models are publicly available on {GitHub} and Huggingface.},
	number = {{arXiv}:2311.15599},
	publisher = {{arXiv}},
	author = {Ding, Xiaohan and Zhang, Yiyuan and Ge, Yixiao and Zhao, Sijie and Song, Lin and Yue, Xiangyu and Shan, Ying},
	urldate = {2024-04-08},
	date = {2024-03-18},
	eprinttype = {arxiv},
	eprint = {2311.15599 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	number = {{arXiv}:1511.06434},
	publisher = {{arXiv}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2024-03-29},
	date = {2016-01-07},
	eprinttype = {arxiv},
	eprint = {1511.06434 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{gama_idsgan_2022,
	location = {Cham},
	title = {{IDSGAN}: Generative Adversarial Networks for Attack Generation Against Intrusion Detection},
	volume = {13282},
	isbn = {978-3-031-05980-3 978-3-031-05981-0},
	url = {https://link.springer.com/10.1007/978-3-031-05981-0_7},
	shorttitle = {{IDSGAN}},
	abstract = {As an essential tool in security, the intrusion detection system bears the responsibility of the defense to network attacks performed by malicious traﬃc. Nowadays, with the help of machine learning algorithms, intrusion detection systems develop rapidly. However, the robustness of this system is questionable when it faces adversarial attacks. For the robustness of detection systems, more potential attack approaches are under research. In this paper, a framework of the generative adversarial networks, called {IDSGAN}, is proposed to generate the adversarial malicious traﬃc records aiming to attack intrusion detection systems by deceiving and evading the detection. Given that the internal structure and parameters of the detection system are unknown to attackers, the adversarial attack examples perform the black-box attacks against the detection system. {IDSGAN} leverages a generator to transform original malicious traﬃc records into adversarial malicious ones. A discriminator classiﬁes traﬃc examples and dynamically learns the real-time black-box detection system. More signiﬁcantly, the restricted modiﬁcation mechanism is designed for the adversarial generation to preserve original attack functionalities of adversarial traﬃc records. The eﬀectiveness of the model is indicated by attacking multiple algorithm-based detection models with diﬀerent attack categories. The robustness is veriﬁed by changing the number of the modiﬁed features. A comparative experiment with adversarial attack baselines demonstrates the superiority of our model.},
	pages = {79--91},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer International Publishing},
	author = {Lin, Zilong and Shi, Yong and Xue, Zhi},
	editor = {Gama, João and Li, Tianrui and Yu, Yang and Chen, Enhong and Zheng, Yu and Teng, Fei},
	urldate = {2024-03-29},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-05981-0_7},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{doersch_tutorial_2021,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	number = {{arXiv}:1606.05908},
	publisher = {{arXiv}},
	author = {Doersch, Carl},
	urldate = {2024-03-29},
	date = {2021-01-03},
	eprinttype = {arxiv},
	eprint = {1606.05908 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bowles_gan_2018,
	title = {{GAN} Augmentation: Augmenting Training Data using Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1810.10863},
	shorttitle = {{GAN} Augmentation},
	abstract = {One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks ({GANs}) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing {GAN} derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient ({DSC}) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.},
	number = {{arXiv}:1810.10863},
	publisher = {{arXiv}},
	author = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hernández, Maria Valdés and Wardlaw, Joanna and Rueckert, Daniel},
	urldate = {2024-03-29},
	date = {2018-10-25},
	eprinttype = {arxiv},
	eprint = {1810.10863 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wong_understanding_2016,
	location = {Gold Coast, Australia},
	title = {Understanding Data Augmentation for Classification: When to Warp?},
	isbn = {978-1-5090-2896-2},
	url = {http://ieeexplore.ieee.org/document/7797091/},
	doi = {10.1109/DICTA.2016.7797091},
	shorttitle = {Understanding Data Augmentation for Classification},
	abstract = {In this paper we investigate the beneﬁt of augmenting data with synthetically created samples when training a machine learning classiﬁer. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the beneﬁts of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classiﬁer, using the standard {MNIST} handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater beneﬁt for improving performance and reducing overﬁtting.},
	eventtitle = {2016 International Conference on Digital Image Computing: Techniques and Applications ({DICTA})},
	pages = {1--6},
	booktitle = {2016 International Conference on Digital Image Computing: Techniques and Applications ({DICTA})},
	publisher = {{IEEE}},
	author = {Wong, Sebastien C. and Gatt, Adam and Stamatescu, Victor and {McDonnell}, Mark D.},
	urldate = {2024-03-29},
	date = {2016-11},
	langid = {english},
}

@misc{devries_dataset_2017,
	title = {Dataset Augmentation in Feature Space},
	url = {http://arxiv.org/abs/1702.05538},
	abstract = {Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.},
	number = {{arXiv}:1702.05538},
	publisher = {{arXiv}},
	author = {{DeVries}, Terrance and Taylor, Graham W.},
	urldate = {2024-03-29},
	date = {2017-02-17},
	eprinttype = {arxiv},
	eprint = {1702.05538 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2024-03-29},
	date = {2019-12},
	langid = {english},
}

@inproceedings{bengio_curriculum_2009,
	location = {Montreal Quebec Canada},
	title = {Curriculum learning},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553380},
	doi = {10.1145/1553374.1553380},
	abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”. In the context of recent research studying the diﬃculty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that signiﬁcant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an eﬀect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	eventtitle = {{ICML} '09: The 26th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming},
	pages = {41--48},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	urldate = {2024-03-29},
	date = {2009-06-14},
	langid = {english},
}

@article{wagner_river_2023,
	title = {River water segmentation in surveillance camera images: A comparative study of offline and online augmentation using 32 {CNNs}},
	volume = {119},
	issn = {15698432},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1569843223001279},
	doi = {10.1016/j.jag.2023.103305},
	shorttitle = {River water segmentation in surveillance camera images},
	abstract = {To obtain reliable water segmentations from image data for real-time monitoring of river water levels, a comparison of 32 convolutional neural networks was performed. They were trained on a new river water segmentation dataset consisting of 1128 images. To prevent overfitting, two methods using offline and online augmentation were developed to improve the variance. It was found that offline augmentation is superior on fewer data, while online augmentation is advantageous for a larger dataset (such as Cityscapes).},
	pages = {103305},
	journaltitle = {International Journal of Applied Earth Observation and Geoinformation},
	shortjournal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Wagner, Franz and Eltner, Anette and Maas, Hans-Gerd},
	urldate = {2024-03-29},
	date = {2023-05},
	langid = {english},
}

@article{chizhova_towards_2024,
	title = {{TOWARDS} {AUTOMATIC} {DEFECTS} {ANALYSES} {FOR} 3D {STRUCTURAL} {MONITORING} {OF} {HISTORIC} {TIMBER}},
	volume = {{XLVIII}-2-W4-2024},
	rights = {All rights reserved},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-2-W4-2024/103/2024/isprs-archives-XLVIII-2-W4-2024-103-2024.html},
	doi = {10.5194/isprs-archives-XLVIII-2-W4-2024-103-2024},
	abstract = {Stability of historic wooden constructions is changing with time and should be inspected appropriately for risk assessment and prevention. The stability or strength values of built-in historic timber are difficult or even impossible to be derived without invasive investigation, but this is particularly problematic for the monitoring of heritage objects. Luckily there are some visible timber surface features, like knots and cracks, which can act as individual evidence to estimate the wood strength as well as to adjust its grade class indicator. In the final project, we aim to compare different approaches for 3D digital documentation of historic wood timbers and focus on automatic knot detection using {AI} techniques. A first feasibility study reported here provides a scientific baseline for the development of an automated method to analyse historic timber stability using 3D surveying and recognised surface features. First results about texture and resolution properties are discussed here.},
	pages = {103--110},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Chizhova, M. and Pan, J. and Luhmann, T. and Karami, A. and Menna, F. and Remondino, F. and Hess, M. and Eißing, T.},
	urldate = {2024-03-29},
	date = {2024-02-14},
	note = {Conference Name: {ISPRS} / {CIPA}{\textless}br{\textgreater} 10th International Workshop 3D-{ARCH} "3D Virtual Reconstruction and Visualization of Complex Architectures" - 21–23 February 2024, Siena, Italy
Publisher: Copernicus {GmbH}},
	keywords = {3D scanning, feature extraction, historic wooden timber, machine learning, multi view photometric stereo, reflectance transformation imaging, structure from motion},
}

@article{chizhova_towards_2024-1,
	title = {{TOWARDS} {AUTOMATIC} {DEFECTS} {ANALYSES} {FOR} 3D {STRUCTURAL} {MONITORING} {OF} {HISTORIC} {TIMBER}},
	volume = {{XLVIII}-2/W4-2024},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-2-W4-2024/103/2024/},
	doi = {10.5194/isprs-archives-XLVIII-2-W4-2024-103-2024},
	abstract = {Stability of historic wooden constructions is changing with time and should be inspected appropriately for risk assessment and prevention. The stability or strength values of built-in historic timber are difficult or even impossible to be derived without invasive investigation, but this is particularly problematic for the monitoring of heritage objects. Luckily there are some visible timber surface features, like knots and cracks, which can act as individual evidence to estimate the wood strength as well as to adjust its grade class indicator. In the final project, we aim to compare different approaches for 3D digital documentation of historic wood timbers and focus on automatic knot detection using {AI} techniques. A first feasibility study reported here provides a scientific baseline for the development of an automated method to analyse historic timber stability using 3D surveying and recognised surface features. First results about texture and resolution properties are discussed here.},
	pages = {103--110},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Chizhova, M. and Pan, J. and Luhmann, T. and Karami, A. and Menna, F. and Remondino, F. and Hess, M. and Eißing, T.},
	urldate = {2024-03-29},
	date = {2024-02-14},
	langid = {english},
}

@online{noauthor_large_nodate,
	title = {Large Scale Image Dataset of Wood Surface Defects},
	url = {https://www.kaggle.com/datasets/nomihsa965/large-scale-image-dataset-of-wood-surface-defects},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	urldate = {2023-11-02},
	langid = {english},
}

@article{roglin_improving_2022,
	title = {Improving classification results on a small medical dataset using a {GAN}; An outlook for dealing with rare disease datasets},
	volume = {4},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.858874/full},
	doi = {10.3389/fcomp.2022.858874},
	abstract = {For clinical decision support systems, automated classification algorithms on medical image data have become more important in the past. For such computer vision problems, deep convolutional neural networks ({DCNNs}) have made breakthroughs. These often require large, annotated, and privacy-cleared datasets as a prerequisite for gaining high-quality results. This proves to be difficult with rare diseases due to limited incidences. Therefore, it is hard to sensitize clinical decision support systems to identify these diseases at an early stage. It has been shown several times, that synthetic data can improve the results of clinical decision support systems. At the same time, the greatest problem for the generation of these synthetic images is the data basis. In this paper, we present four different methods to generate synthetic data from a small dataset. The images are from 2D magnetic resonance tomography of the spine. The annotation resulted in 540 healthy, 47 conspicuously non-pathological, and 106 conspicuously pathological vertebrae. Four methods are presented to obtain optimal generation results in each of these classes. The obtained generation results are then evaluated with a classification net. With this procedure, we showed that adding synthetic annotated data has a positive impact on the classification results of the original data. In addition, one of our methods is appropriate to generate synthetic image data from \&lt;50 images. Thus, we found a general approach for dealing with small datasets in rare diseases, which can be used to build sensitized clinical decision support systems to detect and treat these diseases at an early stage.},
	pages = {858874},
	journaltitle = {Frontiers in Computer Science},
	shortjournal = {Front. Comput. Sci.},
	author = {Röglin, Julia and Ziegeler, Katharina and Kube, Jana and König, Franziska and Hermann, Kay-Geert and Ortmann, Steffen},
	urldate = {2024-03-24},
	date = {2022-08-25},
	langid = {english},
}

@article{islam_gan-based_2020,
	title = {{GAN}-based synthetic brain {PET} image generation},
	volume = {7},
	issn = {2198-4018, 2198-4026},
	url = {https://braininformatics.springeropen.com/articles/10.1186/s40708-020-00104-2},
	doi = {10.1186/s40708-020-00104-2},
	abstract = {In recent days, deep learning technologies have achieved tremendous success in computer vision-related tasks with the help of large-scale annotated dataset. Obtaining such dataset for medical image analysis is very challenging. Working with the limited dataset and small amount of annotated samples makes it difficult to develop a robust automated disease diagnosis model. We propose a novel approach to generate synthetic medical images using generative adversarial networks ({GANs}). Our proposed model can create brain {PET} images for three different stages of Alzheimer’s disease—normal control ({NC}), mild cognitive impairment ({MCI}), and Alzheimer’s disease ({AD}).},
	pages = {3},
	number = {1},
	journaltitle = {Brain Informatics},
	shortjournal = {Brain Inf.},
	author = {Islam, Jyoti and Zhang, Yanqing},
	urldate = {2024-03-24},
	date = {2020-12},
	langid = {english},
}

@article{gao_applying_2024,
	title = {Applying optimized {YOLOv}8 for heritage conservation: enhanced object detection in Jiangnan traditional private gardens},
	volume = {12},
	issn = {2050-7445},
	url = {https://heritagesciencejournal.springeropen.com/articles/10.1186/s40494-024-01144-1},
	doi = {10.1186/s40494-024-01144-1},
	shorttitle = {Applying optimized {YOLOv}8 for heritage conservation},
	abstract = {This study aims to promote the protection and inheritance of cultural heritage in private gardens in the Jiangnan area of China. By establishing a precise visual labeling system and accelerating the construction of a database for private garden features, we deepen the understanding of garden design philosophy. To this end, we propose an improved Jiangnan private garden recognition model based on You Only Look Once ({YOLO}) v8. This model is particularly suitable for processing garden environments with characteristics such as single or complex structures, rich depth of field, and cluttered targets, effectively enhancing the accuracy and efficiency of object recognition. This design integrates the Diverse Branch Block ({DBB}), Bidirectional Feature Pyramid Network ({BiFPN}), and Dynamic Head modules ({DyHead}) to optimize model accuracy, feature fusion, and object detection representational capability, respectively. The enhancements elevated the model’s accuracy by 8.7\%, achieving a mean average precision ({mAP}@0.5) value of 57.1\%. A specialized dataset, comprising 4890 images and encapsulating various angles and lighting conditions of Jiangnan private gardens, was constructed to realize this. Following manual annotation and the application of diverse data augmentation strategies, the dataset bolsters the generalization and robustness of the model. Experimental outcomes reveal that, compared to its predecessor, the improved model has witnessed increments of 15.16\%, 3.25\%, and 11.88\% in precision, {mAP}0.5, and {mAP}0.5:0.95 metrics, respectively, demonstrating exemplary performance in the accuracy and real-time recognition of garden target elements. This research not only furnishes robust technical support for the digitization and intelligent research of Jiangnan private gardens but also provides a potent methodological reference for object detection and classification research in analogous domains.},
	pages = {31},
	number = {1},
	journaltitle = {Heritage Science},
	shortjournal = {Herit Sci},
	author = {Gao, Chan and Zhang, Qingzhu and Tan, Zheyu and Zhao, Genfeng and Gao, Sen and Kim, Eunyoung and Shen, Tao},
	urldate = {2024-03-24},
	date = {2024-01-29},
	langid = {english},
}

@article{talaat_improved_2023,
	title = {An improved fire detection approach based on {YOLO}-v8 for smart cities},
	volume = {35},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-023-08809-1},
	doi = {10.1007/s00521-023-08809-1},
	abstract = {Fires in smart cities can have devastating consequences, causing damage to property, and endangering the lives of citizens. Traditional ﬁre detection methods have limitations in terms of accuracy and speed, making it challenging to detect ﬁres in real time. This paper proposes an improved ﬁre detection approach for smart cities based on the {YOLOv}8 algorithm, called the smart ﬁre detection system ({SFDS}), which leverages the strengths of deep learning to detect ﬁre-speciﬁc features in real time. The {SFDS} approach has the potential to improve the accuracy of ﬁre detection, reduce false alarms, and be costeffective compared to traditional ﬁre detection methods. It can also be extended to detect other objects of interest in smart cities, such as gas leaks or ﬂooding. The proposed framework for a smart city consists of four primary layers: (i) Application layer, (ii) Fog layer, (iii) Cloud layer, and (iv) {IoT} layer. The proposed algorithm utilizes Fog and Cloud computing, along with the {IoT} layer, to collect and process data in real time, enabling faster response times and reducing the risk of damage to property and human life. The {SFDS} achieved state-of-the-art performance in terms of both precision and recall, with a high precision rate of 97.1\% for all classes. The proposed approach has several potential applications, including ﬁre safety management in public areas, forest ﬁre monitoring, and intelligent security systems.},
	pages = {20939--20954},
	number = {28},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Talaat, Fatma M. and {ZainEldin}, Hanaa},
	urldate = {2024-03-24},
	date = {2023-10},
	langid = {english},
}

@article{conners_identifying_1983-1,
	title = {Identifying and Locating Surface Defects in Wood: Part of an Automated Lumber Processing System},
	volume = {{PAMI}-5},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4767446/},
	doi = {10.1109/TPAMI.1983.4767446},
	shorttitle = {Identifying and Locating Surface Defects in Wood},
	abstract = {Continued increases in the cost of materials and labor make it imperative for furniture manufacturers to control costs by improved yield and increased productivity. This paper describes an Automated Lumber Processing System ({ALPS}) that employs computer tomography, optical scanning technology, the calculation of an optimum cutting strategy, and a computer-driven laser cutting device. While certain major hardware components of {ALPS} are already commercially available, a major missing element is the automatic inspection system needed to locate and identify surface defects on boards. This paper reports research aimed at developing such an inspection system. The basic strategy is to divide the digital image of a board into a number of disjoint rectangular regions and classify each independently. This simple procedure has the advantage of allowing an obvious parallel processing implementation. The study shows that measures of tonal and pattern related qualities are needed. The tonal measures are the mean, variance, skewness, and kurtosis of the gray levels. The pattern related measures are those based on cooccurrence matrices. In this initial feasibility study, these combined measures yielded an overall 88.3 percent correct classification on the eight defects most commonly found in lumber. To minimize the number of calculations needed to make the required classifications a sequential classifier is proposed.},
	pages = {573--583},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Conners, Richard W. and Mcmillin, Charles W. and Lin, Kingyao and Vasquez-Espinosa, Ramon E.},
	urldate = {2024-03-21},
	date = {1983-11},
	langid = {english},
}

@article{he_fully_2019,
	title = {A Fully Convolutional Neural Network for Wood Defect Location and Identification},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8812894/},
	doi = {10.1109/ACCESS.2019.2937461},
	abstract = {Defect detection on solid wood surface has two main problems: (1) the real-time performance of the available methods are poor despite good detection accuracy, and (2) the defect extraction process is complicated. Here, we propose a mixed, fully convolutional neural network (Mix-{FCN}) to detect the location of wood defects and classify the types of defects from the wood surface images automatically. The images were collected ﬁrst by a data acquisition device developed in our laboratory. We then employed {TensorFlow} and Python language to construct a {VGG}16 model. We used two kinds of datasets (dataset1 and dataset2) to maximize the limited, collected data and enable the Mix-{FCN} to converge rapidly during training. The weights of the ﬁlters in front of the Mix-{FCN} during training were initialized from the trained {VGG}16 model. The weights of the {VGG}16 net were learned by dataset1. Our model was trained, validated, and tested by dataset 2. Overall classiﬁcation accuracy ({OCA}), pixel accuracy ({PA}), mean intersection over union, detection rate, missing alarm, false alarm rate, and precision were used to evaluate the network, and the performance was good based on the seven evaluation indicators. We achieved 99.14\% {OCA} and 91.31\% {PA}, and a batch of 50 images required only 0.368 s of detection time. Our proposed method has better accuracy and less detection time compared to the previous methods of wood detection.},
	pages = {123453--123462},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {He, Ting and Liu, Ying and Xu, Chengyi and Zhou, Xiaolin and Hu, Zhongkang and Fan, Jianan},
	urldate = {2024-03-21},
	date = {2019},
	langid = {english},
}

@article{silvn_wood_2003,
	title = {Wood inspection with non-supervised clustering},
	volume = {13},
	issn = {0932-8092, 1432-1769},
	url = {http://link.springer.com/10.1007/s00138-002-0084-z},
	doi = {10.1007/s00138-002-0084-z},
	abstract = {The appearance of sawn timber has huge natural variations that the human inspector easily compensates for mentally when determining the types of defects and the grade of each board. However, for automatic wood inspection systems these variations are a major source for complication. This makes it difﬁcult to use textbook methodologies for visual inspection. These methodologies generally aim at systems that are trained in a supervised manner with samples of defects and good material, but selecting and labeling the samples is an error-prone process that limits the accuracy that can be achieved. We present a non-supervised clustering-based approach for detecting and recognizing defects in lumber boards. A key idea is to employ a self-organizing map ({SOM}) for discriminating between sound wood and defects. Human involvement needed for training is minimal. The approach has been tested with color images of lumber boards, and the achieved false detection and error escape rates are low. The approach also provides a self-intuitive visual user interface.},
	pages = {275--285},
	number = {5},
	journaltitle = {Machine Vision and Applications},
	shortjournal = {Machine Vision and Applications},
	author = {Silv�n, Olli and Niskanen, Matti and Kauppinen, Hannu},
	urldate = {2024-03-21},
	date = {2003-03-01},
	langid = {english},
}

@article{shi_defect_2020,
	title = {Defect Detection of Industry Wood Veneer Based on {NAS} and Multi-Channel Mask R-{CNN}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/16/4398},
	doi = {10.3390/s20164398},
	abstract = {Wood veneer defect detection plays a vital role in the wood veneer production industry. Studies on wood veneer defect detection usually focused on detection accuracy for industrial applications but ignored algorithm execution speed; thus, their methods do not meet the required speed of online detection. In this paper, a new detection method is proposed that achieves high accuracy and a suitable speed for online production. Firstly, 2838 wood veneer images were collected using data collection equipment developed in the laboratory and labeled by experienced workers from a wood company. Then, an integrated model, glance multiple channel mask region convolution neural network (R-{CNN}), was constructed to detect wood veneer defects, which included a glance network and a multiple channel mask R-{CNN}. Neural network architect search technology was used to automatically construct the glance network with the lowest number of ﬂoating-point operations to pick out potential defect images out of numerous original wood veneer images. A genetic algorithm was used to merge the intermediate features extracted by the glance network. Multi-Channel Mask R-{CNN} was then used to classify and locate the defects. The experimental results show that the proposed method achieves a 98.70\% overall classiﬁcation accuracy and a 95.31\% mean average precision, and only 2.5 s was needed to detect a batch of 50 standard images and 50 defective images. Compared with other wood veneer defect detection methods, the proposed method is more accurate and faster.},
	pages = {4398},
	number = {16},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Shi, Jiahao and Li, Zhenye and Zhu, Tingting and Wang, Dongyi and Ni, Chao},
	urldate = {2024-03-21},
	date = {2020-08-06},
	langid = {english},
}

@article{chang_novel_2018-1,
	title = {A novel image segmentation approach for wood plate surface defect classification through convex optimization},
	volume = {29},
	issn = {1007-662X, 1993-0607},
	url = {http://link.springer.com/10.1007/s11676-017-0572-7},
	doi = {10.1007/s11676-017-0572-7},
	pages = {1789--1795},
	number = {6},
	journaltitle = {Journal of Forestry Research},
	shortjournal = {J. For. Res.},
	author = {Chang, Zhanyuan and Cao, Jun and Zhang, Yizhuo},
	urldate = {2024-03-21},
	date = {2018-11},
	langid = {english},
}

@inproceedings{cetiner_classification_2014,
	location = {Trabzon, Turkey},
	title = {Classification of {KNOT} defect types},
	isbn = {978-1-4799-4874-1},
	url = {http://ieeexplore.ieee.org/document/6830422/},
	doi = {10.1109/SIU.2014.6830422},
	abstract = {In this study, the experimental studies were carried out on a database containing the types of wood knot. After preprocessing on the images in the database, specific features to knot were obtained using wavelet moments feature extraction algorithm. Type description is carried out with {KNN} classification algorithm by selecting most distinguishing the approximation coefficients on these features. In conclusion, knot images could be classified with the success rate of 98\%.},
	eventtitle = {2014 22nd Signal Processing and Communications Applications Conference ({SIU})},
	pages = {1086--1089},
	booktitle = {2014 22nd Signal Processing and Communications Applications Conference ({SIU})},
	publisher = {{IEEE}},
	author = {Cetiner, Sbrahim and Var, A. Ali and Cetiner, Halit},
	urldate = {2024-03-21},
	date = {2014-04},
	langid = {turkish},
}

@inproceedings{marcano-cedeno_wood_2009,
	location = {Porto},
	title = {Wood defects classification using Artificial Metaplasticity neural network},
	isbn = {978-1-4244-4648-3 978-1-4244-4650-6},
	url = {http://ieeexplore.ieee.org/document/5415189/},
	doi = {10.1109/IECON.2009.5415189},
	eventtitle = {{IECON} 2009 - 35th Annual Conference of {IEEE} Industrial Electronics ({IECON} 2009)},
	pages = {3422--3427},
	booktitle = {2009 35th Annual Conference of {IEEE} Industrial Electronics},
	publisher = {{IEEE}},
	author = {Marcano-Cedeno, Alexis and Quintanilla-Dominguez, J. and Andina, D.},
	urldate = {2024-03-21},
	date = {2009-11},
	langid = {english},
}

@inproceedings{qayyum_wood_2016,
	location = {Colchester, United Kingdom},
	title = {Wood defects classification using {GLCM} based features and {PSO} trained neural network},
	isbn = {978-1-86218-132-8},
	url = {http://ieeexplore.ieee.org/document/7604931/},
	doi = {10.1109/IConAC.2016.7604931},
	abstract = {Machine vision based inspection system are in great focus nowadays for quality control applications. The paper presents a novel approach for classification of wood knot defects for an automated inspection. The proposed technique utilizes gray level co-occurrence matrix based features and a particle swarm optimization trained feedforward neural network. It takes contrast, correlation, energy, homogeneity as input parameters to a feedforward neural network to predict wood defects. {PSO} is used as a learning algorithm. The {MSE} for training data is found to be 0.3483 and 78.26\% accuracy is achieved for testing data. The proposed technique shows promising results to classify wood defects using a {PSO} trained neural network.},
	eventtitle = {2016 22nd International Conference on Automation and Computing ({ICAC})},
	pages = {273--277},
	booktitle = {2016 22nd International Conference on Automation and Computing ({ICAC})},
	publisher = {{IEEE}},
	author = {Qayyum, R. and Kamal, K. and Zafar, T. and Mathavan, S.},
	urldate = {2024-03-21},
	date = {2016-09},
	langid = {english},
}

@article{hwang_classification_2022,
	title = {Classification of wood knots using artificial neural networks with texture and local feature-based image descriptors},
	volume = {76},
	issn = {0018-3830, 1437-434X},
	url = {https://www.degruyter.com/document/doi/10.1515/hf-2021-0051/html},
	doi = {10.1515/hf-2021-0051},
	abstract = {Abstract
            
              This paper describes feature-based techniques for wood knot classification. For automated classification of macroscopic wood knot images, models were established using artificial neural networks with texture and local feature descriptors, and the performances of feature extraction algorithms were compared. Classification models trained with texture descriptors, gray-level co-occurrence matrix and local binary pattern, achieved better performance than those trained with local feature descriptors, scale-invariant feature transform and dense scale-invariant feature transform. Hence, it was confirmed that wood knot classification was more appropriate for texture classification rather than an approach based on morphological classification. The gray-level co-occurrence matrix produced the highest F1 score despite representing images with relatively low-dimensional feature vectors. The scale-invariant feature transform algorithm could not detect a sufficient number of features from the knot images; hence, the histogram of oriented gradients and dense scale-invariant feature transform algorithms that describe the entire image were better for wood knot classification. The artificial neural network model provided better classification performance than the support vector machine and
              k
              -nearest neighbor models, which suggests the suitability of the nonlinear classification model for wood knot classification.},
	pages = {1--13},
	number = {1},
	journaltitle = {Holzforschung},
	author = {Hwang, Sung-Wook and Lee, Taekyeong and Kim, Hyunbin and Chung, Hyunwoo and Choi, Jong Gyu and Yeo, Hwanmyeong},
	urldate = {2024-03-21},
	date = {2022-01-27},
	langid = {english},
}

@article{kim_visual_2019,
	title = {Visual Classification of Wood Knots Using k-Nearest Neighbor and Convolutional Neural Network},
	volume = {47},
	issn = {1017-0715, 2233-7180},
	url = {http://www.woodj.org/archive/view_article?doi=10.5658/WOOD.2019.47.2.229},
	doi = {10.5658/WOOD.2019.47.2.229},
	abstract = {Various wood defects occur during tree growing or wood processing. Thus, to use wood practically, it is necessary to objectively assess their quality based on the usage requirement by accurately classifying their defects. However, manual visual grading and species classification may result in differences due to subjective decisions; therefore, computer-vision-based image analysis is required for the objective evaluation of wood quality and the speeding up of wood production. In this study, the {SIFT}+k-{NN} and {CNN} models were used to implement a model that automatically classifies knots and analyze its accuracy. Toward this end, a total of 1,172 knot images in various shapes from five domestic conifers were used for learning and validation. For the {SIFT}+k-{NN} model, {SIFT} technology was used to extract properties from the knot images and k-{NN} was used for the classification, resulting in the classification with an accuracy of up to 60.53\% when k-index was 17. The {CNN} model comprised 8 convolution layers and 3 hidden layers, and its maximum accuracy was 88.09\% after 1205 epoch, which was higher than that of the {SIFT}+k-{NN} model. Moreover, if there is a large difference in the number of images by knot types, the {SIFT}+k-{NN} tended to show a learning biased toward the knot type with a higher number of images, whereas the {CNN} model did not show a drastic bias regardless of the difference in the number of images. Therefore, the {CNN} model showed better performance in knot classification. It is determined that the wood knot classification by the {CNN} model will show a sufficient accuracy in its practical applicability.},
	pages = {229--238},
	number = {2},
	journaltitle = {Journal of the Korean Wood Science and Technology},
	author = {Kim, Hyunbin and Kim, Mingyu and Park, Yonggun and Yang, Sang-Yun and Chung, Hyunwoo and Kwon, Ohkyung and Yeo, Hwanmyeong},
	urldate = {2024-03-21},
	date = {2019-03},
	langid = {english},
}

@article{gu_wood_2010,
	title = {Wood defect classification based on image analysis and support vector machines},
	volume = {44},
	issn = {0043-7719, 1432-5225},
	url = {http://link.springer.com/10.1007/s00226-009-0287-9},
	doi = {10.1007/s00226-009-0287-9},
	abstract = {This paper addresses the issue of automatic wood defect classiﬁcation. A tree-structure support vector machine ({SVM}) is proposed to classify four types of wood knots by using images captured from lumber boards. Simple and effective features are proposed and extracted by partitioning the knot images into three distinct areas, followed by utilizing a novel order statistic ﬁlter to yield an average pseudo color feature in each area. Excellent results have been obtained for the proposed {SVM} classiﬁer that is trained by 800 wood knot images. Performance evaluation has shown that the proposed {SVM} classiﬁer resulted in an average classiﬁcation rate of 96.5\% and false alarm rate of 2.25\% over 400 test knot images. Future work will include more extensive tests on large data set and the extension of knot types.},
	pages = {693--704},
	number = {4},
	journaltitle = {Wood Science and Technology},
	shortjournal = {Wood Sci Technol},
	author = {Gu, Irene Yu-Hua and Andersson, Henrik and Vicen, Raul},
	urldate = {2024-03-21},
	date = {2010-11},
	langid = {english},
}

@article{gao_transfer_2021,
	title = {A Transfer Residual Neural Network Based on {ResNet}-34 for Detection of Wood Knot Defects},
	volume = {12},
	issn = {1999-4907},
	url = {https://www.mdpi.com/1999-4907/12/2/212},
	doi = {10.3390/f12020212},
	abstract = {In recent years, due to the shortage of timber resources, it has become necessary to reduce the excessive consumption of forest resources. Non-destructive testing technology can quickly ﬁnd wood defects and effectively improve wood utilization. Deep learning has achieved signiﬁcant results as one of the most commonly used methods in the detection of wood knots. However, compared with convolutional neural networks in other ﬁelds, the depth of deep learning models for the detection of wood knots is still very shallow. This is because the number of samples marked in the wood detection is too small, which limits the accuracy of the ﬁnal prediction of the results. In this paper, {ResNet}-34 is combined with transfer learning, and a new {TL}-{ResNet}34 deep learning model with 35 convolution depths is proposed to detect wood knot defects. Among them, {ResNet}-34 is used as a feature extractor for wood knot defects. At the same time, a new method {TL}-{ResNet}34 is proposed, which combines {ResNet}-34 with transfer learning. After that, the wood knot defect dataset was applied to {TL}-{ResNet}34 for testing. The results show that the detection accuracy of the dataset trained by {TL}-{ResNet}34 is signiﬁcantly higher than that of other methods. This shows that the ﬁnal prediction accuracy of the detection of wood knot defects can be improved by {TL}-{ResNet}34.},
	pages = {212},
	number = {2},
	journaltitle = {Forests},
	shortjournal = {Forests},
	author = {Gao, Mingyu and Qi, Dawei and Mu, Hongbo and Chen, Jianfeng},
	urldate = {2024-03-21},
	date = {2021-02-11},
	langid = {english},
}

@article{chen_robust_2020,
	title = {A robust weakly supervised learning of deep Conv-Nets for surface defect inspection},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-020-04819-5},
	doi = {10.1007/s00521-020-04819-5},
	abstract = {Automatic defect detection is a challenging task owing to the complex textured background with non-uniform intensity distribution, weak differences between defects and background, diversity of defect types, and high cost of annotated samples. In order to solve these challenges, this paper proposes a novel end-to-end defect classiﬁcation and segmentation framework based on weakly supervised learning of a convolutional neural network ({CNN}) with attention architecture. Firstly, a novel end-to-end {CNN} architecture integrating the robust classiﬁer and spatial attention module is proposed to enhance defect feature representation ability, which signiﬁcantly improves the classiﬁcation accuracy. Secondly, a new spatial attention class activation map ({SA}-{CAM}) is proposed to improve segmentation adaptability by generating more accurate heatmap. Moreover, for different surface texture, {SA}-{CAM} can signiﬁcantly suppress the background’s inference and highlight defect area. Finally, the proposed weakly supervised learning framework is trained using only global image labels and devoted to two main visual recognition tasks: defect samples classiﬁcation and area segmentation. At the same time, it is robust to complex backgrounds. Results of the experiments verify the generalization of the proposed method on three distinct datasets with different kinds of textures and backgrounds. In the classiﬁcation tasks, the proposed method improves accuracy by 0.66–25.50\%. In the segmentation tasks, the proposed method improves accuracy by 5.49–7.07\%.},
	pages = {11229--11244},
	number = {15},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Chen, Haiyong and Hu, Qidi and Zhai, Baoshuo and Chen, He and Liu, Kun},
	urldate = {2024-03-18},
	date = {2020-08},
	langid = {english},
}

@article{zheng_gbcd-yolo_2024,
	title = {{GBCD}-{YOLO}: A High-Precision and Real-Time Lightweight Model for Wood Defect Detection},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10409188/},
	doi = {10.1109/ACCESS.2024.3356048},
	shorttitle = {{GBCD}-{YOLO}},
	abstract = {With the advancement of the wood processing industry, the demand for the detection of surface defects in wood has become increasingly urgent. The application of automated production technology has enhanced the efficiency and precision of wood processing, which can significantly impact product quality and competitiveness. However, current methods for detecting surface defects in wood suffer from issues such as low detection accuracy, high computational complexity, and poor real-time performance. In response to these challenges, this paper proposes a high-precision, lightweight, real-time wood surface defect detection method based on {YOLO}({GBCD}-{YOLO}) model. Firstly, the Ghost Bottleneck is introduced to improve the computational efficiency and inference speed of deep neural networks. Furthermore, the {BiFormer} is incorporated in the neck to enhance the performance of natural language processing tasks. Simultaneously, {CARAFE} is utilized as an upsampling replacement to enhance perceptual and capture abilities for details. In addition, the Dynamic Head is introduced to enhance the method’s flexibility and generalization ability, and the loss function is replaced with complete intersection over union ({CIoU}). The proposed method was evaluated using an optimized dataset and the {YOLOv}5s model was chosen as the baseline. The experimental results show that compared with the original {YOLOv}5s, the {mAP} (0.5) has been improved by 13.45\%, reaching 88.72\%. The {mAP} (0.5:0.95) increased by 11.95\%, and {FPS} increased by 6.25\%. In addition, the parameter of the improved model has been reduced by 15.49\%. These results indicate that the proposed {GBCD}-{YOLO} improves the real-time detection performance of wood surface defects.},
	pages = {12853--12868},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Zheng, Yunchang and Wang, Mengfan and Zhang, Bo and Shi, Xiangnan and Chang, Qing},
	urldate = {2024-03-18},
	date = {2024},
	langid = {english},
}

@article{mohan_hybrid_2005,
	title = {{HYBRID} {OPTIMIZATION} {FOR} {CLASSIFICATION} {OF} {THE} {WOOD} {KNOTS}},
	volume = {63},
	abstract = {Knots are common wood defects. A knot is a specific imperfection in timber, reducing its strength which can be exploited for artistic effect, resulting in knot selection being an important matter in the wood industry. The value of wood is related to its quality, and this in turn is determined by defect numbers and distribution. This is challenging as in some instances, selection/classification is manual. In this paper, it is proposed to detect and classify the knots in timber boards. Hilbert transforms and Gabor filters are used for pre-processing the image of knots. The features obtained from preprocessing were classified using Multi Layer Perceptron ({MLP}) and Neural Network ({NN}) with Particle Swarm Optimization ({PSO}) and Invasive Weed Optimization ({IWO}) for momentum and learning rate.},
	journaltitle = {. Vol.},
	author = {Mohan, S and Venkatachalapathy, K and Sudhakar, P},
	date = {2005},
	langid = {english},
}

@article{ge_wood_2023,
	title = {Wood Veneer Defect Detection Based on Multiscale {DETR} with Position Encoder Net},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/10/4837},
	doi = {10.3390/s23104837},
	abstract = {Wood is one of the main building materials. However, defects on veneers result in substantial waste of wood resources. Traditional veneer defect detection relies on manual experience or photoelectric-based methods, which are either subjective and inefﬁcient or need substantial investment. Computer vision-based object detection methods have been used in many realistic areas. This paper proposes a new deep learning defect detection pipeline. First, an image collection device is constructed and a total of more than 16,380 defect images are collected coupled with a mixed data augmentation method. Then, a detection pipeline is designed based on {DEtection} {TRansformer} ({DETR}). The original {DETR} needs position encoding functions to be designed and is ineffective for small object detection. To solve these problems, a position encoding net is designed with multiscale feature maps. The loss function is also redeﬁned for much more stable training. The results from the defect dataset show that using a light feature mapping network, the proposed method is much faster with similar accuracy. Using a complex feature mapping network, the proposed method is much more accurate with similar speed.},
	pages = {4837},
	number = {10},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Ge, Yilin and Jiang, Dapeng and Sun, Liping},
	urldate = {2024-03-18},
	date = {2023-05-17},
	langid = {english},
}

@article{xiang_improved_2022,
	title = {An Improved {YOLOv}5 Crack Detection Method Combined With Transformer},
	volume = {22},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {https://ieeexplore.ieee.org/document/9794770/},
	doi = {10.1109/JSEN.2022.3181003},
	abstract = {Efﬁcient detection of pavement cracks can effectively prevent trafﬁc accidents and reduce pavement maintenance costs. In order to overcome the complicated and uneconomical disadvantages of traditional crack detection methods, this paper introduces a pavement crack detection network based on deep learning, which can automatically detect pavement cracks and achieves excellent detection accuracy. And the network can easily use the sensors to collect data to facilitate industrial applications. In additional, considering that most cracks have slim feature, we apply the latest Transformer module in the network to improve the effect of cracks detection. Transformer has a strong ability to capture the long-range dependence of the cracks, which enables the network to learn the context information of the crack region. Furthermore, the network also utilizes some techniques to improve the ability of algorithm to detect various cracks. Our network is trained on pavement data sets containing India, the Czech Republic and Japan. It achieved F1 scores of 0.6739 and 0.6650 on two online test sets with fewer network parameters.},
	pages = {14328--14335},
	number = {14},
	journaltitle = {{IEEE} Sensors Journal},
	shortjournal = {{IEEE} Sensors J.},
	author = {Xiang, Xuezhi and Wang, Zhiyuan and Qiao, Yulong},
	urldate = {2024-03-18},
	date = {2022-07-15},
	langid = {english},
}

@article{mekhalfi_contrasting_2022,
	title = {Contrasting {YOLOv}5, Transformer, and {EfficientDet} Detectors for Crop Circle Detection in Desert},
	volume = {19},
	issn = {1545-598X, 1558-0571},
	url = {https://ieeexplore.ieee.org/document/9453822/},
	doi = {10.1109/LGRS.2021.3085139},
	abstract = {Ongoing discoveries of water reserves have fostered an increasing adoption of crop circles in the desert in several countries. Automatically quantifying and surveying the layout of crop circles in remote areas can be of great use for stakeholders in managing the expansion of the farming land. This letter compares latest deep learning models for crop circle detection and counting, namely Detection Transformers, {EfﬁcientDet} and {YOLOv}5 are evaluated. To this end, we build two datasets, via Google Earth Pro, corresponding to two large crop circle hot spots in Egypt and Saudi Arabia. The images were drawn at an altitude of 20 km above the targets. The models are assessed in within-domain and cross-domain scenarios, and yielded plausible detection potential and inference response.},
	pages = {1--5},
	journaltitle = {{IEEE} Geoscience and Remote Sensing Letters},
	shortjournal = {{IEEE} Geosci. Remote Sensing Lett.},
	author = {Mekhalfi, Mohamed Lamine and Nicolo, Carlo and Bazi, Yakoub and Rahhal, Mohamad Mahmoud Al and Alsharif, Norah A. and Maghayreh, Eslam Al},
	urldate = {2024-03-18},
	date = {2022},
	langid = {english},
}

@inproceedings{li_mask_2023,
	location = {Vancouver, {BC}, Canada},
	title = {Mask {DINO}: Towards A Unified Transformer-based Framework for Object Detection and Segmentation},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10204168/},
	doi = {10.1109/CVPR52729.2023.00297},
	shorttitle = {Mask {DINO}},
	abstract = {In this paper we present Mask {DINO}, a unified object detection and segmentation framework. Mask {DINO} extends {DINO} ({DETR} with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from {DINO} to dotproduct a high-resolution pixel embedding map to predict a set of binary masks. Some key components in {DINO} are extended for segmentation through a shared architecture and training process. Mask {DINO} is simple, efficient, and scalable, and it can benefit from joint large-scale detection and segmentation datasets. Our experiments show that Mask {DINO} significantly outperforms all existing specialized segmentation methods, both on a {ResNet}-50 backbone and a pre-trained model with {SwinL} backbone. Notably, Mask {DINO} establishes the best results to date on instance segmentation (54.5 {AP} on {COCO}), panoptic segmentation (59.4 {PQ} on {COCO}), and semantic segmentation (60.8 {mIoU} on {ADE}20K) among models under one billion parameters. Code is available at https://github.com/{IDEAResearch}/{MaskDINO}.},
	eventtitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {3041--3050},
	booktitle = {2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Li, Feng and Zhang, Hao and Xu, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M. and Shum, Heung-Yeung},
	urldate = {2024-03-18},
	date = {2023-06},
	langid = {english},
}

@inproceedings{ding_learning_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {Learning {RoI} Transformer for Oriented Object Detection in Aerial Images},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953881/},
	doi = {10.1109/CVPR.2019.00296},
	abstract = {Object detection in aerial images is an active yet challenging task in computer vision because of the bird’s-eye view perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely packed objects in aerial images, methods relying on horizontal proposals for common object detection often introduce mismatches between the Region of Interests ({RoIs}) and objects. This leads to the common misalignment between the ﬁnal object classiﬁcation conﬁdence and localization accuracy. In this paper, we propose a {RoI} Transformer to address these problems. The core idea of {RoI} Transformer is to apply spatial transformations on {RoIs} and learn the transformation parameters under the supervision of oriented bounding box ({OBB}) annotations. {RoI} Transformer is with lightweight and can be easily embedded into detectors for oriented object detection. Simply apply the {RoI} Transformer to light-head {RCNN} has achieved state-of-the-art performances on two common and challenging aerial datasets, i.e., {DOTA} and {HRSC}2016, with a neglectable reduction to detection speed. Our {RoI} Transformer exceeds the deformable Position Sensitive {RoI} pooling when oriented bounding-box annotations are available. Extensive experiments have also validated the ﬂexibility and effectiveness of our {RoI} Transformer.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2844--2853},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Ding, Jian and Xue, Nan and Long, Yang and Xia, Gui-Song and Lu, Qikai},
	urldate = {2024-03-18},
	date = {2019-06},
	langid = {english},
}

@article{wang_anchor_2022,
	title = {Anchor {DETR}: Query Design for Transformer-Based Detector},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20158},
	doi = {10.1609/aaai.v36i3.20158},
	shorttitle = {Anchor {DETR}},
	abstract = {In this paper, we propose a novel query design for the transformer-based object detection. In previous transformerbased detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on. It is difﬁcult to optimize as the prediction slot of each object query does not have a speciﬁc mode. In other words, each object query will not focus on a speciﬁc region. To solve these problems, in our query design, object queries are based on anchor points, which are widely used in {CNN}-based detectors. So each object query focuses on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difﬁculty: “one region, multiple objects”. In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in {DETR}. Thanks to the query design and the attention variant, the proposed detector that we called Anchor {DETR}, can achieve better performance and run faster than the {DETR} with 10× fewer training epochs. For example, it achieves 44.2 {AP} with 19 {FPS} on the {MSCOCO} dataset when using the {ResNet}50-{DC}5 feature for training 50 epochs. Extensive experiments on the {MSCOCO} benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-research/{AnchorDETR}.},
	pages = {2567--2575},
	number = {3},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Wang, Yingming and Zhang, Xiangyu and Yang, Tong and Sun, Jian},
	urldate = {2024-03-18},
	date = {2022-06-28},
	langid = {english},
}

@misc{zheng_end--end_2021,
	title = {End-to-End Object Detection with Adaptive Clustering Transformer},
	url = {http://arxiv.org/abs/2011.09315},
	abstract = {End-to-end Object Detection with Transformer ({DETR})proposes to perform object detection with Transformer and achieve comparable performance with two-stage object detection like Faster-{RCNN}. However, {DETR} needs huge computational resources for training and inference due to the high-resolution spatial input. In this paper, a novel variant of transformer named Adaptive Clustering Transformer({ACT}) has been proposed to reduce the computation cost for high-resolution input. {ACT} cluster the query features adaptively using Locality Sensitive Hashing ({LSH}) and ap-proximate the query-key interaction using the prototype-key interaction. {ACT} can reduce the quadratic O(N2) complexity inside self-attention into O({NK}) where K is the number of prototypes in each layer. {ACT} can be a drop-in module replacing the original self-attention module without any training. {ACT} achieves a good balance between accuracy and computation cost ({FLOPs}). The code is available as supplementary for the ease of experiment replication and verification. Code is released at {\textbackslash}url\{https://github.com/gaopengcuhk/{SMCA}-{DETR}/\}},
	number = {{arXiv}:2011.09315},
	publisher = {{arXiv}},
	author = {Zheng, Minghang and Gao, Peng and Zhang, Renrui and Li, Kunchang and Wang, Xiaogang and Li, Hongsheng and Dong, Hao},
	urldate = {2024-03-18},
	date = {2021-10-18},
	eprinttype = {arxiv},
	eprint = {2011.09315 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{avidan_exploring_2022,
	location = {Cham},
	title = {Exploring Plain Vision Transformer Backbones for Object Detection},
	volume = {13669},
	isbn = {978-3-031-20076-2 978-3-031-20077-9},
	url = {https://link.springer.com/10.1007/978-3-031-20077-9_17},
	abstract = {We explore the plain, non-hierarchical Vision Transformer ({ViT}) as a backbone network for object detection. This design enables the original {ViT} architecture to be ﬁne-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for ﬁne-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is suﬃcient to build a simple feature pyramid from a single-scale feature map (without the common {FPN} design) and (ii) it is suﬃcient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain {ViT} backbones pre-trained as Masked Autoencoders ({MAE}), our detector, named {ViTDet}, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 {APbox} on the {COCO} dataset using only {ImageNet}-1K pretraining. We hope our study will draw attention to research on plainbackbone detectors. Code for {ViTDet} is available (https://github.com/ facebookresearch/detectron2/tree/main/projects/{ViTDet}).},
	pages = {280--296},
	booktitle = {Computer Vision – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	urldate = {2024-03-18},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-20077-9_17},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{sun_rethinking_2021,
	location = {Montreal, {QC}, Canada},
	title = {Rethinking Transformer-based Set Prediction for Object Detection},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710041/},
	doi = {10.1109/ICCV48922.2021.00359},
	abstract = {{DETR} is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difﬁculty in the training of {DETR}. Our examinations reveal several factors contributing to the slow convergence of {DETR}, primarily the issues with the Hungarian loss and the Transformer crossattention mechanism. To overcome these issues we propose two solutions, namely, {TSP}-{FCOS} (Transformer-based Set Prediction with {FCOS}) and {TSP}-{RCNN} (Transformerbased Set Prediction with {RCNN}). Experimental results show that the proposed methods not only converge much faster than the original {DETR}, but also signiﬁcantly outperform {DETR} and other baselines in terms of detection accuracy. Code is released at https://github.com/ Edward-Sun/{TSP}-Detection.},
	eventtitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {3591--3600},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris},
	urldate = {2024-03-18},
	date = {2021-10},
	langid = {english},
}

@inproceedings{misra_end--end_2021,
	location = {Montreal, {QC}, Canada},
	title = {An End-to-End Transformer Model for 3D Object Detection},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711345/},
	doi = {10.1109/ICCV48922.2021.00290},
	abstract = {We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3Dspeciﬁc inductive biases, 3DETR requires minimal modiﬁcations to the vanilla Transformer block. Speciﬁcally, we ﬁnd that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3Dspeciﬁc operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized {VoteNet} baselines on the challenging {ScanNetV}2 dataset by 9.5\%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.},
	eventtitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {2886--2897},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
	urldate = {2024-03-18},
	date = {2021-10},
	langid = {english},
}

@article{mao_voxel_nodate,
	title = {Voxel Transformer for 3D Object Detection},
	abstract = {We present Voxel Transformer ({VoTr}), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efﬁciently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive ﬁelds. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables longrange relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. {VoTr} contains a series of sparse and submanifold voxel modules, and can be applied in most voxel-based detectors. Our proposed {VoTr} shows consistent improvement over the convolutional baselines while maintaining computational efﬁciency on the {KITTI} dataset and the Waymo Open dataset.},
	author = {Mao, Jiageng and Xue, Yujing and Niu, Minzhe and Bai, Haoyue and Feng, Jiashi and Liang, Xiaodan and Xu, Hang and Xu, Chunjing},
	langid = {english},
}

@misc{beal_toward_2020,
	title = {Toward Transformer-Based Object Detection},
	url = {http://arxiv.org/abs/2012.09958},
	abstract = {Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive {COCO} results. The model that we propose, {ViT}-{FRCNN}, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view {ViT}-{FRCNN} as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.},
	number = {{arXiv}:2012.09958},
	publisher = {{arXiv}},
	author = {Beal, Josh and Kim, Eric and Tzeng, Eric and Park, Dong Huk and Zhai, Andrew and Kislyuk, Dmitry},
	urldate = {2024-03-18},
	date = {2020-12-17},
	eprinttype = {arxiv},
	eprint = {2012.09958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2024-03-17},
	date = {2017},
}

@article{parthasaradhi_time-series_2005,
	title = {Time-Series Detection of Perspiration as a Liveness Test in Fingerprint Devices},
	volume = {35},
	issn = {1094-6977},
	url = {http://ieeexplore.ieee.org/document/1487582/},
	doi = {10.1109/TSMCC.2005.848192},
	abstract = {Fingerprint scanners may be susceptible to spooﬁng using artiﬁcial materials, or in the worst case, dismembered ﬁngers. An anti-spooﬁng method based on liveness detection has been developed for use in ﬁngerprint scanners. This method quantiﬁes a speciﬁc temporal perspiration pattern present in ﬁngerprints acquired from live claimants. The enhanced perspiration detection algorithm presented here improves our previous work by including other ﬁngerprint scanner technologies; using a larger, more diverse data set; and a shorter time window. Several classiﬁcation methods were tested in order to separate live and spoof ﬁngerprint images. The dataset included ﬁngerprint images from 33 live subjects, 33 spoofs created with dental material and Play-Doh, and fourteen cadaver ﬁngers. Each method had a different performance with respect to each scanner and time window. However, all the classiﬁers achieved approximately 90\% classiﬁcation rate for all scanners, using the reduced time window and the more comprehensive training and test sets.},
	pages = {335--343},
	number = {3},
	journaltitle = {{IEEE} Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
	shortjournal = {{IEEE} Trans. Syst., Man, Cybern. C},
	author = {Parthasaradhi, S.T.V. and Derakhshani, R. and Hornak, L.A. and Schuckers, S.A.C.},
	urldate = {2024-03-17},
	date = {2005-08},
	langid = {english},
}

@inproceedings{li_enhancing_2019,
	title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html},
	abstract = {Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot- product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose {LogSparse} Transformer with only O(L(log L){\textasciicircum}2) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real- world datasets show that it compares favorably to the state-of-the-art.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
	urldate = {2024-03-17},
	date = {2019},
}

@misc{wu_deep_2020,
	title = {Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case},
	url = {http://arxiv.org/abs/2001.08317},
	shorttitle = {Deep Transformer Models for Time Series Forecasting},
	abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness ({ILI}) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
	number = {{arXiv}:2001.08317},
	publisher = {{arXiv}},
	author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
	urldate = {2024-03-17},
	date = {2020-01-22},
	eprinttype = {arxiv},
	eprint = {2001.08317 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhou_informer_2021,
	title = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
	doi = {10.1609/aaai.v35i12.17325},
	shorttitle = {Informer},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting ({LSTF}) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efﬁciently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to {LSTF}, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efﬁcient transformer-based model for {LSTF}, named Informer, with three distinctive characteristics: (i) a {ProbSparse} self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences’ dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efﬁciently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer signiﬁcantly outperforms existing methods and provides a new solution to the {LSTF} problem.},
	pages = {11106--11115},
	number = {12},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	urldate = {2024-03-17},
	date = {2021-05-18},
	langid = {english},
}

@inproceedings{zerveas_transformer-based_2021,
	location = {Virtual Event Singapore},
	title = {A Transformer-based Framework for Multivariate Time Series Representation Learning},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467401},
	doi = {10.1145/3447548.3467401},
	eventtitle = {{KDD} '21: The 27th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	pages = {2114--2124},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
	urldate = {2024-03-17},
	date = {2021-08-14},
	langid = {english},
}

@misc{wen_transformers_2023,
	title = {Transformers in Time Series: A Survey},
	url = {http://arxiv.org/abs/2202.07125},
	shorttitle = {Transformers in Time Series},
	abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
	number = {{arXiv}:2202.07125},
	publisher = {{arXiv}},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	urldate = {2024-03-17},
	date = {2023-05-11},
	eprinttype = {arxiv},
	eprint = {2202.07125 [cs, eess, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@article{xu_vision_2020,
	title = {Vision Measurement of Tunnel Structures with Robust Modelling and Deep Learning Algorithms},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/17/4945},
	doi = {10.3390/s20174945},
	abstract = {The health monitoring of tunnel structures is vital to the safe operation of railway transportation systems. With the increasing mileage of tunnels, regular inspection and health monitoring are urgently demanded for the tunnel structures, especially for information regarding deformation and damage. However, traditional methods of tunnel inspection are time-consuming, expensive and highly dependent on human subjectivity. In this paper, an automatic tunnel monitoring method is investigated based on image data which is collected through the moving vision measurement unit consisting of camera array. Furthermore, geometric modelling and crack inspection algorithms are proposed where a robust three-dimensional tunnel model is reconstructed utilizing a B-spline method and crack identiﬁcation is conducted by means of a Mask R-{CNN} network. The innovation of this investigation is that we combine the robust modelling which could be applied for the deformation analysis and the crack detection where a deep learning method is employed to recognize the tunnel cracks intelligently based on image sensors. In this study, experiments were conducted on a subway tunnel structure several kilometers long, and a robust three-dimensional model is generated and the cracks are identiﬁed automatically with the image data. The superiority of this proposal is that the comprehensive information of geometry deformation and crack damage can ensure the reliability and improve the accuracy of health monitoring.},
	pages = {4945},
	number = {17},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Xu, Xiangyang and Yang, Hao},
	urldate = {2024-03-15},
	date = {2020-09-01},
	langid = {english},
}

@article{minh_trieu_anthropometric_2023,
	title = {The Anthropometric Measurement of Nasal Landmark Locations by Digital 2D Photogrammetry Using the Convolutional Neural Network},
	volume = {13},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/13/5/891},
	doi = {10.3390/diagnostics13050891},
	abstract = {Measuring and labeling human face landmarks are time-consuming jobs that are conducted by experts. Currently, the applications of the Convolutional Neural Network ({CNN}) for image segmentation and classiﬁcation have made great progress. The nose is arguably one of the most attractive parts of the human face. Rhinoplasty surgery is increasingly performed in females and also in males since surgery can help to enhance patient satisfaction with the resulting perceived beautiful ratio following the neoclassical proportions. In this study, the {CNN} model is introduced to extract facial landmarks based on medical theories: it learns the landmarks and recognizes them based on feature extraction during training. The comparison between experiments has proved that the {CNN} model can detect landmarks depending on desired requirements. Anthropometric measurements are carried out by automatic measurement divided into three images with frontal, lateral, and mental views. Measurements are performed including 12 linear distances and 10 angles. The results of the study were evaluated as satisfactory with a normalized mean error ({NME}) of 1.05, an average error for linear measurements of 0.508 mm, and 0.498◦ for angle measurements. Through its results, this study proposed a low-cost automatic anthropometric measurement system with high accuracy and stability.},
	pages = {891},
	number = {5},
	journaltitle = {Diagnostics},
	shortjournal = {Diagnostics},
	author = {Minh Trieu, Nguyen and Truong Thinh, Nguyen},
	urldate = {2024-03-15},
	date = {2023-02-26},
	langid = {english},
}

@article{braun_combining_2019,
	title = {Combining inverse photogrammetry and {BIM} for automated labeling of construction site images for machine learning},
	volume = {106},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580519300329},
	doi = {10.1016/j.autcon.2019.102879},
	abstract = {Image-based object detection provides a valuable basis for site information retrieval and construction progress monitoring. Machine learning approaches, such as neural networks, are able to provide reliable detection rates. However, labeling of training data is a tedious and time-consuming process, as it must be performed manually for a substantial number of images. The paper presents a novel method for automatically labeling construction images based on the combination of 4D Building Information Models and an inverse photogrammetry approach. For the reconstruction of point clouds, which are often used for progress monitoring, a large number of pictures are taken from the site. By aligning the Building Information Model and the resulting point cloud, it is possible to project any building element of the {BIM} model into the acquired pictures. This allows for automated labeling as the semantic information of the element type is provided by the {BIM} model and can be associated with the respective regions. The labeled data can subsequently be used to train an image-based neural network. Since the exact regions for all elements are deﬁned, labels can be generated for basic tasks like classiﬁcation as well as more complex tasks like semantic segmentation. To prove the feasibility of the developed methods, the labeling procedure is applied to several real-world construction sites, providing over 30,000 automatically labeled elements. The correctness of the assigned labels has been validated by pixel based area comparison against manual labels.},
	pages = {102879},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Braun, Alex and Borrmann, André},
	urldate = {2024-03-15},
	date = {2019-10},
	langid = {english},
}

@article{beckman_deep_2019,
	title = {Deep learning-based automatic volumetric damage quantification using depth camera},
	volume = {99},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580518308690},
	doi = {10.1016/j.autcon.2018.12.006},
	abstract = {A depth camera or 3-dimensional scanner was used as a sensor for traditional methods to quantify the identified concrete spalling damage in terms of volume. However, to quantify the concrete spalling damage automatically, the first step is to detect (i.e., identify) the concrete spalling. The multiple spots of spalling can be possible within a single structural element or in multiple structural elements. However, there is, as of yet, no method to detect concrete spalling automatically using deep learning methods. Therefore, in this paper, a faster region-based convolutional neural network (Faster R-{CNN})-based concrete spalling damage detection method is proposed with an inexpensive depth sensor to quantify multiple instances of spalling simultaneously in the same surface separately and consider multiple surfaces in structural elements. A database composed of 1091 images (with 853 × 1440 pixels) labeled for volumetric damage is developed, and the deep learning network is then modified, trained, and validated using the proposed database. The damage quantification is automatically performed by processing the depth data, identifying surfaces, and isolating the damage after merging the output from the Faster R-{CNN} with the depth stream of the sensor. The trained Faster R-{CNN} presented an average precision ({AP}) of 90.79\%. Volume quantifications show a mean precision error ({MPE}) of 9.45\% when considering distances from 100 cm to 250 cm between the element and the sensor. Also, an {MPE} of 3.24\% was obtained for maximum damage depth measurements across the same distance range.},
	pages = {114--124},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Beckman, Gustavo H. and Polyzois, Dimos and Cha, Young-Jin},
	urldate = {2024-03-15},
	date = {2019-03},
	langid = {english},
}

@article{kaashki_deep_2021,
	title = {Deep Learning-Based Automated Extraction of Anthropometric Measurements From a Single 3-D Scan},
	volume = {70},
	issn = {0018-9456, 1557-9662},
	url = {https://ieeexplore.ieee.org/document/9517270/},
	doi = {10.1109/TIM.2021.3106126},
	abstract = {The appearance of 3-D scanners, generating point clouds, has revolutionized anthropometric data collection systems and their applications. Anthropometric data are of paramount importance in several applications, including fashion design, medical diagnosis, and virtual character modeling, all of which require a fully automatic anthropometric measurement extraction method. 3-D-based methods for anthropometric measurement extraction becomes more and more popular due to their improved accuracy compared to classical image-based approaches. Existing 3-D methods can be mainly classiﬁed into two categories: landmark and template-based methods. The former is highly dependent on the estimated landmarks which are highly sensitive to noise in the input or missing data. The latter has to iteratively solve an objective function to deform a body template to ﬁt the scan, which is time-consuming while being also sensitive to noise and missing data. In this study, we propose the ﬁrst approach for automatic contact-less anthropometric measurements extraction based on deep-learning ({AM}-{DL}). A novel module dubbed multiscale {EdgeConv} is proposed to learn local features from point clouds at multiple scales. Multiscale {EdgeConv} can be directly integrated with other neural networks for various tasks, e.g., classiﬁcation of point clouds. We exploit this module to design an encoder–decoder architecture that learns to deform a template model to ﬁt a given scan. The measurement values are then calculated on the deformed template model. To evaluate the proposed method, 27 female and 25 male subjects were scanned using a photogrametry-based scanner and measured by an experienced tailor. Experimental results on the synthetic {ModelNet}40 dataset and on the real scans demonstrate that the proposed method outperforms state-of-the-art methods, and performs sufﬁciently close to a professional tailor.},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Instrumentation and Measurement},
	shortjournal = {{IEEE} Trans. Instrum. Meas.},
	author = {Kaashki, Nastaran Nourbakhsh and Hu, Pengpeng and Munteanu, Adrian},
	urldate = {2024-03-15},
	date = {2021},
	langid = {english},
}

@article{belloni_tack_2020,
	title = {{TACK} {PROJECT}: {TUNNEL} {AND} {BRIDGE} {AUTOMATIC} {CRACK} {MONITORING} {USING} {DEEP} {LEARNING} {AND} {PHOTOGRAMMETRY}},
	volume = {{XLIII}-B4-2020},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/741/2020/isprs-archives-XLIII-B4-2020-741-2020.html},
	doi = {10.5194/isprs-archives-XLIII-B4-2020-741-2020},
	shorttitle = {{TACK} {PROJECT}},
	abstract = {Civil infrastructures, such as tunnels and bridges, are directly related to the overall economic and demographic growth of countries. The aging of these infrastructures increases the probability of catastrophic failures that results in loss of lives and high repair costs; all over the world, these factors drive the need for advanced infrastructure monitoring systems. For these reasons, in the last years, different types of devices and innovative infrastructure monitoring techniques have been investigated to automate the process and overcome the main limitation of standard visual inspections that are used nowadays. This paper presents some preliminary findings of an ongoing research project, named {TACK}, that combines advanced deep learning techniques and innovative photogrammetric algorithms to develop a monitoring system. Specifically, the project focuses on the development of an automatic procedure for crack detection and measurement using images of tunnels and bridges acquired with a mobile mapping system. In this paper, some preliminary results are shown to investigate the potential of a deep learning algorithm in detecting cracks occurred in concrete material. The model is a {CNN} (Convolutional Neural Network) based on the U-Net architecture; in this study, we tested the transferability of the model that has been trained on a small available labeled dataset and tested on a large set of images acquired using a customized mobile mapping system. The results have shown that it is possible to effectively detect cracks in unseen imagery and that the primary source of errors is the false positive detection of crack-like objects (i.e., contact wires, cables and tile borders).},
	pages = {741--745},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Belloni, V. and Sjölander, A. and Ravanelli, R. and Crespi, M. and Nascetti, A.},
	urldate = {2024-03-15},
	date = {2020-08-25},
	note = {Conference Name: {XXIV} {ISPRS} Congress, Commission {IV} (Volume {XLIII}-B4-2020) - 2020 edition
Publisher: Copernicus {GmbH}},
	keywords = {Crack detection, Deep learning, Infrastructure monitoring, Photogrammetry},
}

@inproceedings{karras_training_2020,
	title = {Training Generative Adversarial Networks with Limited Data},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html},
	abstract = {Training generative adversarial networks ({GAN}) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing {GAN} on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching {StyleGAN}2 results with an order of magnitude fewer images. We expect this to open up new application domains for {GANs}. We also find that the widely used {CIFAR}-10 is, in fact, a limited data benchmark, and improve the record {FID} from 5.59 to 2.42.},
	pages = {12104--12114},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	urldate = {2024-03-05},
	date = {2020},
}

@inproceedings{jiang_transgan_2021,
	title = {{TransGAN}: Two Pure Transformers Can Make One Strong {GAN}, and That Can Scale Up},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/7c220a2091c26a7f5e9f1cfb099511e3-Abstract.html},
	shorttitle = {{TransGAN}},
	pages = {14745--14758},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	urldate = {2024-03-13},
	date = {2021},
}

@inproceedings{zhang_styleswin_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {{StyleSwin}: Transformer-based {GAN} for High-resolution Image Generation},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880033/},
	doi = {10.1109/CVPR52688.2022.01102},
	shorttitle = {{StyleSwin}},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {11294--11304},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Zhang, Bowen and Gu, Shuyang and Zhang, Bo and Bao, Jianmin and Chen, Dong and Wen, Fang and Wang, Yong and Guo, Baining},
	urldate = {2024-03-13},
	date = {2022-06},
	langid = {english},
}

@misc{yang_lr-gan_2017,
	title = {{LR}-{GAN}: Layered Recursive Generative Adversarial Networks for Image Generation},
	url = {http://arxiv.org/abs/1703.01560},
	shorttitle = {{LR}-{GAN}},
	abstract = {We present {LR}-{GAN}: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks ({GANs}), the proposed {GAN} learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that {LR}-{GAN} can generate more natural images with objects that are more human recognizable than {DCGAN}.},
	number = {{arXiv}:1703.01560},
	publisher = {{arXiv}},
	author = {Yang, Jianwei and Kannan, Anitha and Batra, Dhruv and Parikh, Devi},
	urldate = {2024-03-13},
	date = {2017-08-01},
	eprinttype = {arxiv},
	eprint = {1703.01560 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{waheed_covidgan_2020,
	title = {{CovidGAN}: Data Augmentation Using Auxiliary Classifier {GAN} for Improved Covid-19 Detection},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9093842/},
	doi = {10.1109/ACCESS.2020.2994762},
	shorttitle = {{CovidGAN}},
	abstract = {Coronavirus ({COVID}-19) is a viral disease caused by severe acute respiratory syndrome coronavirus 2 ({SARS}-{CoV}-2). The spread of {COVID}-19 seems to have a detrimental effect on the global economy and health. A positive chest X-ray of infected patients is a crucial step in the battle against {COVID}-19. Early results suggest that abnormalities exist in chest X-rays of patients suggestive of {COVID}-19. This has led to the introduction of a variety of deep learning systems and studies have shown that the accuracy of {COVID}-19 patient detection through the use of chest X-rays is strongly optimistic. Deep learning networks like convolutional neural networks ({CNNs}) need a substantial amount of training data. Because the outbreak is recent, it is difﬁcult to gather a signiﬁcant number of radiographic images in such a short time. Therefore, in this research, we present a method to generate synthetic chest X-ray ({CXR}) images by developing an Auxiliary Classiﬁer Generative Adversarial Network ({ACGAN}) based model called {CovidGAN}. In addition, we demonstrate that the synthetic images produced from {CovidGAN} can be utilized to enhance the performance of {CNN} for {COVID}-19 detection. Classiﬁcation using {CNN} alone yielded 85\% accuracy. By adding synthetic images produced by {CovidGAN},the accuracy increased to 95\%. We hope this method will speed up {COVID}-19 detection and lead to more robust systems of radiology.},
	pages = {91916--91923},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Waheed, Abdul and Goyal, Muskan and Gupta, Deepak and Khanna, Ashish and Al-Turjman, Fadi and Pinheiro, Placido Rogerio},
	urldate = {2024-03-13},
	date = {2020},
	langid = {english},
}

@inproceedings{bao_cvae-gan_2017,
	location = {Venice},
	title = {{CVAE}-{GAN}: Fine-Grained Image Generation through Asymmetric Training},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237561/},
	doi = {10.1109/ICCV.2017.299},
	shorttitle = {{CVAE}-{GAN}},
	abstract = {We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in ﬁne-grained categories, such as faces of a speciﬁc person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the ﬁne-grained category label fed into the resulting generative model, we can generate images in a speciﬁc category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classiﬁer network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the {GAN} training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, ﬂowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with ﬁne-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2764--2773},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	urldate = {2024-03-13},
	date = {2017-10},
	langid = {english},
}

@article{abedi_gan-based_2022,
	title = {{GAN}-Based Approaches for Generating Structured Data in the Medical Domain},
	volume = {12},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/14/7075},
	doi = {10.3390/app12147075},
	abstract = {Modern machine and deep learning methods require large datasets to achieve reliable and robust results. This requirement is often difficult to meet in the medical field, due to data sharing limitations imposed by privacy regulations or the presence of a small number of patients (e.g., rare diseases). To address this data scarcity and to improve the situation, novel generative models such as Generative Adversarial Networks ({GANs}) have been widely used to generate synthetic data that mimic real data by representing features that reflect health-related information without reference to real patients. In this paper, we consider several {GAN} models to generate synthetic data used for training binary (malignant/benign) classifiers, and compare their performances in terms of classification accuracy with cases where only real data are considered. We aim to investigate how synthetic data can improve classification accuracy, especially when a small amount of data is available. To this end, we have developed and implemented an evaluation framework where binary classifiers are trained on extended datasets containing both real and synthetic data. The results show improved accuracy for classifiers trained with generated data from more advanced {GAN} models, even when limited amounts of original data are available.},
	pages = {7075},
	number = {14},
	journaltitle = {Applied Sciences},
	author = {Abedi, Masoud and Hempel, Lars and Sadeghi, Sina and Kirsten, Toralf},
	urldate = {2024-03-13},
	date = {2022-01},
	langid = {english},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{GANs}, data enlargement, data evaluation, synthetic data generation},
}

@misc{mariani_bagan_2018,
	title = {{BAGAN}: Data Augmentation with Balancing {GAN}},
	url = {http://arxiv.org/abs/1803.09655},
	shorttitle = {{BAGAN}},
	abstract = {Image classification datasets are often imbalanced, characteristic that negatively affects the accuracy of deep-learning classifiers. In this work we propose balancing {GAN} ({BAGAN}) as an augmentation tool to restore balance in imbalanced datasets. This is challenging because the few minority-class images may not be enough to train a {GAN}. We overcome this issue by including during the adversarial training all available images of majority and minority classes. The generative model learns useful features from majority classes and uses these to generate images for minority classes. We apply class conditioning in the latent space to drive the generation process towards a target class. The generator in the {GAN} is initialized with the encoder module of an autoencoder that enables us to learn an accurate class-conditioning in the latent space. We compare the proposed methodology with state-of-the-art {GANs} and demonstrate that {BAGAN} generates images of superior quality when trained with an imbalanced dataset.},
	number = {{arXiv}:1803.09655},
	publisher = {{arXiv}},
	author = {Mariani, Giovanni and Scheidegger, Florian and Istrate, Roxana and Bekas, Costas and Malossi, Cristiano},
	urldate = {2024-03-13},
	date = {2018-06-05},
	eprinttype = {arxiv},
	eprint = {1803.09655 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jordon_pate-gan_2019,
	title = {{PATE}-{GAN}: {GENERATING} {SYNTHETIC} {DATA} {WITH} {DIFFERENTIAL} {PRIVACY} {GUARANTEES}},
	abstract = {Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets ({GAN}) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modiﬁes the Private Aggregation of Teacher Ensembles ({PATE}) framework and applies it to {GANs}. Our modiﬁed framework (which we call {PATE}-{GAN}) allows us to tightly bound the inﬂuence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that {PATE}-{GAN} consistently outperforms the stateof-the-art method with respect to this and other notions of synthetic data quality.},
	author = {Jordon, James and Yoon, Jinsung},
	date = {2019},
	langid = {english},
}

@article{niu_defect_2020,
	title = {Defect Image Sample Generation With {GAN} for Improving Defect Recognition},
	issn = {1545-5955, 1558-3783},
	url = {https://ieeexplore.ieee.org/document/9000806/},
	doi = {10.1109/TASE.2020.2967415},
	abstract = {This article aims to improve deep-learning-based surface defect recognition. Owing to the insufﬁciency of the defect images in practical production lines and the high cost of labeling, it is difﬁcult to obtain a sufﬁcient defect data set in terms of diversity and quantity. A new generation method called surface defect-generation adversarial network ({SDGAN}), which employs generative adversarial networks ({GANs}), is proposed to generate defect images using a large number of defect-free images from industrial sites. Experiments show that the defect images generated by the {SDGAN} have better image quality and diversity than those generated by the state-of-the-art methods. The {SDGAN} is applied to expand the commutator cylinder surface defect image data sets with and without labels (referred to as the {CCSD}-L and {CCSD}-{NL} data sets, respectively). Regarding anomaly recognition, a 1.77\% error rate and a 49.43\% relative improvement ({IMP}) for the {CCSD}-{NL} defect data set are obtained. Regarding defect classiﬁcation, a 0.74\% error rate and a 57.47\% {IMP} for the {CCSD}-L defect data set are achieved. Moreover, defect classiﬁcation trained on the images augmented by the {SDGAN} is robust to uneven and poor lighting conditions.},
	pages = {1--12},
	journaltitle = {{IEEE} Transactions on Automation Science and Engineering},
	shortjournal = {{IEEE} Trans. Automat. Sci. Eng.},
	author = {Niu, Shuanlong and Li, Bin and Wang, Xinggang and Lin, Hui},
	urldate = {2024-03-13},
	date = {2020},
	langid = {english},
}

@misc{tian_cr-gan_2018,
	title = {{CR}-{GAN}: Learning Complete Representations for Multi-view Generation},
	url = {http://arxiv.org/abs/1806.11191},
	shorttitle = {{CR}-{GAN}},
	abstract = {Generating multi-view images from a single-view input is an essential yet challenging problem. It has broad applications in vision, graphics, and robotics. Our study indicates that the widely-used generative adversarial network ({GAN}) may learn "incomplete" representations due to the single-pathway framework: an encoder-decoder network followed by a discriminator network. We propose {CR}-{GAN} to address this problem. In addition to the single reconstruction path, we introduce a generation sideway to maintain the completeness of the learned embedding space. The two learning pathways collaborate and compete in a parameter-sharing manner, yielding considerably improved generalization ability to "unseen" dataset. More importantly, the two-pathway framework makes it possible to combine both labeled and unlabeled data for self-supervised learning, which further enriches the embedding space for realistic generations. The experimental results prove that {CR}-{GAN} significantly outperforms state-of-the-art methods, especially when generating from "unseen" inputs in wild conditions.},
	number = {{arXiv}:1806.11191},
	publisher = {{arXiv}},
	author = {Tian, Yu and Peng, Xi and Zhao, Long and Zhang, Shaoting and Metaxas, Dimitris N.},
	urldate = {2024-03-13},
	date = {2018-06-28},
	eprinttype = {arxiv},
	eprint = {1806.11191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{teng_sketch2vis_2021,
	location = {Pasadena, {CA}, {USA}},
	title = {Sketch2Vis: Generating Data Visualizations from Hand-drawn Sketches with Deep Learning},
	isbn = {978-1-66544-337-1},
	url = {https://ieeexplore.ieee.org/document/9680034/},
	doi = {10.1109/ICMLA52953.2021.00141},
	shorttitle = {Sketch2Vis},
	abstract = {Data visualization has become a vital tool to help people understand the driving forces behind real-world phenomena. Although the learning curve of visualization tools have been reduced, domain experts still often require signiﬁcant amounts of training to use them effectively. To reduce this learning curve even further, this paper proposes Sketch2Vis, a novel solution using deep learning techniques and tools to generate the source code for multi-platform data visualizations automatically from handdrawn sketches provided by domain experts, which is similar to how an expert might sketch on a cocktail napkin and ask a software engineer to implement the sketched visualization.},
	eventtitle = {2021 20th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	pages = {853--858},
	booktitle = {2021 20th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	publisher = {{IEEE}},
	author = {Teng, Zhongwei and Fu, Quchen and White, Jules and Schmidt, Douglas C.},
	urldate = {2024-03-13},
	date = {2021-12},
	langid = {english},
}

@inproceedings{soni_deep_2021,
	location = {Chennai, India},
	title = {Deep Learning Based Approach to Generate Realistic Data for {ADAS} Applications},
	isbn = {978-1-66543-277-1},
	url = {https://ieeexplore.ieee.org/document/9465529/},
	doi = {10.1109/ICCCSP52374.2021.9465529},
	abstract = {Quantity, quality and diversity of datasets are prerequisites for training the deep-learning autonomous driving models. One major issue identified from the literature is the lack of realistic training data which eventually leads to a less robust model. Simulators can help in dealing with reducing reality gaps, however, commonly available simulators generate data that are far removed from the real world scenarios. The model proposed in this study is based on video-to-video synthesis and image synthesis methods using Generative Adversarial Networks ({GANs}). The results indicate improved realism. Kanade-Lucas-Tomasi ({KLT}) and Fr’echet Inception Distance ({FID}) based temporal coherence evaluation metrics have also been proposed as a possible alternative to human perception driven evaluations.},
	eventtitle = {2021 5th International Conference on Computer, Communication and Signal Processing ({ICCCSP})},
	pages = {1--5},
	booktitle = {2021 5th International Conference on Computer, Communication and Signal Processing ({ICCCSP})},
	publisher = {{IEEE}},
	author = {Soni, Rajat Kumar and Nair, Binoy B},
	urldate = {2024-03-13},
	date = {2021-05-24},
	langid = {english},
}

@misc{triastcyn_generating_2019,
	title = {Generating Artificial Data for Private Deep Learning},
	url = {http://arxiv.org/abs/1803.03148},
	abstract = {In this paper, we propose generating artificial data that retain statistical properties of real data as the means of providing privacy with respect to the original dataset. We use generative adversarial network to draw privacy-preserving artificial data samples and derive an empirical method to assess the risk of information disclosure in a differential-privacy-like way. Our experiments show that we are able to generate artificial data of high quality and successfully train and validate machine learning models on this data while limiting potential privacy loss.},
	number = {{arXiv}:1803.03148},
	publisher = {{arXiv}},
	author = {Triastcyn, Aleksei and Faltings, Boi},
	urldate = {2024-03-13},
	date = {2019-04-28},
	eprinttype = {arxiv},
	eprint = {1803.03148 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
	url = {http://arxiv.org/abs/1706.02413},
	shorttitle = {{PointNet}++},
	abstract = {Few prior works study deep learning on point sets. {PointNet} by Qi et al. is a pioneer in this direction. However, by design {PointNet} does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies {PointNet} recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called {PointNet}++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	number = {{arXiv}:1706.02413},
	publisher = {{arXiv}},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	urldate = {2024-03-13},
	date = {2017-06-07},
	eprinttype = {arxiv},
	eprint = {1706.02413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ren_faster_2015,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} and Fast R-{CNN} have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network ({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. {RPNs} are trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. With a simple alternating optimization, {RPN} and Fast R-{CNN} can be trained to share convolutional features. For the very deep {VGG}-16 model, our detection system has a frame rate of 5fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007 (73.2\% {mAP}) and 2012 (70.4\% {mAP}) using 300 proposals per image. Code is available at https://github.com/{ShaoqingRen}/faster\_rcnn.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	urldate = {2024-03-13},
	date = {2015},
}

@article{hubner_evaluation_2020,
	title = {Evaluation of {HoloLens} Tracking and Depth Sensing for Indoor Mapping Applications},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/4/1021},
	doi = {10.3390/s20041021},
	abstract = {The Microsoft {HoloLens} is a head-worn mobile augmented reality device that is capable of mapping its direct environment in real-time as triangle meshes and localize itself within these three-dimensional meshes simultaneously. The device is equipped with a variety of sensors including four tracking cameras and a time-of-ﬂight ({ToF}) range camera. Sensor images and their poses estimated by the built-in tracking system can be accessed by the user. This makes the {HoloLens} potentially interesting as an indoor mapping device. In this paper, we introduce the different sensors of the device and evaluate the complete system in respect of the task of mapping indoor environments. The overall quality of such a system depends mainly on the quality of the depth sensor together with its associated pose derived from the tracking system. For this purpose, we ﬁrst evaluate the performance of the {HoloLens} depth sensor and its tracking system separately. Finally, we evaluate the overall system regarding its capability for mapping multi-room environments.},
	pages = {1021},
	number = {4},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Hübner, Patrick and Clintworth, Kate and Liu, Qingyi and Weinmann, Martin and Wursthorn, Sven},
	urldate = {2024-03-05},
	date = {2020-02-14},
	langid = {english},
}

@inproceedings{deng_depth-supervised_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {Depth-supervised {NeRF}: Fewer Views and Faster Training for Free},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880067/},
	doi = {10.1109/CVPR52688.2022.01254},
	shorttitle = {Depth-supervised {NeRF}},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {12872--12881},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
	urldate = {2024-03-05},
	date = {2022-06},
	langid = {english},
}

@article{zhang_deep_2018,
	title = {Deep Learning based Object Distance Measurement Method for Binocular Stereo Vision Blind Area},
	volume = {9},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=9&Issue=9&Code=ijacsa&SerialNo=77},
	doi = {10.14569/IJACSA.2018.090977},
	abstract = {Visual ﬁeld occlusion is one of the causes of urban trafﬁc accidents in the process of reversing. In order to meet the requirements of vehicle safety and intelligence, a method of target distance measurement based on deep learning and binocular vision is proposed. The method ﬁrst establishes binocular stereo vision model and calibrates intrinsic extrinsic and extrinsic parameters, uses Faster R-{CNN} algorithm to identify and locate obstacle objects in the image, then substitutes the obtained matching points into a calibrated binocular stereo model for spatial coordinates of the target object. Finally, the obstacle distance is calculated by the formula. In different positions, take pictures of obstacles from different angles to conduct physical tests. Experimental results show that this method can effectively achieve obstacle object identiﬁcation and positioning, and improve the adverse effect of visual ﬁeld blindness on driving safety.},
	number = {9},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	shortjournal = {ijacsa},
	author = {Zhang, Jiaxu and Hu, Shaolin and Shi, Haoqiang},
	urldate = {2024-03-05},
	date = {2018},
	langid = {english},
}

@inproceedings{mousavian_3d_2017,
	location = {Honolulu, {HI}},
	title = {3D Bounding Box Estimation Using Deep Learning and Geometry},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100080/},
	doi = {10.1109/CVPR.2017.597},
	abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method ﬁrst regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The ﬁrst network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which signiﬁcantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging {KITTI} object detection benchmark [2] both on the ofﬁcial metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and ﬂat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5632--5640},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
	urldate = {2024-03-05},
	date = {2017-07},
	langid = {english},
}

@article{loey_within_2020,
	title = {Within the Lack of Chest {COVID}-19 X-ray Dataset: A Novel Detection Model Based on {GAN} and Deep Transfer Learning},
	volume = {12},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/12/4/651},
	doi = {10.3390/sym12040651},
	shorttitle = {Within the Lack of Chest {COVID}-19 X-ray Dataset},
	abstract = {The coronavirus ({COVID}-19) pandemic is putting healthcare systems across the world under unprecedented and increasing pressure according to the World Health Organization ({WHO}). With the advances in computer algorithms and especially Artiﬁcial Intelligence, the detection of this type of virus in the early stages will help in fast recovery and help in releasing the pressure oﬀ healthcare systems. In this paper, a {GAN} with deep transfer learning for coronavirus detection in chest X-ray images is presented. The lack of datasets for {COVID}-19 especially in chest X-rays images is the main motivation of this scientiﬁc study. The main idea is to collect all the possible images for {COVID}-19 that exists until the writing of this research and use the {GAN} network to generate more images to help in the detection of this virus from the available X-rays images with the highest accuracy possible. The dataset used in this research was collected from diﬀerent sources and it is available for researchers to download and use it. The number of images in the collected dataset is 307 images for four diﬀerent types of classes. The classes are the {COVID}-19, normal, pneumonia bacterial, and pneumonia virus. Three deep transfer models are selected in this research for investigation. The models are the Alexnet, Googlenet, and Restnet18. Those models are selected for investigation through this research as it contains a small number of layers on their architectures, this will result in reducing the complexity, the consumed memory and the execution time for the proposed model. Three case scenarios are tested through the paper, the ﬁrst scenario includes four classes from the dataset, while the second scenario includes 3 classes and the third scenario includes two classes. All the scenarios include the {COVID}-19 class as it is the main target of this research to be detected. In the ﬁrst scenario, the Googlenet is selected to be the main deep transfer model as it achieves 80.6\% in testing accuracy. In the second scenario, the Alexnet is selected to be the main deep transfer model as it achieves 85.2\% in testing accuracy, while in the third scenario which includes two classes ({COVID}-19, and normal), Googlenet is selected to be the main deep transfer model as it achieves 100\% in testing accuracy and 99.9\% in the validation accuracy. All the performance measurement strengthens the obtained results through the research.},
	pages = {651},
	number = {4},
	journaltitle = {Symmetry},
	shortjournal = {Symmetry},
	author = {Loey, Mohamed and Smarandache, Florentin and M. Khalifa, Nour Eldeen},
	urldate = {2024-03-05},
	date = {2020-04-20},
	langid = {english},
}

@inproceedings{hardy_md-gan_2019,
	location = {Rio de Janeiro, Brazil},
	title = {{MD}-{GAN}: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets},
	isbn = {978-1-72811-246-6},
	url = {https://ieeexplore.ieee.org/document/8821025/},
	doi = {10.1109/IPDPS.2019.00095},
	shorttitle = {{MD}-{GAN}},
	abstract = {A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks ({GANs}). Those generative models are computationally demanding, as a {GAN} is composed of two deep neural networks, and because it trains on large datasets. A {GAN} is generally trained on a single server.},
	eventtitle = {2019 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	pages = {866--877},
	booktitle = {2019 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
	publisher = {{IEEE}},
	author = {Hardy, Corentin and Le Merrer, Erwan and Sericola, Bruno},
	urldate = {2024-03-05},
	date = {2019-05},
	langid = {english},
}

@misc{nataraj_detecting_2019,
	title = {Detecting {GAN} generated Fake Images using Co-occurrence Matrices},
	url = {http://arxiv.org/abs/1903.06836},
	abstract = {The advent of Generative Adversarial Networks ({GANs}) has brought about completely novel ways of transforming and manipulating pixels in digital images. {GAN} based techniques such as Image-to-Image translations, {DeepFakes}, and other automated methods have become increasingly popular in creating fake images. In this paper, we propose a novel approach to detect {GAN} generated fake images using a combination of co-occurrence matrices and deep learning. We extract co-occurrence matrices on three color channels in the pixel domain and train a model using a deep convolutional neural network ({CNN}) framework. Experimental results on two diverse and challenging {GAN} datasets comprising more than 56,000 images based on unpaired image-to-image translations ({cycleGAN} [1]) and facial attributes/expressions ({StarGAN} [2]) show that our approach is promising and achieves more than 99\% classification accuracy in both datasets. Further, our approach also generalizes well and achieves good results when trained on one dataset and tested on the other.},
	number = {{arXiv}:1903.06836},
	publisher = {{arXiv}},
	author = {Nataraj, Lakshmanan and Mohammed, Tajuddin Manhar and Chandrasekaran, Shivkumar and Flenner, Arjuna and Bappy, Jawadul H. and Roy-Chowdhury, Amit K. and Manjunath, B. S.},
	urldate = {2024-03-05},
	date = {2019-10-02},
	eprinttype = {arxiv},
	eprint = {1903.06836 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{mcinnes_umap_2018,
	title = {{UMAP}: Uniform Manifold Approximation and Projection},
	volume = {3},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.00861},
	doi = {10.21105/joss.00861},
	shorttitle = {{UMAP}},
	abstract = {{McInnes} et al., (2018). {UMAP}: Uniform Manifold Approximation and Projection. Journal of Open Source Software, 3(29), 861, https://doi.org/10.21105/joss.00861},
	pages = {861},
	number = {29},
	journaltitle = {Journal of Open Source Software},
	author = {{McInnes}, Leland and Healy, John and Saul, Nathaniel and Großberger, Lukas},
	urldate = {2024-02-28},
	date = {2018-09-02},
	langid = {english},
}

@inproceedings{huang_densely_2017,
	location = {Honolulu, {HI}},
	title = {Densely Connected Convolutional Networks},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8099726/},
	doi = {10.1109/CVPR.2017.243},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2261--2269},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	urldate = {2024-02-26},
	date = {2017-07},
	langid = {english},
}

@inproceedings{ronneberger_u-net_2015,
	location = {Cham},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	isbn = {978-3-319-24574-4},
	doi = {10.1007/978-3-319-24574-4_28},
	series = {Lecture Notes in Computer Science},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	pages = {234--241},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	date = {2015},
	langid = {english},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
}

@misc{milletari_v-net_2016,
	title = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},
	url = {http://arxiv.org/abs/1606.04797},
	shorttitle = {V-Net},
	abstract = {Convolutional Neural Networks ({CNNs}) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our {CNN} is trained end-to-end on {MRI} volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	number = {{arXiv}:1606.04797},
	publisher = {{arXiv}},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	urldate = {2024-02-25},
	date = {2016-06-15},
	eprinttype = {arxiv},
	eprint = {1606.04797 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ng_feature_2004,
	location = {Banff, Alberta, Canada},
	title = {Feature selection, \textit{L} $_{\textrm{1}}$ vs. \textit{L} $_{\textrm{2}}$ regularization, and rotational invariance},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015435},
	doi = {10.1145/1015330.1015435},
	abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two diﬀerent regularization methods for preventing overﬁtting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn “well,”) grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be eﬀective even if there are exponentially many irrelevant features as there are training examples. We also give a lowerbound showing that any rotationally invariant algorithm—including logistic regression with L2 regularization, {SVMs}, and neural networks trained by backpropagation—has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
	eventtitle = {Twenty-first international conference},
	pages = {78},
	booktitle = {Twenty-first international conference on Machine learning  - {ICML} '04},
	publisher = {{ACM} Press},
	author = {Ng, Andrew Y.},
	urldate = {2024-02-24},
	date = {2004},
	langid = {english},
}

@article{srivastava_dropout_2014,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	pages = {1929--1958},
	journaltitle = {Journal of Machine Learning Research 15 (2014) 1929-1958},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014},
	langid = {english},
	keywords = {dropout},
}

@article{iwatsubo_japanese_2010,
	title = {Japanese Alzheimer's Disease Neuroimaging Initiative: Present status and future},
	volume = {6},
	issn = {1552-5260, 1552-5279},
	url = {https://alz-journals.onlinelibrary.wiley.com/doi/10.1016/j.jalz.2010.03.011},
	doi = {10.1016/j.jalz.2010.03.011},
	shorttitle = {Japanese Alzheimer's Disease Neuroimaging Initiative},
	abstract = {Japanese Alzheimer's Disease Neuroimaging Initiative (J‐{ADNI}) was launched in 2008, aiming at conducting a longitudinal workup of a standardized neuroimaging, biomarker and clinico‐psychological surveys. The research protocol was designed to maximize compatibility with that of {US}‐{ADNI}, including structural magnetic resonance imaging analysis for the evaluation of brain atrophy, fluorodeoxyglucose and amyloid positron emission tomography, cerebrospinal fluid sampling,
                {APOE}
                genotyping, together with a set of clinical and psychometric tests that were prepared to achieve the highest compatibility to those used in the United States. Japanese {ADNI} has recruited {\textasciitilde}357 participants (142 amnestic mild cognitive impairment, {\textasciitilde}134 normal aged and 72 mild Alzheimer's disease ({AD}), as of April 15, 2010). World‐wide {ADNI} activities will establish the rigorous quantitative descriptions of the natural course of {AD} in its very early stages. The data, as well as the methodologies and infrastructures, will facilitate the clinical trials of disease‐modifying therapies for {AD} using surrogate biomarkers.},
	pages = {297--299},
	number = {3},
	journaltitle = {Alzheimer's \& Dementia},
	shortjournal = {Alzheimer's \&amp; Dementia},
	author = {Iwatsubo, Takeshi},
	urldate = {2024-02-20},
	date = {2010-05},
	langid = {english},
}

@article{wang_high-generalizability_2022,
	title = {A high-generalizability machine learning framework for predicting the progression of Alzheimer’s disease using limited data},
	volume = {5},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00577-x},
	doi = {10.1038/s41746-022-00577-x},
	abstract = {Abstract
            Alzheimer’s disease is a neurodegenerative disease that imposes a substantial financial burden on society. A number of machine learning studies have been conducted to predict the speed of its progression, which varies widely among different individuals, for recruiting fast progressors in future clinical trials. However, because the data in this field are very limited, two problems have yet to be solved: the first is that models built on limited data tend to induce overfitting and have low generalizability, and the second is that no cross-cohort evaluations have been done. Here, to suppress the overfitting caused by limited data, we propose a hybrid machine learning framework consisting of multiple convolutional neural networks that automatically extract image features from the point of view of brain segments, which are relevant to cognitive decline according to clinical findings, and a linear support vector classifier that uses extracted image features together with non-image information to make robust final predictions. The experimental results indicate that our model achieves superior performance (accuracy: 0.88, area under the curve [{AUC}]: 0.95) compared with other state-of-the-art methods. Moreover, our framework demonstrates high generalizability as a result of evaluations using a completely different cohort dataset (accuracy: 0.84, {AUC}: 0.91) collected from a different population than that used for training.},
	pages = {43},
	number = {1},
	journaltitle = {npj Digital Medicine},
	shortjournal = {npj Digit. Med.},
	author = {Wang, Caihua and Li, Yuanzhong and Tsuboshita, Yukihiro and Sakurai, Takuya and Goto, Tsubasa and Yamaguchi, Hiroyuki and Yamashita, Yuichi and Sekiguchi, Atsushi and Tachimori, Hisateru and {for the Alzheimer’s Disease Neuroimaging Initiative}},
	urldate = {2024-01-07},
	date = {2022-04-12},
	langid = {english},
}

@article{lee_predicting_2019,
	title = {Predicting Alzheimer’s disease progression using multi-modal deep learning approach},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-37769-z},
	doi = {10.1038/s41598-018-37769-z},
	abstract = {Abstract
            Alzheimer’s disease ({AD}) is a progressive neurodegenerative condition marked by a decline in cognitive functions with no validated disease modifying treatment. It is critical for timely treatment to detect {AD} in its earlier stage before clinical manifestation. Mild cognitive impairment ({MCI}) is an intermediate stage between cognitively normal older adults and {AD}. To predict conversion from {MCI} to probable {AD}, we applied a deep learning approach, multimodal recurrent neural network. We developed an integrative framework that combines not only cross-sectional neuroimaging biomarkers at baseline but also longitudinal cerebrospinal fluid ({CSF}) and cognitive performance biomarkers obtained from the Alzheimer’s Disease Neuroimaging Initiative cohort ({ADNI}). The proposed framework integrated longitudinal multi-domain data. Our results showed that 1) our prediction model for {MCI} conversion to {AD} yielded up to 75\% accuracy (area under the curve ({AUC}) = 0.83) when using only single modality of data separately; and 2) our prediction model achieved the best performance with 81\% accuracy ({AUC} = 0.86) when incorporating longitudinal multi-domain data. A multi-modal deep learning approach has potential to identify persons at risk of developing {AD} who might benefit most from a clinical trial or as a stratification approach within clinical trials.},
	pages = {1952},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Lee, Garam and Nho, Kwangsik and Kang, Byungkon and Sohn, Kyung-Ah and Kim, Dokyoon and {for Alzheimer’s Disease Neuroimaging Initiative}},
	urldate = {2024-02-20},
	date = {2019-02-13},
	langid = {english},
}

@article{hussain_yolo-v1_2023,
	title = {{YOLO}-v1 to {YOLO}-v8, the Rise of {YOLO} and Its Complementary Nature toward Digital Manufacturing and Industrial Defect Detection},
	volume = {11},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/11/7/677},
	doi = {10.3390/machines11070677},
	abstract = {Since its inception in 2015, the {YOLO} (You Only Look Once) variant of object detectors has rapidly grown, with the latest release of {YOLO}-v8 in January 2023. {YOLO} variants are underpinned by the principle of real-time and high-classiﬁcation performance, based on limited but efﬁcient computational parameters. This principle has been found within the {DNA} of all {YOLO} variants with increasing intensity, as the variants evolve addressing the requirements of automated quality inspection within the industrial surface defect detection domain, such as the need for fast detection, high accuracy, and deployment onto constrained edge devices. This paper is the ﬁrst to provide an in-depth review of the {YOLO} evolution from the original {YOLO} to the recent release ({YOLO}-v8) from the perspective of industrial manufacturing. The review explores the key architectural advancements proposed at each iteration, followed by examples of industrial deployment for surface defect detection endorsing its compatibility with industrial requirements.},
	pages = {677},
	number = {7},
	journaltitle = {Machines},
	shortjournal = {Machines},
	author = {Hussain, Muhammad},
	urldate = {2024-02-17},
	date = {2023-06-23},
	langid = {english},
}

@inproceedings{deng_rnet_2018,
	location = {Stockholm, Sweden},
	title = {R³Net: Recurrent Residual Refinement Network for Saliency Detection},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/95},
	doi = {10.24963/ijcai.2018/95},
	shorttitle = {R³Net},
	abstract = {Saliency detection is a fundamental yet challenging task in computer vision, aiming at highlighting the most visually distinctive objects in an image. We propose a novel recurrent residual reﬁnement network (R3Net) equipped with residual reﬁnement blocks ({RRBs}) to more accurately detect salient regions of an input image. Our {RRBs} learn the residual between the intermediate saliency prediction and the ground truth by alternatively leveraging the low-level integrated features and the highlevel integrated features of a fully convolutional network ({FCN}). While the low-level integrated features are capable of capturing more saliency details, the high-level integrated features can reduce non-salient regions in the intermediate prediction. Furthermore, the {RRBs} can obtain complementary saliency information of the intermediate prediction, and add the residual into the intermediate prediction to reﬁne the saliency maps. We evaluate the proposed R3Net on ﬁve widely-used saliency detection benchmarks by comparing it with 16 stateof-the-art saliency detectors. Experimental results show that our network outperforms our competitors in all the benchmark datasets.},
	eventtitle = {Twenty-Seventh International Joint Conference on Artificial Intelligence \{{IJCAI}-18\}},
	pages = {684--690},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Deng, Zijun and Hu, Xiaowei and Zhu, Lei and Xu, Xuemiao and Qin, Jing and Han, Guoqiang and Heng, Pheng-Ann},
	urldate = {2024-02-17},
	date = {2018-07},
	langid = {english},
}

@article{carbonell_lecture_nodate,
	title = {Lecture Notes in Artificial Intelligence},
	author = {Carbonell, Edited J G and Siekmann, J},
	langid = {english},
}

@article{park_deep_2023,
	title = {Deep joint learning of pathological region localization and Alzheimer’s disease diagnosis},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-38240-4},
	doi = {10.1038/s41598-023-38240-4},
	abstract = {Abstract
            The identification of Alzheimer’s disease ({AD}) using structural magnetic resonance imaging ({sMRI}) has been studied based on the subtle morphological changes in the brain. One of the typical approaches is a deep learning-based patch-level feature representation. For this approach, however, the predetermined patches before learning the diagnostic model can limit classification performance. To mitigate this problem, we propose the {BrainBagNet} with a position-based gate ({PG}), which applies position information of brain images represented through the 3D coordinates. Our proposed method represents the patch-level class evidence based on both {MR} scan and position information for image-level prediction. To validate the effectiveness of our proposed framework, we conducted comprehensive experiments comparing it with state-of-the-art methods, utilizing two publicly available datasets: the Alzheimer’s Disease Neuroimaging Initiative ({ADNI}) and the Australian Imaging, Biomarkers and Lifestyle ({AIBL}) dataset. Furthermore, our experimental results demonstrate that our proposed method outperforms the existing competing methods in terms of classification performance for both {AD} diagnosis and mild cognitive impairment conversion prediction tasks. In addition, we performed various analyses of the results from diverse perspectives to obtain further insights into the underlying mechanisms and strengths of our proposed framework. Based on the results of our experiments, we demonstrate that our proposed framework has the potential to advance deep-learning-based patch-level feature representation studies for {AD} diagnosis and {MCI} conversion prediction. In addition, our method provides valuable insights, such as interpretability, and the ability to capture subtle changes, into the underlying pathological processes of {AD} and {MCI}, benefiting both researchers and clinicians.},
	pages = {11664},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Park, Changhyun and Jung, Wonsik and Suk, Heung-Il},
	urldate = {2023-11-11},
	date = {2023-07-19},
	langid = {english},
}

@article{odusami_analysis_2021,
	title = {Analysis of Features of Alzheimer’s Disease: Detection of Early Stage from Functional Brain Changes in Magnetic Resonance Images Using a Finetuned {ResNet}18 Network},
	volume = {11},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/11/6/1071},
	doi = {10.3390/diagnostics11061071},
	shorttitle = {Analysis of Features of Alzheimer’s Disease},
	abstract = {One of the ﬁrst signs of Alzheimer’s disease ({AD}) is mild cognitive impairment ({MCI}), in which there are small variants of brain changes among the intermediate stages. Although there has been an increase in research into the diagnosis of {AD} in its early levels of developments lately, brain changes, and their complexity for functional magnetic resonance imaging ({fMRI}), makes early detection of {AD} difﬁcult. This paper proposes a deep learning-based method that can predict {MCI}, early {MCI} ({EMCI}), late {MCI} ({LMCI}), and {AD}. The Alzheimer’s Disease Neuroimaging Initiative ({ADNI}) {fMRI} dataset consisting of 138 subjects was used for evaluation. The ﬁnetuned {ResNet}18 network achieved a classiﬁcation accuracy of 99.99\%, 99.95\%, and 99.95\% on {EMCI} vs. {AD}, {LMCI} vs. {AD}, and {MCI} vs. {EMCI} classiﬁcation scenarios, respectively. The proposed model performed better than other known models in terms of accuracy, sensitivity, and speciﬁcity.},
	pages = {1071},
	number = {6},
	journaltitle = {Diagnostics},
	shortjournal = {Diagnostics},
	author = {Odusami, Modupe and Maskeliūnas, Rytis and Damaševičius, Robertas and Krilavičius, Tomas},
	urldate = {2024-01-17},
	date = {2021-06-10},
	langid = {english},
}

@article{kodytek_large-scale_2022,
	title = {A large-scale image dataset of wood surface defects for automated vision-based quality control processes},
	volume = {10},
	issn = {2046-1402},
	url = {https://f1000research.com/articles/10-581/v2},
	doi = {10.12688/f1000research.52903.2},
	abstract = {The wood industry is facing many challenges. The high variability of raw material and the complexity of manufacturing processes results in a wide range of visible structure defects, which have to be controlled by trained specialists. These manual processes are not only tedious and biased, but also less effective. To overcome the drawbacks of the manual quality control processes, several automated vision-based systems have been proposed. Even though some conducted studies achieved a higher recognition rate than trained experts, researchers have to deal with a lack of large-scale databases and authentic data in this field. To address this issue, we performed a data acquisition experiment set in the industrial environment, where we were able to acquire an extensive set of authentic data from a production line. For this purpose, we designed and implemented a complex technical solution suitable for high-speed acquisition during harsh manufacturing conditions. In this data note, we present a large-scale dataset of high-resolution sawn timber surface images containing more than 43 000 labelled surface defects and covering 10 types of the most common wood defects. Moreover, with each image record, we provide two types of labels allowing researchers to perform semantic segmentation, as well as defect classification, and localization.},
	pages = {581},
	journaltitle = {F1000Research},
	shortjournal = {F1000Res},
	author = {Kodytek, Pavel and Bodzas, Alexandra and Bilik, Petr},
	urldate = {2024-01-14},
	date = {2022-06-27},
	langid = {english},
}

@incollection{banfi_building_2018,
	location = {Cham},
	title = {Building Information Modeling for Cultural Heritage: The Management of Generative Process for Complex Historical Buildings},
	isbn = {978-3-319-75826-8},
	url = {https://doi.org/10.1007/978-3-319-75826-8_10},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Building Information Modeling for Cultural Heritage},
	abstract = {Building Information Modeling ({BIM}) enhances the sharing of information during the traditional process for new construction, but most of the time, it requires high levels of knowledge management for the historical digital model (H-{BIM}). The innovation in the Digital Cultural Heritage ({DCH}) domain is supported by the development of Information and Communications Technologies ({ICT}) and modern tools that are able to transmit morphological characteristics of the buildings in all their uniqueness. The latest research in the field of H-{BIM} shows a significant emergence of innovative methods and management initiatives for the generation of complex historical elements, leading to the confrontation of the paradigm of regularity (simple geometric shapes) with the new paradigm of complexity (historical building elements). This paper proves the benefits of the {BIM} for project management of the Centre Block of the Canadian Parliament in Ottawa, Ontario Canada, and shows the results obtained by the introduction of Advanced Modeling Techniques ({AMT}) during the generative process, reducing time and cost for the creation of the complex architectural and structural elements. The uniqueness of the forms of historical buildings is a real value to be transmitted throughout the building’s lifecycle with high Levels of Detail ({LOD}). Proper management of geometric primitives and Non-Uniform Rational Basis Spline ({NURBS}) models have guaranteed the conversion of spatial data (point clouds) from laser scanning and photogrammetry (geometric survey) into parametric applications. This paper explores the generative process of one of the most complex spaces within The Centre Block building of Parliament Hill—Confederation Hall.},
	pages = {119--130},
	booktitle = {Digital Cultural Heritage: Final Conference of the Marie Skłodowska-Curie Initial Training Network for Digital Cultural Heritage, {ITN}-{DCH} 2017, Olimje, Slovenia, May 23–25, 2017, Revised Selected Papers},
	publisher = {Springer International Publishing},
	author = {Banfi, F. and Chow, L. and Reina Ortiz, M. and Ouimet, C. and Fai, S.},
	editor = {Ioannides, Marinos},
	urldate = {2023-12-11},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-75826-8_10},
}

@article{barazzetti_cloud--bim--fem_2015,
	title = {Cloud-to-{BIM}-to-{FEM}: Structural simulation with accurate historic {BIM} from laser scans},
	volume = {57},
	issn = {1569190X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1569190X15000994},
	doi = {10.1016/j.simpat.2015.06.004},
	shorttitle = {Cloud-to-{BIM}-to-{FEM}},
	abstract = {The complexity of historic constructions, with irregular geometry, inhomogeneous materials, variable morphology, alterations and damages, poses numerous challenges in the digital modeling and simulation of structural performances under different types of actions. Although recent developments in Building Information Modeling have introduced advanced simulation capabilities, the numerical characterization of historic buildings is still a challenging task for the lack of reliable procedures for structural simulation.},
	pages = {71--87},
	journaltitle = {Simulation Modelling Practice and Theory},
	shortjournal = {Simulation Modelling Practice and Theory},
	author = {Barazzetti, Luigi and Banfi, Fabrizio and Brumana, Raffaella and Gusmeroli, Gaia and Previtali, Mattia and Schiantarelli, Giuseppe},
	urldate = {2023-12-11},
	date = {2015-09},
	langid = {english},
}

@article{bradley_bim_2016,
	title = {{BIM} for infrastructure: An overall review and constructor perspective},
	volume = {71},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092658051630173X},
	doi = {10.1016/j.autcon.2016.08.019},
	shorttitle = {{BIM} for infrastructure},
	abstract = {The subject of building information modelling ({BIM}) has become a central topic to the improvement of the {AECOO} (Architecture, Engineering, Construction, Owner and Operator) industry around the world, to the point where the concept is being expanded into domains it was not originally conceived to address. Transitioning {BIM} into the domain of infrastructure projects has provided challenges and emphasized the constructor perspective of {BIM}. Therefore, this study aims to collect the relevant literature regarding {BIM} within the Infrastructure domain and its use from the constructor perspective to review and analyse the current industry positioning and research state of the art, with regards to the set criteria. The review highlighted a developing base of {BIM} for infrastructure. From the analysis, the related research gaps were identiﬁed regarding information integration, alignment of {BIM} processes to constructor business processes \& the effective governance and value of information. From this a unique research strategy utilising a framework for information governance coupled with a graph based distributed data environment is outlined to further progress the integration and efﬁciency of {AECOO} Infrastructure projects.},
	pages = {139--152},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Bradley, Alex and Li, Haijiang and Lark, Robert and Dunn, Simon},
	urldate = {2023-12-11},
	date = {2016-11},
	langid = {english},
}

@article{brumana_hr_2018,
	title = {{HR} {LOD} based {HBIM} to detect influences on geometry and shape by stereotomic construction techniques of brick vaults},
	volume = {10},
	issn = {1866-928X},
	url = {https://doi.org/10.1007/s12518-018-0209-3},
	doi = {10.1007/s12518-018-0209-3},
	abstract = {The use of construction techniques in cloister vaults in noble buildings, as covering elements for square or rectangular rooms, is widespread across Europe. The geometric continuity at the intrados makes generally possible the execution all over the span of frescoes, stucco and decorations, with a great diffusion of a great variety of solutions. The construction of brick vaults, from the late Middle Age, was sped up by limiting the centring to the wooden planks arches that were instrumental in the profile determination. Starting from laser scanning, photogrammetric and thermographic techniques, the punctual reconstruction of the geometry and construction techniques allowed to recognise and understand the constructive richness, the multiplicity and unicity of each vaulted element, made of recurrent elements and specific features, thus sketching a mixed pattern of workers and highlighting the constructive knowledge of ‘stereotomy’ applied to the brick block vaults. Nowadays, the availability of several {BIM}-based modelling procedures and tools based on high detailed surveys allows to identify and reconstruct the shape, drawing reliable assumptions about the construction methods and the execution time. The research methodology here proposed intends to tackle an updatable geographic catalogue, able to transfer the construction richness, inheriting the historic lesson of French ‘repertoires’ to generate modern {HBIM} vault libraries (abaci). The paper focuses on a well-documented case, the Magio Grasselli palace in Cremona in which the cloister vaults of two main rooms, and others, show different construction systems embodied by the geometry. The methodology has shown how the cloister vault typology can be turned to a dome construction in the same vault, and how ‘stereotomy’, the capacity of skilled workers to control the space, modified the typical geometry, made by the ‘generative’ construction process used for the cloister vault (intended as the intersection of 2 barrel vaults), turning it into a dome in the upper part, giving back a sort of morphing, merging the two different generative rules (dome and vault) as described hereafter and creating unexpected scenic effect.},
	pages = {529--543},
	number = {4},
	journaltitle = {Applied Geomatics},
	shortjournal = {Appl Geomat},
	author = {Brumana, Raffaella and Condoleo, Paola and Grimoldi, Alberto and Banfi, Fabrizio and Landi, Angelo Giuseppe and Previtali, Mattia},
	urldate = {2023-12-11},
	date = {2018-12-01},
	langid = {english},
}

@online{noauthor_zotero_nodate,
	title = {Zotero {\textbar} Your personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2023-12-11},
}

@inproceedings{liu_early_2014,
	location = {Beijing, China},
	title = {Early diagnosis of Alzheimer's disease with deep learning},
	isbn = {978-1-4673-1961-4},
	url = {http://ieeexplore.ieee.org/document/6868045/},
	doi = {10.1109/ISBI.2014.6868045},
	abstract = {The accurate diagnosis of Alzheimer’s disease ({AD}) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped. Although many studies have applied machine learning methods for computer-aided-diagnosis ({CAD}) of {AD} recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models. In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of {AD} and its prodromal stage, Mild Cognitive Impairment ({MCI}). Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge. A significant performance gain on classification of all diagnosis groups was achieved in our experiments.},
	eventtitle = {2014 {IEEE} 11th International Symposium on Biomedical Imaging ({ISBI} 2014)},
	pages = {1015--1018},
	booktitle = {2014 {IEEE} 11th International Symposium on Biomedical Imaging ({ISBI})},
	publisher = {{IEEE}},
	author = {Liu, Siqi and Liu, Sidong and Cai, Weidong and Pujol, Sonia and Kikinis, Ron and Feng, Dagan},
	urldate = {2023-11-11},
	date = {2014-04},
	langid = {english},
}

@article{khojaste-sarakhsi_deep_2022,
	title = {Deep learning for Alzheimer's disease diagnosis: A survey},
	volume = {130},
	issn = {09333657},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365722000975},
	doi = {10.1016/j.artmed.2022.102332},
	shorttitle = {Deep learning for Alzheimer's disease diagnosis},
	abstract = {Alzheimer's Disease ({AD}) is an irreversible neurodegenerative disease that results in a progressive decline in cognitive abilities. Since {AD} starts several years before the onset of the symptoms, its early detection is chal­ lenging due to subtle changes in biomarkers mainly detectable in different neuroimaging modalities. Developing computer-aided diagnostic models based on deep learning can provide excellent opportunities for the analysis of different neuroimage modalities along with other non-image biomarkers. In this survey, we perform a comparative analysis of about 100 published papers since 2019 that employ basic deep architectures such as {CNN}, {RNN}, and generative models for {AD} diagnosis. Moreover, about 60 papers that have applied a trending topic or architecture for {AD} are investigated. Explainable models, normalizing flows, graph-based deep archi­ tectures, self-supervised learning, and attention mechanisms are considered. The main challenges in this body of literature have been categorized and explained from data-related, methodology-related, and clinical adoption aspects. We conclude our paper by addressing some future perspectives and providing recommendations to conduct further studies for {AD} diagnosis.},
	pages = {102332},
	journaltitle = {Artificial Intelligence in Medicine},
	shortjournal = {Artificial Intelligence in Medicine},
	author = {Khojaste-Sarakhsi, M. and Haghighi, Seyedhamidreza Shahabi and Ghomi, S.M.T. Fatemi and Marchiori, Elena},
	urldate = {2023-11-11},
	date = {2022-08},
	langid = {english},
}

@article{ding_deep_2019,
	title = {A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using $^{\textrm{18}}$ F-{FDG} {PET} of the Brain},
	volume = {290},
	issn = {0033-8419, 1527-1315},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2018180958},
	doi = {10.1148/radiol.2018180958},
	abstract = {Purpose:  To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease ({AD}), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose ({FDG}) {PET} of the brain and compare its performance to that of radiologic readers. Materials and Methods:  Prospective 18F-{FDG} {PET} brain images from the Alzheimer’s Disease Neuroimaging Initiative ({ADNI}) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of {InceptionV}3 architecture was trained on 90\% of {ADNI} data set and tested on the remaining 10\%, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic ({ROC}), saliency map, and t-distributed stochastic neighbor embedding.
Results:  The algorithm achieved area under the {ROC} curve of 0.98 (95\% confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of {AD} in the independent test set (82\% specificity at 100\% sensitivity), an average of 75.8 months prior to the final diagnosis, which in {ROC} space outperformed reader performance (57\% [four of seven] sensitivity, 91\% [30 of 33] specificity; P , .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain.
Conclusion:  By using fluorine 18 fluorodeoxyglucose {PET} of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82\% specificity at 100\% sensitivity, an average of 75.8 months prior to the final diagnosis.},
	pages = {456--464},
	number = {2},
	journaltitle = {Radiology},
	shortjournal = {Radiology},
	author = {Ding, Yiming and Sohn, Jae Ho and Kawczynski, Michael G. and Trivedi, Hari and Harnish, Roy and Jenkins, Nathaniel W. and Lituiev, Dmytro and Copeland, Timothy P. and Aboian, Mariam S. and Mari Aparici, Carina and Behr, Spencer C. and Flavell, Robert R. and Huang, Shih-Ying and Zalocusky, Kelly A. and Nardo, Lorenzo and Seo, Youngho and Hawkins, Randall A. and Hernandez Pampaloni, Miguel and Hadley, Dexter and Franc, Benjamin L.},
	urldate = {2023-11-11},
	date = {2019-02},
	langid = {english},
}

@article{ebrahimighahnavieh_deep_2020,
	title = {Deep learning to detect Alzheimer's disease from neuroimaging: A systematic literature review},
	volume = {187},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260719310946},
	doi = {10.1016/j.cmpb.2019.105242},
	shorttitle = {Deep learning to detect Alzheimer's disease from neuroimaging},
	pages = {105242},
	journaltitle = {Computer Methods and Programs in Biomedicine},
	shortjournal = {Computer Methods and Programs in Biomedicine},
	author = {Ebrahimighahnavieh, Mr Amir and Luo, Suhuai and Chiong, Raymond},
	urldate = {2023-11-11},
	date = {2020-04},
	langid = {english},
}

@article{jo_deep_2019,
	title = {Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data},
	volume = {11},
	issn = {1663-4365},
	url = {https://www.frontiersin.org/article/10.3389/fnagi.2019.00220/full},
	doi = {10.3389/fnagi.2019.00220},
	shorttitle = {Deep Learning in Alzheimer's Disease},
	pages = {220},
	journaltitle = {Frontiers in Aging Neuroscience},
	shortjournal = {Front. Aging Neurosci.},
	author = {Jo, Taeho and Nho, Kwangsik and Saykin, Andrew J.},
	urldate = {2023-11-11},
	date = {2019-08-20},
	langid = {english},
}

@article{chu_integrating_2018,
	title = {Integrating mobile Building Information Modelling and Augmented Reality systems: An experimental study},
	volume = {85},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580517301218},
	doi = {10.1016/j.autcon.2017.10.032},
	shorttitle = {Integrating mobile Building Information Modelling and Augmented Reality systems},
	abstract = {The beneﬁts of Building Information Modelling ({BIM}) have typically been tied to its capability to support information structuring and exchange through the centralization of information. Its increasing adoption and the associated ease of data acquisition has created information intensive work environments, which can result in information overload and thus negatively impact workers task eﬃciency during construction. Augmented Reality ({AR}) has been proposed as a mechanism to enhance the process of information extraction from building information models to improve the eﬃciency and eﬀectiveness of workers' tasks. Yet, there is limited research that has evaluated the eﬀectiveness and usability of {AR} in this domain. This research aims to address this gap and evaluate the eﬀectiveness of {BIM} and {AR} system integration to enhance task eﬃciency through improving the information retrieval process during construction. To achieve this, a design science research approach was adopted that enabled the development and performance of a mobile {BIM} {AR} system (artefact) with cloud-based storage capabilities to be tested and evaluated using a portable desktop experiment. A total of 20 participants compared existing manual information retrieval methods (control group), with information retrieval through the artefact (non-control group). The results revealed that the participants using the artefact were approximately 50\% faster in completing their experiment tasks, and committed less errors, when compared to the control group. This research demonstrates that a minor modiﬁcation to existing information formats (2D plans) with the inclusion of Quick Response markers can signiﬁcantly improve the information retrieval process and that {BIM} and {AR} integration has the potential to enhance task eﬃciency.},
	pages = {305--316},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Chu, Michael and Matthews, Jane and Love, Peter E.D.},
	urldate = {2023-11-07},
	date = {2018-01},
	langid = {english},
}

@inproceedings{guo_real-time_2021,
	location = {Nara, Japan},
	title = {Real-time Object Detection with Deep Learning for Robot Vision on Mixed Reality Device},
	isbn = {978-1-66541-875-1},
	url = {https://ieeexplore.ieee.org/document/9391811/},
	doi = {10.1109/LifeTech52111.2021.9391811},
	abstract = {Mixed reality device sensing capabilities are valuable for robots, for example, the inertial measurement unit ({IMU}) sensor and time-of-flight ({TOF}) depth sensor can support the robot in navigating its environment. This paper demonstrates a deep learning ({YOLO} model) background, realtime object detection system implemented on mixed reality device. The goal of the system is to create a real-time communication system between {HoloLens} and Ubuntu systems to enable real-time object detection using the {YOLO} model. The experimental results show that the proposed method has a fast speed to achieve real-time object detection using {HoloLens}. This enables Microsoft {HoloLens} as a device for robot vision. To enhance human-robot interaction, we will apply it to a wearable robot arm system to automatically grasp objects in the future.},
	eventtitle = {2021 {IEEE} 3rd Global Conference on Life Sciences and Technologies ({LifeTech})},
	pages = {82--83},
	booktitle = {2021 {IEEE} 3rd Global Conference on Life Sciences and Technologies ({LifeTech})},
	publisher = {{IEEE}},
	author = {Guo, Jiazhen and Chen, Peng and Jiang, Yinlai and Yokoi, Hiroshi and Togo, Shunta},
	urldate = {2023-11-07},
	date = {2021-03-09},
	langid = {english},
}

@inproceedings{bahri_accurate_2019,
	location = {Athens, Greece},
	title = {Accurate Object Detection System on {HoloLens} Using {YOLO} Algorithm},
	isbn = {978-1-72813-572-4},
	url = {https://ieeexplore.ieee.org/document/9057151/},
	doi = {10.1109/ICCAIRO47923.2019.00042},
	abstract = {We demonstrate in our paper, an implementation on Microsoft {HoloLens}, deep learning supported in the context of object detection. The main aim of this system is to create the more accurate object detection model for Augmented Reality using communication between the deep learning processing and the Microsoft {HoloLens} as Input/Output device. This system aims to help the wearable device user to detect and to recognize between objects in real world. For the object detection approach, a deep learning model has been used for the implementation of this system called {YOLO}. This model is near to real-time and it supports to detect more than 9000 objects. Our system provides the annotation of augmented object detected and its limitation area or bounding box via {HoloLens}. It allows to detect the new position of moving object in a few milliseconds. Preliminary results show a great rate of object detection with a detection time comparable.},
	eventtitle = {2019 International Conference on Control, Artificial Intelligence, Robotics \& Optimization ({ICCAIRO})},
	pages = {219--224},
	booktitle = {2019 International Conference on Control, Artificial Intelligence, Robotics \& Optimization ({ICCAIRO})},
	publisher = {{IEEE}},
	author = {Bahri, Haythem and Krcmarik, David and Koci, Jan},
	urldate = {2023-11-07},
	date = {2019-05},
	langid = {english},
}

@inproceedings{deligiannakis_mixed_2022,
	location = {{CA}, {USA}},
	title = {Mixed Reality with Hardware Acceleration: implementing the multimodal user interface vision},
	isbn = {978-1-66545-725-5},
	url = {https://ieeexplore.ieee.org/document/10024443/},
	doi = {10.1109/AIVR56993.2022.00031},
	shorttitle = {Mixed Reality with Hardware Acceleration},
	abstract = {A Mixed Reality prototype application is proposed using the {HoloLens} Head Mounted Display device delegating {AI} tasks to edge or cloud systems. Processing and Battery life limitations of Head Mounted Display ({HMD}) devices do not allow these tasks to be executed for substantial periods of time without having an assistive external infrastructure to computationally support this process. Additionally, persistent and time sensitive {AI} procedures are delegated to achieve reliable and robust results. The architecture delegates the detection process to {NVIDIA} Jetson {TX}2, a {GPU}-accelerated, embedded and power-efficient device. We apply {YOLO} as the deep learning algorithm for object detection and use trained models on the Microsoft’s Common Objects in Context ({COCO}) dataset. We project visual and auditory information about the discovered objects.},
	eventtitle = {2022 {IEEE} International Conference on Artificial Intelligence and Virtual Reality ({AIVR})},
	pages = {153--156},
	booktitle = {2022 {IEEE} International Conference on Artificial Intelligence and Virtual Reality ({AIVR})},
	publisher = {{IEEE}},
	author = {Deligiannakis, Nektarios and Pavlopoulou, Maria-Evangelia and Papataxiarxis, Vassilis and Hadjiefthymiades, Stathes},
	urldate = {2023-11-07},
	date = {2022-12},
	langid = {english},
}

@inproceedings{mahurkar_integrating_2018,
	location = {New York City, {NY}, {USA}},
	title = {Integrating {YOLO} Object Detection with Augmented Reality for {iOS} Apps},
	isbn = {978-1-5386-7693-6},
	url = {https://ieeexplore.ieee.org/document/8796579/},
	doi = {10.1109/UEMCON.2018.8796579},
	abstract = {This paper proposes an implementation for detecting the identity and location of objects in real-time video preview and overlaying 3D graphics on them in {iOS} Apps. Object Detection is a technology that detects the identity (what) and location (where) of a variable number of objects in an Image. {YOLO}: You Only Look Once, is a real-time object detection system. {TinyYOLO} ({YOLOv}2 with a Darknet base network) is an implementation often used for mobile applications, which is much faster than {YOLO}, but less accurate [1]. An {MLModel} [2] was built using {TinyYOLO} that could detect the identity and location of the pre-trained objects in an image. Every frame of a video preview was passed as an input to the model. If the model detected an object, the 2D coordinates of the object in the image were recorded. The 2D coordinates were converted to 3D coordinates using the {ARKit} {HitTest} {API} [3]. After the 3D Coordinates were retrieved, {ARKit} was used to overlay a 3D graphics at the object’s location. Integrating Object Detection and Augmented Reality helped locate the 3D position of objects in the environment and overlay graphics on them.},
	eventtitle = {2018 9th {IEEE} Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference ({UEMCON})},
	pages = {585--589},
	booktitle = {2018 9th {IEEE} Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference ({UEMCON})},
	publisher = {{IEEE}},
	author = {Mahurkar, Sagar},
	urldate = {2023-11-07},
	date = {2018-11},
	langid = {english},
}

@article{ghasemi_deep_2022,
	title = {Deep learning-based object detection in augmented reality: A systematic review},
	volume = {139},
	issn = {01663615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166361522000586},
	doi = {10.1016/j.compind.2022.103661},
	shorttitle = {Deep learning-based object detection in augmented reality},
	abstract = {Recent advances in augmented reality ({AR}) and artificial intelligence have caused these technologies to pioneer innovation and alteration in any field and industry. The fast-paced developments in computer vision ({CV}) and augmented reality facilitated analyzing and understanding the surrounding environments. This paper systematically reviews and presents studies that integrated augmented/mixed reality and deep learning for object detection over the past decade. Five sources including Scopus, Web of Science, {IEEE} Xplore, {ScienceDirect}, and {ACM} were used to collect data. Finally, a total of sixty-nine papers were analyzed from two perspectives: (1) application analysis of deep learning-based object detection in the context of augmented reality and (2) analyzing the use of servers or local {AR} devices to perform the object detection computations to understand the relation between object detection algorithms and {AR} technology. Furthermore, the advantages of using deep learning-based object detection to solve the {AR} problems and limitations hindering the ultimate use of this technology are critically discussed. Our findings affirm the promising future of integrating {AR} and {CV}.},
	pages = {103661},
	journaltitle = {Computers in Industry},
	shortjournal = {Computers in Industry},
	author = {Ghasemi, Yalda and Jeong, Heejin and Choi, Sung Ho and Park, Kyeong-Beom and Lee, Jae Yeol},
	urldate = {2023-11-07},
	date = {2022-08},
	langid = {english},
}

@article{fang_tinier-yolo_2020,
	title = {Tinier-{YOLO}: A Real-Time Object Detection Method for Constrained Environments},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8941141/},
	doi = {10.1109/ACCESS.2019.2961959},
	shorttitle = {Tinier-{YOLO}},
	abstract = {Deep neural networks ({DNNs}) have shown prominent performance in the ﬁeld of object detection. However, {DNNs} usually run on powerful devices with high computational ability and sufﬁcient memory, which have greatly limited their deployment for constrained environments such as embedded devices. {YOLO} is one of the state-of-the-art {DNN}-based object detection approaches with good performance both on speed and accuracy and Tiny-{YOLO}-V3 is its latest variant with a small model that can run on embedded devices. In this paper, Tinier-{YOLO}, which is originated from Tiny-{YOLO}-V3, is proposed to further shrink the model size while achieving improved detection accuracy and real-time performance. In Tinier-{YOLO}, the ﬁre module in {SqueezeNet} is appointed by investigating the number of ﬁre modules as well as their positions in the model in order to reduce the number of model parameters and then reduce the model size. For further improving the proposed Tinier-{YOLO} in terms of detection accuracy and real-time performance, the connectivity style between ﬁre modules in Tinier-{YOLO} differs from {SqueezeNet} in that dense connection is introduced and ﬁne designed to strengthen the feature propagation and ensure the maximum information ﬂow in the network. The object detection performance is enhanced in Tinier-{YOLO} by using the passthrough layer that merges feature maps from the front layers to get ﬁne-grained features, which can counter the negative effect of reducing the model size. The resulting Tinier-{YOLO} yields a model size of 8.9MB (almost 4× smaller than Tiny-{YOLO}-V3) while achieving 25 {FPS} real-time performance on Jetson {TX}1 and an {mAP} of 65.7\% on {PASCAL} {VOC} and 34.0\% on {COCO}. Tinier-{YOLO} alse posses comparable results in {mAP} and faster runtime speed with smaller model size and {BFLOP}/s value compared with other lightweight models like {SqueezeNet} {SSD} and {MobileNet} {SSD}.},
	pages = {1935--1944},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Fang, Wei and Wang, Lin and Ren, Peiming},
	urldate = {2023-11-07},
	date = {2020},
	langid = {english},
}

@article{goos_lecture_nodate,
	title = {Lecture Notes in Computer Science},
	author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
	langid = {english},
}

@inproceedings{uma_review_2023,
	location = {Trichy, India},
	title = {A Review on Augmented Reality and {YOLO}},
	isbn = {9798350300888},
	url = {https://ieeexplore.ieee.org/document/10275842/},
	doi = {10.1109/ICOSEC58147.2023.10275842},
	abstract = {This study comprehensively examines two emerging technologies, Augmented Reality({AR}) and You Only Look Once ({YOLO}): these two technologies revolutionized object detection and computer vision. {YOLO} instantly identifies objects in images and videos, whereas {AR} superimposes virtual objects in the real world. This review integrates {AR} and {YOLO} and covers the aspects of each, its applications, drawbacks, and future directions. Also, this article explores the fundamentals of augmented reality and several types of {AR} systems, including marker-based, marker-less, and projection-based systems. The review explains how {YOLO} works with single network architecture and real-time detection from {YOLOv}1 through {YOLOv}8. Finally, this study summarizes the key takeaways from {YOLO}'s development and offers insights into its future, outlining potential research directions to enhance real-time object detection systems further. Furthermore, the study highlights the potential of integrating {YOLO} and {AR} to improve user interactions. It also addresses the challenges associated with integrating {AR} and {YOLO}, providing a comprehensive overview of the obstacles that must be overcome.},
	eventtitle = {2023 4th International Conference on Smart Electronics and Communication ({ICOSEC})},
	pages = {1025--1030},
	booktitle = {2023 4th International Conference on Smart Electronics and Communication ({ICOSEC})},
	publisher = {{IEEE}},
	author = {Uma, M. and Abirami, S. and Ambika, M. and Kavitha, M. and Sureshkumar, S and R, Kaviyaraj},
	urldate = {2023-11-07},
	date = {2023-09-20},
	langid = {english},
}

@article{song_review_2021,
	title = {Review and analysis of augmented reality ({AR}) literature for digital fabrication in architecture},
	volume = {128},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580521002132},
	doi = {10.1016/j.autcon.2021.103762},
	abstract = {The use of Augmented Reality ({AR}) technologies has increased recently, due to the equipment update and the mature technology. For architectural design, especially in digital fabrication projects, more designers begin to integrate {AR} methods to achieve the visualization in the process. To help unskilled labors for holographic on-site previewing and instruction training, experimental and practice-based studies in {AR} for the architectural digital fabrication have emerged in recent years. Now, it is a great opportunity to discuss the topic of {AR} in architectural digital fabrication. By presenting a statistical review of {AR} technology in architecture projects, this literature review aims to review ongoing research and provide pathways for further research in architectural digital fabrication. This review article is based on information found in journal publications and conference papers in the fields of architecture, engineering, robotics, and digital fabrication, published to date (from 2010 to 2020). The review narrows the literature within these papers by filtering 84 articles through the keyword “Augmented Reality”, “Digital Fabrication” and “Assembly”. The selected articles can be categorized based on the most use of {AR} function in architectural digital fabrication into an order of the following three classifications with the most significant growth in the last years: (A) {AR} 3D holographic instruction, (B) {AR} data sharing, (C) {AR} for {HumanComputer} interaction. The information collected from these articles within their classifications is meant to give insight into the current state-of-the-art of {AR} in the architectural digital fabrication area, as well as to summarize how the topic has matured and developed over time in the research and industry literature. This article has not only analyzed the existing literature but also highlighted new emerging fields in {AR} research and the future trends of {AR} function in architectural digital fabrication.},
	pages = {103762},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Song, Yang and Koeck, Richard and Luo, Shan},
	urldate = {2023-11-07},
	date = {2021-08},
	langid = {english},
}

@article{ren_edge-computing_2019,
	title = {An Edge-Computing Based Architecture for Mobile Augmented Reality},
	volume = {33},
	issn = {0890-8044, 1558-156X},
	url = {https://ieeexplore.ieee.org/document/8612452/},
	doi = {10.1109/MNET.2018.1800132},
	abstract = {In order to mitigate the long processing delay and high energy consumption of mobile augmented reality ({AR}) applications, mobile edge computing ({MEC}) has been recently proposed and is envisioned as a promising means to deliver better Quality of Experience ({QoE}) for {AR} consumers. In this article, we first present a comprehensive {AR} overview, including the indispensable components of general {AR} applications, fashionable {AR} devices, and several existing techniques for overcoming the thorny latency and energy consumption problems. Then we propose a novel hierarchical computation architecture by inserting an edge layer between the conventional user layer and cloud layer. Based on the proposed architecture, we further develop an innovative operation mechanism to improve the performance of mobile {AR} applications. Three key technologies are also discussed to further assist the proposed {AR} architecture. Simulation results are finally provided to verify that our proposals can significantly improve latency and energy performance as compared to existing baseline schemes.},
	pages = {162--169},
	number = {4},
	journaltitle = {{IEEE} Network},
	shortjournal = {{IEEE} Network},
	author = {Ren, Jinke and He, Yinghui and Huang, Guan and Yu, Guanding and Cai, Yunlong and Zhang, Zhaoyang},
	urldate = {2023-11-07},
	date = {2019-07},
	langid = {english},
}

@article{davila_delgado_research_2020,
	title = {A research agenda for augmented and virtual reality in architecture, engineering and construction},
	volume = {45},
	issn = {14740346},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474034620300914},
	doi = {10.1016/j.aei.2020.101122},
	abstract = {This paper presents a study on the usage landscape of augmented reality ({AR}) and virtual reality ({VR}) in the architecture, engineering and construction sectors, and proposes a research agenda to address the existing gaps in required capabilities. A series of exploratory workshops and questionnaires were conducted with the participation of 54 experts from 36 organisations from industry and academia. Based on the data collected from the workshops, six {AR} and {VR} use-cases were defined: stakeholder engagement, design support, design review, construction support, operations and management support, and training. Three main research categories for a future research agenda have been proposed, i.e.: (i) engineering-grade devices, which encompasses research that enables robust devices that can be used in practice, e.g. the rough and complex conditions of construction sites; (ii) workflow and data management; to effectively manage data and processes required by {AR} and {VR} technologies; and (iii) new capabilities; which includes new research required that will add new features that are necessary for the specific construction industry demands. This study provides essential information for practitioners to inform adoption decisions. To researchers, it provides a research road map to inform their future research efforts. This is a foundational study that formalises and categorises the existing usage of {AR} and {VR} in the construction industry and provides a roadmap to guide future research efforts.},
	pages = {101122},
	journaltitle = {Advanced Engineering Informatics},
	shortjournal = {Advanced Engineering Informatics},
	author = {Davila Delgado, Juan Manuel and Oyedele, Lukumon and Demian, Peter and Beach, Thomas},
	urldate = {2023-11-07},
	date = {2020-08},
	langid = {english},
}

@article{sidani_recent_2021,
	title = {Recent tools and techniques of {BIM}-Based Augmented Reality: A systematic review},
	volume = {42},
	issn = {23527102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352710221003570},
	doi = {10.1016/j.jobe.2021.102500},
	shorttitle = {Recent tools and techniques of {BIM}-Based Augmented Reality},
	abstract = {The Architecture, Engineering, Construction, and Operations ({AECO}) sector has always been recognised as a competitive and complex sector. In recent years, increased demands on construction projects in different do­ mains such as safety, energy, time and cost management have pushed the industry towards new tools and methods, including more efficient use of digital technologies.},
	pages = {102500},
	journaltitle = {Journal of Building Engineering},
	shortjournal = {Journal of Building Engineering},
	author = {Sidani, Adeeb and Matoseiro Dinis, Fábio and Duarte, Joana and Sanhudo, Luís and Calvetti, Diego and Santos Baptista, João and Poças Martins, João and Soeiro, Alfredo},
	urldate = {2023-11-07},
	date = {2021-10},
	langid = {english},
}

@article{von_atzigen_holoyolo_2021,
	title = {{HoloYolo}: A proof-of-concept study for marker-less surgical navigation of spinal rod implants with augmented reality and on-device machine learning},
	volume = {17},
	issn = {1478-596X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rcs.2184},
	doi = {10.1002/rcs.2184},
	shorttitle = {{HoloYolo}},
	abstract = {Background Existing surgical navigation approaches of the rod bending procedure in spinal fusion rely on optical tracking systems that determine the location of placed pedicle screws using a hand-held marker. Methods We propose a novel, marker-less surgical navigation proof-of-concept to bending rod implants. Our method combines augmented reality with on-device machine learning to generate and display a virtual template of the optimal rod shape without touching the instrumented anatomy. Performance was evaluated on lumbosacral spine phantoms against a pointer-based navigation benchmark approach and ground truth data obtained from computed tomography. Results Our method achieved a mean error of 1.83 ± 1.10 mm compared to 1.87 ± 1.31 mm measured in the marker-based approach, while only requiring 21.33 ± 8.80 s as opposed to 36.65 ± 7.49 s attained by the pointer-based method. Conclusion Our results suggests that the combination of augmented reality and machine learning has the potential to replace conventional pointer-based navigation in the future.},
	pages = {e2184},
	number = {1},
	journaltitle = {The International Journal of Medical Robotics and Computer Assisted Surgery},
	author = {von Atzigen, Marco and Liebmann, Florentin and Hoch, Armando and Bauer, David E. and Snedeker, Jess Gerrit and Farshad, Mazda and Fürnstahl, Philipp},
	urldate = {2023-11-07},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rcs.2184},
}

@article{malta_augmented_2021,
	title = {Augmented Reality Maintenance Assistant Using {YOLOv}5},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/11/4758},
	doi = {10.3390/app11114758},
	abstract = {Maintenance professionals and other technical staff regularly need to learn to identify new parts in car engines and other equipment. The present work proposes a model of a task assistant based on a deep learning neural network. A {YOLOv}5 network is used for recognizing some of the constituent parts of an automobile. A dataset of car engine images was created and eight car parts were marked in the images. Then, the neural network was trained to detect each part. The results show that {YOLOv}5s is able to successfully detect the parts in real time video streams, with high accuracy, thus being useful as an aid to train professionals learning to deal with new equipment using augmented reality. The architecture of an object recognition system using augmented reality glasses is also designed.},
	pages = {4758},
	number = {11},
	journaltitle = {Applied Sciences},
	author = {Malta, Ana and Mendes, Mateus and Farinha, Torres},
	urldate = {2023-11-07},
	date = {2021-01},
	langid = {english},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
}

@article{kohli_review_2022,
	title = {A review on Virtual Reality and Augmented Reality use-cases of Brain Computer Interface based applications for smart cities},
	volume = {88},
	issn = {01419331},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933121005391},
	doi = {10.1016/j.micpro.2021.104392},
	abstract = {Brain Computer Interfaces ({BCIs}) and Extended Reality ({XR}) have seen significant advances as independent disciplines over the past 50 years. {XR} has been developed as an umbrella domain, covering Virtual Reality ({VR}), Augmented Reality ({AR}) and Mixed Reality ({MR}), giving rise to human–machine interactions. This intersection sees diverse applications ranging from rehabilitation, navigation, entertainment, robotics and home control for smart cities. This review takes an in-depth look at {BCI} and {XR} technologies, and gives examples of how their combination produces promising results pertaining to the above stated applications. It presents a detailed discussion on the background of {BCI}, {VR} and {AR} technologies and further their individual applications. The review then discusses the works that use the conjunction of these technologies for various real life applications in smart cities. In addition, we also present the future scope of applications that use a combination of {BCI} and {XR} technologies.},
	pages = {104392},
	journaltitle = {Microprocessors and Microsystems},
	shortjournal = {Microprocessors and Microsystems},
	author = {Kohli, Varun and Tripathi, Utkarsh and Chamola, Vinay and Rout, Bijay Kumar and Kanhere, Salil S.},
	urldate = {2023-11-07},
	date = {2022-02},
	langid = {english},
}

@article{lee_towards_2022,
	title = {Towards Augmented Reality Driven Human-City Interaction: Current Research on Mobile Headsets and Future Challenges},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3467963},
	doi = {10.1145/3467963},
	shorttitle = {Towards Augmented Reality Driven Human-City Interaction},
	abstract = {Interaction design for Augmented Reality ({AR}) is gaining attention from both academia and industry. This survey discusses 260 articles (68.8\% of articles published between 2015–2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by reviewing the latest trends of information visualization, constrained interfaces, and embodied interaction for {AR} headsets. We highlight under-explored issues in interface design and input techniques that warrant further research and conjecture that {AR} with complementary Conversational User Interfaces ({CUIs}) is a crucial enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of {AR} in Human-City Interaction.},
	pages = {1--38},
	number = {8},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Lee, Lik-Hang and Braud, Tristan and Hosio, Simo and Hui, Pan},
	urldate = {2023-11-07},
	date = {2022-11-30},
	langid = {english},
}

@article{litvak_enhancing_2020,
	title = {Enhancing cultural heritage outdoor experience with augmented-reality smart glasses},
	volume = {24},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-020-01366-7},
	doi = {10.1007/s00779-020-01366-7},
	abstract = {Technologies such as context-aware mobile augmented reality ({MAR}) offer new ways of delivering information and creating new experiences for visitors of cultural heritage ({CH}) sites. Significant progress has been made in {MAR} for indoor museum visits. However, studies investigating the way that visitors interact with {MAR} in an open-air museum are lacking, as are studies aimed at identifying major usability and technology acceptance issues. This paper presents a special type of {MAR}—an augmented reality smart glasses ({ARSG})-based mobile guide that served as a testbed for exploring the potential of {ARSG} to enhance visitor experience at outdoor {CH} sites. We developed the {ARSG}-based guide with location and orientation-based functionalities that provided visitors with context-aware information regarding the points of interest ({POIs}) in their field of view ({FOV}). We compared it with a more conventional smartphone-based guide (identical application). The results show that visitors are positive towards using {ARSG} as a tool for exploring cultural heritage ({CH}) sites and learning about exhibits. The results also highlighted several challenges that must be overcome before an {ARSG}-based system can be fully implemented for outdoor {CH} sites. Nonetheless, the visitors’ positive attitude indicates that {ARSG} technology may soon become an accepted part of {CH} tourism (and tourism in general).},
	pages = {873--886},
	number = {6},
	journaltitle = {Personal and Ubiquitous Computing},
	shortjournal = {Pers Ubiquit Comput},
	author = {Litvak, Eran and Kuflik, Tsvi},
	urldate = {2023-11-07},
	date = {2020-12},
	langid = {english},
}

@article{banfi_extended_2019,
	title = {Extended reality and informative models for the architectural heritage: from scan-to-{BIM} process to virtual and augmented reality},
	volume = {10},
	issn = {1989-9947},
	url = {https://polipapers.upv.es/index.php/var/article/view/11923},
	doi = {10.4995/var.2019.11923},
	shorttitle = {Extended reality and informative models for the architectural heritage},
	abstract = {The dissemination of the tangible and intangible values of heritage building represents one of the most important objectives in the field of Digital Cultural Heritage ({DCH}). In recent years, different studies and research applied to heritage monuments have shown how it is possible to improve the awareness of the architectural heritage through the integration of latest developments in the field of 3D survey, 3D modelling, Building Information Modeling ({BIM}) and {eXtended} Reality ({XR}). On the other hand, this digital workflow requires a huge amount of data sources and a holistic approach to reach a high level of information sharing coming from different disciplines and sectors such as restoration, geomatics, 3D virtual museums and serious gaming. In conjunction with entertainment software and gaming, this research shows the main results obtained during the generative process of digital environments oriented to improve the level of information and to enrich the contents coming from the informative models. The case study is represented by one of the most important Lombard monuments: the Basilica of Sant’Ambrogio in Milan. This study, starting from the 3D survey and the data collection of the historical records of the church, improves the creation of an {XR} experience that reaches a new level of interactivity for different types of devices (desktop, mobile, {VR} headset) and users (experts, non-experts).},
	pages = {14},
	number = {21},
	journaltitle = {Virtual Archaeology Review},
	shortjournal = {Virtual archaeol. rev.},
	author = {Banfi, Fabrizio and Brumana, Raffaella and Stanga, Chiara},
	urldate = {2023-11-07},
	date = {2019-07-25},
	langid = {english},
}

@article{han_compelling_2020,
	title = {A Compelling Virtual Tour of the Dunhuang Cave With an Immersive Head-Mounted Display},
	volume = {40},
	issn = {0272-1716, 1558-1756},
	url = {https://ieeexplore.ieee.org/document/8821384/},
	doi = {10.1109/MCG.2019.2936753},
	abstract = {The Dunhuang Caves are the home to the largest Buddhist art sites in the world and are listed as a {UNESCO} World Heritage Site. Over time, the murals have been damaged by both humans and nature. In this article, we present an immersive virtual reality system for exploring spatial cultural heritage, which utilizes the digitized data from the Dunhuang Research Academy to represent the virtual environment of the cave. In this system, the interaction techniques that allow users to ﬂexibly experience any of the artifacts or displays contribute to their understanding of the cultural heritage. Additionally, we evaluated the system by conducting a user study to examine the extent of user acquaintance after the entire experience. Our result has shown what participants learn from the spatial context and augmented information in the {VR}. This can be used as design considerations for developing other spatial heritages.},
	pages = {40--55},
	number = {1},
	journaltitle = {{IEEE} Computer Graphics and Applications},
	shortjournal = {{IEEE} Comput. Grap. Appl.},
	author = {Han, Ping-Hsuan and Chen, Yang-Sheng and Liu, Iou-Shiuan and Jang, Yu-Ping and Tsai, Ling and Chang, Alvin and Hung, Yi-Ping},
	urldate = {2023-11-07},
	date = {2020-01-01},
	langid = {english},
}

@article{paliokas_gamified_2020,
	title = {A Gamified Augmented Reality Application for Digital Heritage and Tourism},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/21/7868},
	doi = {10.3390/app10217868},
	abstract = {Although Augmented Reality ({AR}) technology has entered many market and knowledge domains such as games and leisure activities, it remains rather limited in digital heritage. After studying the potentiality of using modern {AR} elements in a museum context, this paper proposes the use of additional game and educational elements in the core {AR} application in order to enhance the overall on-the-spot museum visitor’s experience. An agile {AR} application design methodology was followed by taking into account the needs of small-to-medium sized real-world museums. Moreover, a heuristic evaluation protocol was applied by a group of experts in order to test the proof-of-concept {AR} application, in which some novel elements were proposed such as the {AR} quiz game. The main ﬁndings indicate that enhanced {AR} experiences in museum settings can make a nice ﬁt with the user environment, physical and perceptual abilities, known metaphors, and user position and motion in 3D space. Moreover, {AR} services can be provided under a minimum distraction and physical eﬀort. As a conclusion, {AR} technologies are mature enough to be standardized for museum usage, while the audience seems to be ready to take advantage of the related enhanced museum experiences to maximize both user satisfaction and learning outcomes.},
	pages = {7868},
	number = {21},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Paliokas, Ioannis and Patenidis, Athanasios T. and Mitsopoulou, Eirini E. and Tsita, Christina and Pehlivanides, George and Karyati, Elli and Tsafaras, Spyros and Stathopoulos, Evangelos A. and Kokkalas, Alexandros and Diplaris, Sotiris and Meditskos, Georgios and Vrochidis, Stefanos and Tasiopoulou, Eleana and Riggas, Christodoulos and Votis, Konstantinos and Kompatsiaris, Ioannis and Tzovaras, Dimitrios},
	urldate = {2023-11-07},
	date = {2020-11-06},
	langid = {english},
}

@article{xiong_augmented_2021,
	title = {Augmented reality and virtual reality displays: emerging technologies and future perspectives},
	volume = {10},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-021-00658-8},
	doi = {10.1038/s41377-021-00658-8},
	shorttitle = {Augmented reality and virtual reality displays},
	abstract = {With rapid advances in high-speed communication and computation, augmented reality ({AR}) and virtual reality ({VR}) are emerging as next-generation display platforms for deeper human-digital interactions. Nonetheless, to simultaneously match the exceptional performance of human vision and keep the near-eye display module compact and lightweight imposes unprecedented challenges on optical engineering. Fortunately, recent progress in holographic optical elements ({HOEs}) and lithography-enabled devices provide innovative ways to tackle these obstacles in {AR} and {VR} that are otherwise difﬁcult with traditional optics. In this review, we begin with introducing the basic structures of {AR} and {VR} headsets, and then describing the operation principles of various {HOEs} and lithographyenabled devices. Their properties are analyzed in detail, including strong selectivity on wavelength and incident angle, and multiplexing ability of volume {HOEs}, polarization dependency and active switching of liquid crystal {HOEs}, device fabrication, and properties of micro-{LEDs} (light-emitting diodes), and large design freedoms of metasurfaces.},
	pages = {216},
	number = {1},
	journaltitle = {Light: Science \& Applications},
	shortjournal = {Light Sci Appl},
	author = {Xiong, Jianghao and Hsiang, En-Lin and He, Ziqian and Zhan, Tao and Wu, Shin-Tson},
	urldate = {2023-11-07},
	date = {2021-10-25},
	langid = {english},
}

@article{de_souza_cardoso_survey_2020,
	title = {A survey of industrial augmented reality},
	volume = {139},
	issn = {03608352},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S036083521930628X},
	doi = {10.1016/j.cie.2019.106159},
	abstract = {This article aims to evaluate the impact of Augmented Reality ({AR}) applicability and usefulness on real industrial processes by employing a systematic literature review ({SLR}). The {SLR} was performed in five digital libraries to identify articles and reviews concerning the {AR} applicability from 2012 to 2018. A patent search in Google’s patents database was also conducted, for the same period. This paper describes how {AR} has been applied, which industries are most interested in the technology, how the technology has been developed to meet industry needs, as well as the benefits and challenges of {AR}. This survey concludes by providing a starting point for companies interested in integrating {AR} into their processes and proposing future directions for {AR} developers and researchers.},
	pages = {106159},
	journaltitle = {Computers \& Industrial Engineering},
	shortjournal = {Computers \& Industrial Engineering},
	author = {De Souza Cardoso, Luís Fernando and Mariano, Flávia Cristina Martins Queiroz and Zorzal, Ezequiel Roberto},
	urldate = {2023-11-07},
	date = {2020-01},
	langid = {english},
}

@article{noauthor_supporting_nodate,
	title = {Supporting data for Deep Learning and Machine Vision based approaches for automated wood defect detection and quality control.},
	url = {https://zenodo.org/records/4694695},
	doi = {10.5281/zenodo.4694695},
	abstract = {The dataset contains more than 43 000 labeled wood surface defects and covers overall ten types of the most common defects, including live knots, dead knots, knots with crack, cracks, resins, marrows, quartzity, missing knots, blue stain, and overgrown. Each image in the dataset is provided with a semantic map and a bounding box label that allows performing semantic segmentation as well as localization tasks. All data were collected directly from a wood production line during the manufacturing process.},
	urldate = {2023-11-05},
	langid = {english},
	keywords = {dataset},
}

@misc{oshea_introduction_2015,
	title = {An Introduction to Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1511.08458},
	abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network ({ANN}). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of {ANN} architecture is that of the Convolutional Neural Network ({CNN}). {CNNs} are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with {ANNs}. This document provides a brief introduction to {CNNs}, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of {ANNs} and machine learning.},
	number = {{arXiv}:1511.08458},
	publisher = {{arXiv}},
	author = {O'Shea, Keiron and Nash, Ryan},
	urldate = {2023-11-02},
	date = {2015-12-02},
	eprinttype = {arxiv},
	eprint = {1511.08458 [cs]},
}

@article{lecun_convolutional_nodate,
	title = {Convolutional Networks for Images, Speech, and Time-Series},
	author = {{LeCun}, Yann and Bengio, Yoshua and Laboratories, T Bell},
	langid = {english},
}

@inproceedings{landrieu_large-scale_2018,
	location = {Salt Lake City, {UT}},
	title = {Large-Scale Point Cloud Semantic Segmentation with Superpoint Graphs},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578577/},
	doi = {10.1109/CVPR.2018.00479},
	abstract = {We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of largescale point clouds of millions of points. We argue that the organization of 3D point clouds can be efﬁciently captured by a structure called superpoint graph ({SPG}), derived from a partition of the scanned scene into geometrically homogeneous elements. {SPGs} offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor {LiDAR} scans (+11.9 and +8.8 {mIoU} points for both Semantic3D test sets), as well as indoor scans (+12.4 {mIoU} points for the S3DIS dataset).},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {4558--4567},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Landrieu, Loic and Simonovsky, Martin},
	urldate = {2023-11-01},
	date = {2018-06},
	langid = {english},
}

@article{zhang_review_2019,
	title = {A Review of Deep Learning-Based Semantic Segmentation for Point Cloud},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8930503/},
	doi = {10.1109/ACCESS.2019.2958671},
	abstract = {In recent years, the popularity of depth sensors and 3D scanners has led to a rapid development of 3D point clouds. Semantic segmentation of point cloud, as a key step in understanding 3D scenes, has attracted extensive attention of researchers. Recent advances in this topic are dominantly led by deep learning-based methods. In this paper, we provide a survey covering various aspects ranging from indirect segmentation to direct segmentation. Firstly, we review methods of indirect segmentation based on multi-views and voxel grids, as well as direct segmentation methods from different perspectives including point ordering, multi-scale, feature fusion and fusion of graph convolutional neural network ({GCNN}). Then, the common datasets for point cloud segmentation are exposed to help researchers choose which one is the most suitable for their tasks. Following that, we devote a part of the paper to analyze the quantitative results of these methods. Finally, the development trend of point cloud semantic segmentation technology is prospected.},
	pages = {179118--179133},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Zhang, Jiaying and Zhao, Xiaoli and Chen, Zheng and Lu, Zhejun},
	urldate = {2023-11-01},
	date = {2019},
	langid = {english},
}

@online{noauthor_-cnn_2019,
	title = {一文看懂卷积神经网络-{CNN}（基本原理+独特价值+实际应用）},
	url = {https://easyai.tech/ai-definition/cnn/},
	abstract = {卷积神经网络 - {CNN} 最擅长的就是图片的处理。它受到人类视觉神经系统的启发。{CNN} 有2大特点：1. 能够有效的将大数据量的图片降维成小数据量2. 能够有效的保留图片特征，符合图片处理的原则目前 {CNN} 已经得到了广泛的应用，比如：人脸识别、自动驾驶、美图秀秀、安防等很多领域。},
	titleaddon = {产品经理的人工智能学习库},
	urldate = {2023-11-01},
	date = {2019-01-03},
	langid = {pinyin},
}

@online{noauthor_yolo_nodate,
	title = {{YOLO} models for Object Detection Explained [{YOLOv}8 Updated]},
	url = {https://encord.com/blog/yolo-object-detection-guide/},
	abstract = {What is {YOLO} (You Only Look Once)?  One of the most, if not the most, well-known models in Artificial intelligence ({AI}) is the “{YOLO}” model seri},
	urldate = {2023-11-01},
	langid = {pinyin},
}

@online{noauthor_yolov8_nodate,
	title = {{YOLOv}8: A New State-of-the-Art Computer Vision Model},
	url = {https://yolov8.com/},
	urldate = {2023-11-01},
}

@article{konstantinidis_multi-modal_2023,
	title = {Multi-modal sorting in plastic and wood waste streams},
	volume = {199},
	issn = {09213449},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921344923003786},
	doi = {10.1016/j.resconrec.2023.107244},
	pages = {107244},
	journaltitle = {Resources, Conservation and Recycling},
	shortjournal = {Resources, Conservation and Recycling},
	author = {Konstantinidis, Fotios K. and Sifnaios, Savvas and Arvanitakis, George and Tsimiklis, Georgios and Mouroutsos, Spyridon G. and Amditis, Angelos and Gasteratos, Antonios},
	urldate = {2023-11-01},
	date = {2023-12},
	langid = {english},
}

@article{wang_tsw-yolo-v8n_2023,
	title = {{TSW}-{YOLO}-v8n: Optimization of Detection Algorithms for Surface Defects on Sawn Timber},
	volume = {18},
	rights = {Copyright (c) 2023 Mingtao Wang, Mingxi Li, Wenyan Cui, Xiaoyang Xiang, Huaqiong Duo},
	issn = {1930-2126},
	url = {https://ojs.cnr.ncsu.edu/index.php/BRJ/article/view/22962},
	shorttitle = {{TSW}-{YOLO}-v8n},
	abstract = {The goal of this work was to better meet the demand for rapid detection of surface defects in sawn timber in forestry production. This paper introduces a two-way feature fusion network based on the {YOLO}-v8 algorithm and proposes a feature fusion network model that combines the attention mechanism and loss function optimization. In this way it increases the tiny target detection head in order to more effectively detect small defective targets in the wood, thus realizing the model's high-efficiency and low-consumption functional design. The results show that the improved {TSW}-{YOLO}-v8n model realized the identification of eight kinds of defects in sawn timber with a high efficiency of 91.10\% {mAP}50 and an average detection 6 ms, which is 5.1\% higher than the original model’s {mAP}50 and 1 ms shorter than the original model’s average detection time. The comparison of the original model and its mainstream algorithms shows that the model of this paper had better performance and better detection capability. Thus, the improved model achieved better overall performance and stronger detection ability, which provides a new idea for the development of detection technology in the forestry industry.},
	pages = {8444--8457},
	number = {4},
	journaltitle = {{BioResources}},
	author = {Wang, Mingtao and Li, Mingxi and Cui, Wenyan and Xiang, Xiaoyang and Duo, Huaqiong},
	urldate = {2023-11-01},
	date = {2023-10-26},
	langid = {english},
	note = {Number: 4},
}

@article{wan_pointnest_2023,
	title = {{PointNest}: Learning Deep Multiscale Nested Feature Propagation for Semantic Segmentation of 3-D Point Clouds},
	volume = {16},
	issn = {1939-1404, 2151-1535},
	url = {https://ieeexplore.ieee.org/document/10251503/},
	doi = {10.1109/JSTARS.2023.3315557},
	shorttitle = {{PointNest}},
	pages = {9051--9066},
	journaltitle = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	shortjournal = {{IEEE} J. Sel. Top. Appl. Earth Observations Remote Sensing},
	author = {Wan, Jie and Zeng, Ziyin and Qiu, Qinjun and Xie, Zhong and Xu, Yongyang},
	urldate = {2023-10-23},
	date = {2023},
	langid = {english},
}

@inproceedings{charles_pointnet_2017,
	location = {Honolulu, {HI}},
	title = {{PointNet}: Deep Learning on Point Sets for 3D Classification and Segmentation},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099499/},
	doi = {10.1109/CVPR.2017.16},
	shorttitle = {{PointNet}},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named {PointNet}, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, {PointNet} is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {77--85},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
	urldate = {2023-09-21},
	date = {2017-07},
	langid = {english},
	keywords = {{ObsCite}, pointnet},
}

@inproceedings{aoki_pointnetlk_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {{PointNetLK}: Robust \& Efficient Point Cloud Registration Using {PointNet}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954359/},
	doi = {10.1109/CVPR.2019.00733},
	shorttitle = {{PointNetLK}},
	abstract = {{PointNet} has revolutionized how we think about representing point clouds. For classiﬁcation and segmentation tasks, the approach and its subsequent extensions are stateof-the-art. To date, the successful application of {PointNet} to point cloud registration has remained elusive. In this paper we argue that {PointNet} itself can be thought of as a learnable “imaging” function. As a consequence, classical vision algorithms for image alignment can be applied on the problem – namely the Lucas \& Kanade ({LK}) algorithm. Our central innovations stem from: (i) how to modify the {LK} algorithm to accommodate the {PointNet} imaging function, and (ii) unrolling {PointNet} and the {LK} algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efﬁciency – opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https: //github.com/hmgoforth/{PointNetLK} .},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {7156--7165},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Aoki, Yasuhiro and Goforth, Hunter and Srivatsan, Rangaprasad Arun and Lucey, Simon},
	urldate = {2023-09-21},
	date = {2019-06},
	langid = {english},
}

@article{zheng_fourier_2019,
	title = {A Fourier Descriptor of 2D Shapes Based on Multiscale Centroid Contour Distances Used in Object Recognition in Remote Sensing Images},
	volume = {19},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/19/3/486},
	doi = {10.3390/s19030486},
	abstract = {A shape descriptor is an effective tool for describing the shape feature of an object in remote sensing images. Researchers have put forward a lot of excellent descriptors. The discriminability of some descriptors is very strong in the experiments, but usually their computational cost is large, which makes them unsuitable to be used in practical applications. This paper proposes a new descriptor-{FMSCCD} (Fourier descriptor based on multiscale centroid contour distance)—which is a frequency domain descriptor based on the {CCD} (centroid contour distance) method, multiscale description, and Fourier transform. The principle of {FMSCCD} is simple, and the computational cost is very low. What is commendable is that its discriminability is still strong, and its compatibility with other features is also great. Experiments on three databases demonstrate its strong discriminability and operational efficiency.},
	pages = {486},
	number = {3},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Zheng, Yan and Guo, Baolong and Chen, Zhijie and Li, Cheng},
	urldate = {2023-08-31},
	date = {2019-01-24},
	langid = {english},
}

@article{zhang_shape_nodate,
	title = {Shape Retrieval Using Fourier Descriptors},
	abstract = {Shape is one of the most important features in Content Based Image Retrieval ({CBIR}). Many shape representations and retrieval methods exists. However, most of those methods either do not well represent shape or are difficult to do normalization (making matching hard). Among them, methods based Fourier descriptors ({FD}) achieve both well representation and well normalization. Different shape signatures have been exploited to derive {FDs}, however, {FDs} derived from different signatures can have significant different effect on the result of retrieval. In this paper, we build a Java retrieval framework to compare shape retrieval using {FDs} derived from different signatures. Common issues and techniques for shape representation and normalization are also analyzed in the paper. Data is given to show the retrieval result.},
	author = {Zhang, Dengsheng and Lu, Guojun},
	langid = {english},
}

@article{yang_surface_2021,
	title = {Surface Detection of Solid Wood Defects Based on {SSD} Improved with {ResNet}},
	volume = {12},
	issn = {1999-4907},
	url = {https://www.mdpi.com/1999-4907/12/10/1419},
	doi = {10.3390/f12101419},
	abstract = {Due to the lack of forest resources in China and the low detection efﬁciency of wood surface defects, the output of solid wood panels is not high. Therefore, this paper proposes a method for detecting surface defects of solid wood panels based on a Single Shot {MultiBox} Detector algorithm ({SSD}) to detect typical wood surface defects. The wood panel images are acquired by an independently designed image acquisition system. The {SSD} model included the ﬁrst ﬁve layers of the {VGG}16 network, the {SSD} feature mapping layer, the feature detection layer, and the Non-Maximum Suppression ({NMS}) module. We used {TensorFlow} to train the network and further improved it on the basis of the {SSD} network structure. As the basic network part of the improved {SSD} model, the deep residual network ({ResNet}) replaced the {VGG} network part of the original {SSD} network to optimize the input features of the regression and classiﬁcation tasks of the predicted bounding box. The solid wood panels selected in this paper are Chinese ﬁr and pine. The defects include live knots, dead knots, decay, mildew, cracks, and pinholes. A total of more than 5000 samples were collected, and the data set was expanded to 100,000 through data enhancement methods. After using the improved {SSD} model, the average detection accuracy of the defects we obtained was 89.7\%, and the average detection time was 90 ms. Both the detection accuracy and the detection speed were improved.},
	pages = {1419},
	number = {10},
	journaltitle = {Forests},
	shortjournal = {Forests},
	author = {Yang, Yutu and Wang, Honghong and Jiang, Dong and Hu, Zhongkang},
	urldate = {2023-08-31},
	date = {2021-10-18},
	langid = {english},
}

@article{xu_recognition_2020,
	title = {Recognition and Grasping of Disorderly Stacked Wood Planks Using a Local Image Patch and Point Pair Feature Method},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/21/6235},
	doi = {10.3390/s20216235},
	abstract = {Considering the diﬃcult problem of robot recognition and grasping in the scenario of disorderly stacked wooden planks, a recognition and positioning method based on local image features and point pair geometric features is proposed here and we deﬁne a local patch point pair feature. First, we used self-developed scanning equipment to collect images of wood boards and a robot to drive a {RGB}-D camera to collect images of disorderly stacked wooden planks. The image patches cut from these images were input to a convolutional autoencoder to train and obtain a local texture feature descriptor that is robust to changes in perspective. Then, the small image patches around the point pairs of the plank model are extracted, and input into the trained encoder to obtain the feature vector of the image patch, combining the point pair geometric feature information to form a feature description code expressing the characteristics of the plank. After that, the robot drives the {RGB}-D camera to collect the local image patches of the point pairs in the area to be grasped in the scene of the stacked wooden planks, also obtaining the feature description code of the wooden planks to be grasped. Finally, through the process of point pair feature matching, pose voting and clustering, the pose of the plank to be grasped is determined. The robot grasping experiment here shows that both the recognition rate and grasping success rate of planks are high, reaching 95.3\% and 93.8\%, respectively. Compared with the traditional point pair feature method ({PPF}) and other methods, the method present here has obvious advantages and can be applied to stacked wood plank grasping environments.},
	pages = {6235},
	number = {21},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Xu, Chengyi and Liu, Ying and Ding, Fenglong and Zhuang, Zilong},
	urldate = {2023-08-31},
	date = {2020-10-31},
	langid = {english},
}

@article{wood_applying_1990,
	title = {Applying Fourier and Associated Transforms to Pattern Characterization in Textiles},
	volume = {60},
	issn = {0040-5175, 1746-7748},
	url = {http://journals.sagepub.com/doi/10.1177/004051759006000404},
	doi = {10.1177/004051759006000404},
	abstract = {The regular periodic nature of many textile patterns permits Fourier transform techniques in image processing to be used to measure their visual characteristics. In carpets, the patterns may be due to either the arrangement of the pile or the repetition of a colored design. The Fourier power spectrum provides a useful description of the spatial frequency content in a digital image, and in particular the coarseness of any texture present. It is also an intermediate step in deriving the two-dimensional autocorrelation function, which graphically describes the translational and rotational symmetry of an image. The cross-correlation function enables comparisons of similar patterns to be made and gives a means of measuring the changes in pattern definition that arise from wear. The low or high frequency components in an image can be suppressed with appropriate Fourier masks, allowing the pile texture or the colored pattern in a patterned carpet to be enhanced. This sort of transformation permits image processing methods that have proved useful in measuring the pile texture in plain carpets to be applied to patterned types.},
	pages = {212--220},
	number = {4},
	journaltitle = {Textile Research Journal},
	shortjournal = {Textile Research Journal},
	author = {Wood, Errol J.},
	urldate = {2023-08-31},
	date = {1990-04},
	langid = {english},
}

@article{wang_color_2021,
	title = {Color Classification and Texture Recognition System of Solid Wood Panels},
	volume = {12},
	issn = {1999-4907},
	url = {https://www.mdpi.com/1999-4907/12/9/1154},
	doi = {10.3390/f12091154},
	abstract = {Solid wood panels are widely used in the wood ﬂooring and furniture industries, and paneling is an excellent material for indoor decoration. The classiﬁcation of colors helps to improve the appearance of wood products assembled from multiple panels due to the differences in surface colors of solid wood panels. Traditional wood surface color classiﬁcation mainly depends on workers’ visual observations, and manual color classiﬁcation is prone to visual fatigue and quality instability. In order to reduce labor costs of sorting and to improve production efﬁciency, in this study, we introduced machine vision technology and an unsupervised learning technique. First-order color moments, second-order color moments, and color histogram peaks were selected to extract feature vectors and to realize data dimension reduction. The feature vector set was divided into different clusters by the K-means algorithm to achieve color classiﬁcation and, thus, the solid wood panels with similar surface color were classiﬁed into one category. Furthermore, during twice clustering based on second-order color moment, texture recognition was realized on the basis of color classiﬁcation. A sample of beech wood was selected as the research object, not only was color classiﬁcation completed, but texture recognition was also realized. The experimental results veriﬁed the effectiveness of the technical proposal.},
	pages = {1154},
	number = {9},
	journaltitle = {Forests},
	shortjournal = {Forests},
	author = {Wang, Zhengguang and Zhuang, Zilong and Liu, Ying and Ding, Fenglong and Tang, Min},
	urldate = {2023-08-31},
	date = {2021-08-26},
	langid = {english},
}

@article{wang_wood_2013,
	title = {Wood Recognition Using Image Texture Features},
	volume = {8},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0076101},
	doi = {10.1371/journal.pone.0076101},
	abstract = {Inspired by theories of higher local order autocorrelation ({HLAC}), this paper presents a simple, novel, yet very powerful approach for wood recognition. The method is suitable for wood database applications, which are of great importance in wood related industries and administrations. At the feature extraction stage, a set of features is extracted from Mask Matching Image ({MMI}). The {MMI} features preserve the mask matching information gathered from the {HLAC} methods. The texture information in the image can then be accurately extracted from the statistical and geometrical features. In particular, richer information and enhanced discriminative power is achieved through the length histogram, a new histogram that embodies the width and height histograms. The performance of the proposed approach is compared to the state-of-the-art {HLAC} approaches using the wood stereogram dataset {ZAFU} {WS} 24. By conducting extensive experiments on {ZAFU} {WS} 24, we show that our approach significantly improves the classification accuracy.},
	pages = {e76101},
	number = {10},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Wang, Hang-jun and Zhang, Guang-qun and Qi, Heng-nian},
	editor = {Wu, Rongling},
	urldate = {2023-08-31},
	date = {2013-10-11},
	langid = {english},
}

@article{treccani_deep_2022,
	title = {A {DEEP} {LEARNING} {APPROACH} {FOR} {THE} {RECOGNITION} {OF} {URBAN} {GROUND} {PAVEMENTS} {IN} {HISTORICAL} {SITES}},
	volume = {{XLIII}-B4-2022},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B4-2022/321/2022/},
	doi = {10.5194/isprs-archives-XLIII-B4-2022-321-2022},
	abstract = {Urban management is a topic of great interest for local administrators, particularly because it is strongly connected to smart city issues and can have a great impact on making cities more sustainable. In particular, thinking about the management of the physical accessibility of cities, the possibility of automating data collection in urban areas is of great interest. Focusing then on historical centres and urban areas of cities and historical sites, it can be noted that their ground surfaces are generally characterised by the use of a multitude of different pavements. To strengthen the management of such urban areas, a comprehensive mapping of the different pavements can be very useful. In this paper, the survey of a historical city (Sabbioneta, in northern Italy) carried out with a Mobile Mapping System ({MMS}) was used as a starting point. The approach here presented exploit Deep Learning ({DL}) to classify the different pavings. Firstly, the points belonging to the ground surfaces of the point cloud were selected and the point cloud was rasterised. Then the raster images were used to perform a material classification using the Deep Learning approach, implementing U-Net coupled with {ResNet} 18. Five different classes of materials were identified, namely sampietrini, bricks, cobblestone, stone, asphalt. The average accuracy of the result is 94\%.},
	pages = {321--326},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Treccani, D. and Balado, J. and Fernández, A. and Adami, A. and Díaz-Vilariño, L.},
	urldate = {2023-08-31},
	date = {2022-06-01},
	langid = {english},
}

@inproceedings{sarfraz_object_2009,
	location = {Malacca, Malaysia},
	title = {Object Recognition Using Fourier Descriptors and Genetic Algorithm},
	isbn = {978-1-4244-5330-6},
	url = {http://ieeexplore.ieee.org/document/5370987/},
	doi = {10.1109/SoCPaR.2009.70},
	abstract = {This work presents study and experimentation for object recognition when isolated objects are under discussion. The circumstances of similarity transformations, presence of noise, and occlusion have been included as the part of the study. For simplicity, instead of objects, outlines of the objects have been used for the whole process of the recognition. Fourier Descriptors have been used as features of the objects. From the analysis and results using Fourier Descriptors, the following questions arise: What is the optimum number of descriptors to be used? Are these descriptors of equal importance? To answer these questions, the problem of selecting the best descriptors has been formulated as an optimization problem. Genetic Algorithm technique has been mapped and used successfully to have an object recognition system using minimal number of Fourier Descriptors. The proposed method assigns, for each of these descriptors, a weighting factor that reflects the relative importance of that descriptor.},
	eventtitle = {2009 International Conference of Soft Computing and Pattern Recognition},
	pages = {318--323},
	booktitle = {2009 International Conference of Soft Computing and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Sarfraz, M. and {Mehmood-ul-Hassan} and Iqbal, M.},
	urldate = {2023-08-31},
	date = {2009},
	langid = {english},
}

@article{salvador_garcia_use_2018,
	title = {The use of {HBIM} models as a tool for dissemination and public use management of historical architecture: A review},
	volume = {13},
	issn = {1743-7601, 1743-761X},
	url = {http://www.witpress.com/doi/journals/SDP-V13-N1-96-107},
	doi = {10.2495/SDP-V13-N1-96-107},
	shorttitle = {The use of {HBIM} models as a tool for dissemination and public use management of historical architecture},
	abstract = {Disseminating detailed and accessible information about the built heritage is fundamental to help understand the value and meaning of that heritage to society. Recent research highlights the potential of the Historic Building Information Modeling ({HBIM}) system for managing and disseminating heritage. However, this area is still in an early stage of development. This study aims to present a state-of-the-art review on the use of {HBIM} for both disseminating the value of historic architecture and managing the public use of heritage assets. The research methodology consisted of extensive bibliographic computer database searches of the study topic through specialized search tools. To this end, significant keywords have been used, such as {HBIM} to culture dissemination, {HBIM} to {ICT} and {HBIM} to public use management, among others. A total of 85 papers were initially selected. After a preliminary reading, 37 studies have been selected for this review. Later, a quantitative and critical analysis identifying the main themes and perspectives of these academic papers has been carried out. The results indicate that technological solutions have been developed to convert {HBIM} into models suitable for portable devices, the purpose of which is to offer virtual tours ({VR}) and augmented reality ({AR}) applications, as well as architectural heritage maintenance applications. There are also gaps in knowledge that have allowed us to establish a starting point for future research. As a conclusion, we can advance that it is essential to further the heritage interpretation of {HBIM} information so that the non-expert public can better understand architectural assets and their history. Also observable is the need for optimizing {HBIM} processes for heritage diffusion and public use management. Finally, we also noted it would be very useful to identify the most efficient way of collecting and processing data in order to achieve the best {HBIM} technical information for dissemination purposes.},
	pages = {96--107},
	number = {1},
	journaltitle = {International Journal of Sustainable Development and Planning},
	shortjournal = {Int. J. {SDP}},
	author = {Salvador García, Elena and García-Valldecabres, Jorge and Viñals Blasco, María José},
	urldate = {2023-08-31},
	date = {2018-01-01},
	langid = {english},
}

@article{sahebkheir_single_2023,
	title = {{SINGLE} {IMAGE} {SUPER} {RESOLUTION} {VIA} {COUPLED} {SPARSE} {AND} {LOW} {RANK} {DICTIONARY} {LEARNING}},
	volume = {X-4/W1-2022},
	issn = {2194-9050},
	url = {https://isprs-annals.copernicus.org/articles/X-4-W1-2022/661/2023/},
	doi = {10.5194/isprs-annals-X-4-W1-2022-661-2023},
	abstract = {Limitations in imaging systems and the effects of changes in sensing have caused limitation in acquiring high resolution images such as satellite images and magnetic resonance imaging ({MRI}). Sparsity can reduce the noises and improve the resolution. Super resolution in medical and satellite imagery is essential because low resolution image analysis is very difficult. Sparsity techniques have significant influence on computer vision specially when the main objective is extracting the meaningful information. The success of sparsity is related to the nature of signals such as image and sound which are naturally sparse because they were founded based on Wavelet and Fourier equations. In this research, we proposed a method for restoring a clear image from the related low-resolution parts of both {MRI} and satellite images. First, we proposed a widespread structure for learning the couple low rank and sparse main characteristic representation. Combined optimization of the nuclear and L1 norms extracts the total low rank formation and the local patterns lodged in the image. In that case the reconstructed image will be more informative and matrix decomposition problem can recover a noisy observation matrix into an approximation of low rank matrix and a second matrix which contains some low dimensional structure. We assumed that by removing the blur and noise from these images, they would be reconstructed in the highest quality. The proposed method was compared with a variety dictionary learning approaches which addressed super resolution problem, such as tensor sparsity, Generative Bayesian and {TV} based methods. We demonstrated the results of applied method on {MRI} and satellite images, showing both visual and psnr improvements. Dealing with complex data in best manner shows the robustness of the proposed method.},
	pages = {661--667},
	journaltitle = {{ISPRS} Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {{ISPRS} Ann. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Sahebkheir, S. and Esmaeily, A. and Saba, M.},
	urldate = {2023-08-31},
	date = {2023-01-14},
	langid = {english},
}

@article{nasir_acoustic_2022,
	title = {Acoustic emission monitoring of wood materials and timber structures: A critical review},
	volume = {350},
	issn = {09500618},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950061822025338},
	doi = {10.1016/j.conbuildmat.2022.128877},
	shorttitle = {Acoustic emission monitoring of wood materials and timber structures},
	abstract = {The growing interest in timber construction and using more wood for civil engineering applications has given highlighted importance of developing non-destructive evaluation ({NDE}) methods for structural health moni­ toring and quality control of wooden construction. This study, critically reviews the acoustic emission ({AE}) method and its applications in the wood and timber industry. Various other {NDE} methods for wood monitoring such as infrared spectroscopy, stress wave, guided wave propagation, X-ray computed tomography and ther­ mography are also included. The concept and experimentation of {AE} are explained, and the impact of wood properties on {AE} signal velocity and energy attenuation is discussed. The state-of-the-art {AE} monitoring of wood and timber structures is organized into six applications: (1) wood machining monitoring; (2) wood drying; (3) wood fracture; (4) timber structural health monitoring; (5) termite infestation monitoring; and (6) quality control. For each application, the opportunities that the {AE} method offers for in-situ monitoring or smart assessment of wood-based materials are discussed, and the challenges and direction for future research are critically outlined. Overall, compared with structural health monitoring of other materials, less attention has been paid to data-driven methods and machine learning applied to {AE} monitoring of wood and timber. In addition, most studies have focused on extracting simple time-domain features, whereas there is a gap in using sophisticated signal processing and feature engineering techniques. Future research should explore the sensor fusion for monitoring full-scale timber buildings and structures and focus on applying {AE} to large-size structures containing defects. Moreover, the effectiveness of {AE} methods used for wood composites and mass timber structures should be further studied.},
	pages = {128877},
	journaltitle = {Construction and Building Materials},
	shortjournal = {Construction and Building Materials},
	author = {Nasir, Vahid and Ayanleye, Samuel and Kazemirad, Siavash and Sassani, Farrokh and Adamopoulos, Stergios},
	urldate = {2023-08-31},
	date = {2022-10},
	langid = {english},
}

@article{musicco_automatic_2021,
	title = {{AUTOMATIC} {POINT} {CLOUD} {SEGMENTATION} {FOR} {THE} {DETECTION} {OF} {ALTERATIONS} {ON} {HISTORICAL} {BUILDINGS} {THROUGH} {AN} {UNSUPERVISED} {AND} {CLUSTERING}-{BASED} {MACHINE} {LEARNING} {APPROACH}},
	volume = {V-2-2021},
	issn = {2194-9050},
	url = {https://isprs-annals.copernicus.org/articles/V-2-2021/129/2021/},
	doi = {10.5194/isprs-annals-V-2-2021-129-2021},
	abstract = {The article describes an innovative procedure for the three-dimensional analysis of decay morphologies of ancient buildings, through the application of machine learning methods for the automatic segmentation of point clouds. In the field of Cultural Heritage conservation, photogrammetric data can be exploited, for diagnostic and monitoring support, to recognize different typologies of alterations visible on the masonry surface, starting from colour information. Actually, certain stone and plaster surface pathologies (biological patina, biological colonization, chromatic alterations, spots,...) are typically characterized by chromatic variations. To this purpose, colour-based segmentation with hierarchical clustering has been implemented on colour data of point clouds, considered in the {HSV} colour-space. In addition, geometry-based segmentation of 3D reconstructions has been performed, in order to identify the main architectural elements (walls, vaults), and to associate them to the detected defects. The proposed workflow has been applied to some ancient buildings’ environments, chosen because of their irregularity both in geometrical and colorimetric characteristics.},
	pages = {129--136},
	journaltitle = {{ISPRS} Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {{ISPRS} Ann. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Musicco, A. and Galantucci, R. A. and Bruno, S. and Verdoscia, C. and Fatiguso, F.},
	urldate = {2023-08-31},
	date = {2021-06-17},
	langid = {english},
}

@article{muradov_non-destructive_nodate,
	title = {Non-destructive system for in-wall moisture assessment of cultural heritage buildings},
	abstract = {Cultural heritage sites are exposed to several factors that cause their deterioration and degradation, namely moisture content. There are several destructive and non-destructive methods available to monitor moisture. However, the destructive methods are avoided in cultural heritage to prevent the damage of surfaces while the non-destructive methods are limited to the penetration capabilities. In this study, geo-positioning of non-destructive moisture assessment was proposed based on microwave spectroscopy and close-range photogrammetry. The experimental data was collected at the Museum of King Jan {III}'s Palace at Wilanów, Poland. The data was analysed using clustering algorithms (t-{SNE}, {PCA}, K-Means and Hierarchical), which demonstrated clear clusters within the data. Although, did not fully align with the pin-type moisture data as the proposed microwave system was able to penetrate through the material, whereas the pin-type meter only measured the surface. Therefore, the microwave sensing approach provided more detailed moisture information of the measured material.},
	author = {Muradov, Magomed and Kot, Patryk and Markiewicz, Jakub and Łapiński, Sławomir and Onisk, Katarzyna and Shaw, Andy and Hashim, Khalid and Zawieska, Dorota},
	langid = {english},
}

@article{markiewicz_quality_2020,
	title = {The Quality Assessment of Different Geolocalisation Methods for a Sensor System to Monitor Structural Health of Monumental Objects},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/10/2915},
	doi = {10.3390/s20102915},
	abstract = {Cultural heritage objects are aﬀected by a wide range of factors causing their deterioration and decay over time such as ground deformations, changes in hydrographic conditions, vibrations or excess of moisture, which can cause scratches and cracks formation in the case of historic buildings. The electromagnetic spectroscopy has been widely used for non-destructive structural health monitoring of concrete structures. However, the limitation of this technology is a lack of geolocalisation in the space for multispectral architectural documentation. The aim of this study is to examine diﬀerent geolocalisation methods in order to determine the position of the sensor system, which will then allow to georeference the results of measurements performed by this device and apply corrections to the sensor response, which is a crucial element required for further data processing related to the object structure and its features. The classical surveying, terrestrial laser scanning ({TLS}), and Structure-from-Motion ({SfM}) photogrammetry methods were used in this investigation at three test sites. The methods were reviewed and investigated. The results indicated that {TLS} technique should be applied for simple structures and plain textures, while the {SfM} technique should be used for marble-based and other translucent or semi-translucent structures in order to achieve the highest accuracy for geolocalisation of the proposed sensor system.},
	pages = {2915},
	number = {10},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Markiewicz, Jakub and Łapiński, Sławomir and Kot, Patryk and Tobiasz, Aleksandra and Muradov, Magomed and Nikel, Joanna and Shaw, Andy and Al-Shamma’a, Ahmed},
	urldate = {2023-08-31},
	date = {2020-05-21},
	langid = {english},
}

@article{liu_automatic_2020,
	title = {{AUTOMATIC} {DETECTION} {OF} {TIMBER}-{CRACKS} {IN} {WOODEN} {ARCHITECTURAL} {HERITAGE} {USING} {YOLOv}3 {ALGORITHM}},
	volume = {{XLIII}-B2-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B2-2020/1471/2020/},
	doi = {10.5194/isprs-archives-XLIII-B2-2020-1471-2020},
	abstract = {As there usually exist widespread crack, decay, deformation and other damages in the wooden architectural heritage ({WAH}). It is of great significance to detect the damages automatically and rapidly in order to grasp the status for daily repairs. Traditional methods use artificial feature-driven point clouds and image processing technology for object detection. With the development of big data and {GPU} computing performance, data-driven deep learning technology has been widely used for monitoring {WAH}. Deep learning technology is more accurate, faster, and more robust than traditional methods.In this paper, we conducted a case study to detect timber-crack damages in {WAH}, and selected the {YOLOv}3 algorithm with {DarkNet}-53 as the backbone network in the deep learning technology according to the characteristics of the crack. A large timber-crack dataset was first constructed, based on which the timber-crack detection model was trained and tested. The results were analyzed both qualitatively and quantitatively, showing that our proposed method was able to reach an accuracy of more than 90\% through processing each image for less than 0.1s. The promising results illustrate the validity of our self-constructed dataset as well as the reliability of {YOLOv}3 algorithm for the crack detection of wooden heritage.},
	pages = {1471--1476},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Liu, Y. and Hou, M. and Li, A. and Dong, Y. and Xie, L. and Ji, Y.},
	urldate = {2023-08-31},
	date = {2020-08-14},
	langid = {english},
}

@article{karami_fft-based_2023,
	title = {{FFT}-{BASED} {FILTERING} {APPROACH} {TO} {FUSE} {PHOTOGRAMMETRY} {AND} {PHOTOMETRIC} {STEREO} 3D {DATA}},
	volume = {X-4/W1-2022},
	issn = {2194-9050},
	url = {https://isprs-annals.copernicus.org/articles/X-4-W1-2022/363/2023/},
	doi = {10.5194/isprs-annals-X-4-W1-2022-363-2023},
	abstract = {Image-based 3D reconstruction has been successfully employed for micro-measurements and industrial quality control purposes. However, obtaining a highly-detailed and reliable 3D reconstruction and inspection of non-collaborative surfaces is still an open issue. Photometric stereo ({PS}) offers the high spatial frequencies of the surface, but the low freq uency is erroneous due to the mathematical model's assumptions and simplifications on how light interacts with the object surface. Photogrammetry, on the o ther hand, gives precise low-frequency information but fails to utilize high frequencies. As a result, in this research, we present a fusion strategy in Fourier domain to replace the low spatial frequencies of {PS} with the corresponding photogrammetric frequencies in order to have correct low frequencies while maintaining high frequencies from {PS}. The proposed method was tested on three different objects. Different cloud-to-cloud comparisons were provided between reference data and the 3D points derived from the proposed method to evaluate high and low frequency information. The obtained 3D findings demonst rated how the proposed methodology generates a high-detail 3D reconstruction of the surface topography (below 20 µm) while maintaining low -frequency information (0.09 µm on average for three different testingobjects) by fusing photogrammetric and {PS} depth data with the proposed {FFT}-based method.},
	pages = {363--370},
	journaltitle = {{ISPRS} Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {{ISPRS} Ann. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Karami, A. and Varshosaz, M. and Menna, F. and Remondino, F. and Luhmann, T.},
	urldate = {2023-08-31},
	date = {2023-01-13},
	langid = {english},
}

@article{hwang_computer_2021,
	title = {Computer vision-based wood identification and its expansion and contribution potentials in wood science: A review},
	volume = {17},
	issn = {1746-4811},
	url = {https://plantmethods.biomedcentral.com/articles/10.1186/s13007-021-00746-1},
	doi = {10.1186/s13007-021-00746-1},
	shorttitle = {Computer vision-based wood identification and its expansion and contribution potentials in wood science},
	abstract = {The remarkable developments in computer vision and machine learning have changed the methodologies of many scientific disciplines. They have also created a new research field in wood science called computer vision-based wood identification, which is making steady progress towards the goal of building automated wood identification systems to meet the needs of the wood industry and market. Nevertheless, computer vision-based wood identification is still only a small area in wood science and is still unfamiliar to many wood anatomists. To familiarize wood scientists with the artificial intelligence-assisted wood anatomy and engineering methods, we have reviewed the published main‑stream studies that used or developed machine learning procedures. This review could help researchers understand computer vision and machine learning techniques for wood identification and choose appropriate techniques or strategies for their study objectives in wood science.},
	pages = {47},
	number = {1},
	journaltitle = {Plant Methods},
	shortjournal = {Plant Methods},
	author = {Hwang, Sung-Wook and Sugiyama, Junji},
	urldate = {2023-08-31},
	date = {2021-04-28},
	langid = {english},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	number = {{arXiv}:1512.03385},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2023-08-31},
	date = {2015-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.03385 [cs]},
}

@article{gkantou_novel_2019,
	title = {Novel Electromagnetic Sensors Embedded in Reinforced Concrete Beams for Crack Detection},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/23/5175},
	doi = {10.3390/s19235175},
	abstract = {This paper investigates the possibility of applying novel microwave sensors for crack detection in reinforced concrete structures. Initially, a microstrip patch antenna with a split ring resonator ({SRR}) structure was designed, simulated and fabricated. To evaluate the sensor’s performance, a series of structural tests were carried out and the sensor responses were monitored. Four reinforced concrete ({RC}) beam specimens, designed according to the European Standards, were tested under three-point bending. The load was applied incrementally to the beams and the static responses were monitored via the use of a load cell, displacement transducers and crack width gauges (Demec studs). In parallel, signal readings from the microwave sensors, which were employed prior to the casting of the concrete and located along the neutral axis at the mid-span of the beam, were recorded at various load increments. The microwave measurements were analysed and compared with those from crack width gauges. A strong linear relationship between the crack propagation and the electromagnetic signal across the full captured spectrum was found, demonstrating the technique’s capability and its potential for further research, oﬀering a reliable, low-cost option for structural health monitoring ({SHM}).},
	pages = {5175},
	number = {23},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Gkantou, Michaela and Muradov, Magomed and Kamaris, George S. and Hashim, Khalid and Atherton, William and Kot, Patryk},
	urldate = {2023-08-31},
	date = {2019-11-26},
	langid = {english},
}

@article{ding_detecting_2020,
	title = {Detecting Defects on Solid Wood Panels Based on an Improved {SSD} Algorithm},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/18/5315},
	doi = {10.3390/s20185315},
	abstract = {Wood is widely used in construction, the home, and art applications all over the world because of its good mechanical properties and aesthetic value. However, because the growth and preservation of wood are greatly aﬀected by the environment, it often contains diﬀerent types of defects that aﬀect its performance and ornamental value. To solve the issues of high labor costs and low eﬃciency in the detection of wood defects, we used machine vision and deep learning methods in this work. A color charge-coupled device camera was used to collect the surface images of two types of wood from Akagi and Pinus sylvestris trees. A total of 500 images with a size of 200 × 200 pixels containing wood knots, dead knots, and checking defects were obtained. The transfer learning method was used to apply the single-shot multibox detector ({SSD}), a target detection algorithm and the {DenseNet} network was introduced to improve the algorithm. The mean average precision for detecting the three types of defects, live knots, dead knots and checking was 96.1\%.},
	pages = {5315},
	number = {18},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Ding, Fenglong and Zhuang, Zilong and Liu, Ying and Jiang, Dong and Yan, Xiaoan and Wang, Zhengguang},
	urldate = {2023-08-31},
	date = {2020-09-17},
	langid = {english},
}

@article{bassier_automated_2017,
	title = {{AUTOMATED} {CLASSIFICATION} {OF} {HERITAGE} {BUILDINGS} {FOR} {AS}-{BUILT} {BIMUSING} {MACHINE} {LEARNING} {TECHNIQUES}},
	volume = {{IV}-2/W2},
	issn = {2194-9050},
	url = {https://isprs-annals.copernicus.org/articles/IV-2-W2/25/2017/},
	doi = {10.5194/isprs-annals-IV-2-W2-25-2017},
	abstract = {Semantically rich three dimensional models such as Building Information Models ({BIMs}) are increasingly used in digital heritage. They provide the required information to varying stakeholders during the different stages of the historic buildings life cyle which is crucial in the conservation process. The creation of as-built {BIM} models is based on point cloud data. However, manually interpreting this data is labour intensive and often leads to misinterpretations. By automatically classifying the point cloud, the information can be proccesed more effeciently. A key aspect in this automated scan-to-{BIM} process is the classiﬁcation of building objects.},
	pages = {25--30},
	journaltitle = {{ISPRS} Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {{ISPRS} Ann. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Bassier, M. and Vergauwen, M. and Van Genechten, B.},
	urldate = {2023-08-31},
	date = {2017-08-16},
	langid = {english},
}

@article{balletti_survey_nodate,
	title = {{THE} {SURVEY} {OF} {THE} {WOODEN} {STRUCTURE} {OF} {THE} {ROOF} {OF} {PALAZZO} {DUCALE} {IN} {VENICE}},
	abstract = {The wooden structure that supports the roof of Palazzo Ducale is very complex becouse of continuous works of maintainance and restoration occurred during centuries. The photogrammetric laboratory of {CIRCE} – {IUAV} is employed in the survey and representation of the overstanding structure of Major Council Hall that, by one side, supports the covering and , by the other, upholds the box ceiling of the hall. The truss structure is trampleble and visitable only throught a thin catwalk standing on the middle of the orizontal tie beam. A traditional topographical survey appears hard just becouse of the unavoidable movement of the structure that is very elastic. Photogrammtery is unproposeble just for the strict space and for the its complexity. It was decided to realise some laser scans setting up the instruments having care to not urge the wooden flooring to avoid any movements. Nine scans have been done surveying 10 milions points. Clouds were alligned using targerts to obtain one unique cloud. From the big cloud some “sub-clouds” of points have been extracted contained each one a truss, doing as matter of fact some thick trasversal sections. The sub-clouds have been employed in the 3D modelling of each wooden element applying the solid modelling tecniques based on primitives to better interpolate the laser points.},
	author = {Balletti, Caterina and Guerra, Francesco and Gerbaudi, Francesco},
	langid = {english},
}

@misc{li_fssd_2018,
	title = {{FSSD}: Feature Fusion Single Shot Multibox Detector},
	url = {http://arxiv.org/abs/1712.00960},
	doi = {10.48550/arXiv.1712.00960},
	shorttitle = {{FSSD}},
	abstract = {{SSD} (Single Shot Multibox Detector) is one of the best object detection algorithms with both high accuracy and fast speed. However, {SSD}'s feature pyramid detection method makes it hard to fuse the features from different scales. In this paper, we proposed {FSSD} (Feature Fusion Single Shot Multibox Detector), an enhanced {SSD} with a novel and lightweight feature fusion module which can improve the performance significantly over {SSD} with just a little speed drop. In the feature fusion module, features from different layers with different scales are concatenated together, followed by some down-sampling blocks to generate new feature pyramid, which will be fed to multibox detectors to predict the final detection results. On the Pascal {VOC} 2007 test, our network can achieve 82.7 {mAP} (mean average precision) at the speed of 65.8 {FPS} (frame per second) with the input size 300\${\textbackslash}times\$300 using a single Nvidia 1080Ti {GPU}. In addition, our result on {COCO} is also better than the conventional {SSD} with a large margin. Our {FSSD} outperforms a lot of state-of-the-art object detection algorithms in both aspects of accuracy and speed. Code is available at https://github.com/lzx1413/{CAFFE}\_SSD/tree/fssd.},
	number = {{arXiv}:1712.00960},
	publisher = {{arXiv}},
	author = {Li, Zuoxin and Zhou, Fuqiang},
	urldate = {2023-08-31},
	date = {2018-05-16},
	eprinttype = {arxiv},
	eprint = {1712.00960 [cs]},
}

@article{bayrak_deep_2023,
	title = {{DEEP} {LEARNING} {BASED} {AERIAL} {IMAGERY} {CLASSIFICATION} {FOR} {TREE} {SPECIES} {IDENTIFICATION}},
	volume = {{XLVIII}-M-1-2023},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-M-1-2023/471/2023/isprs-archives-XLVIII-M-1-2023-471-2023.html},
	doi = {10.5194/isprs-archives-XLVIII-M-1-2023-471-2023},
	abstract = {Forest monitoring and tree species categorization has a vital importance in terms of biodiversity conservation, ecosystem health assessment, climate change mitigation, and sustainable resource management. Due to large-scale coverage of forest areas, remote sensing technology plays a crucial role in the monitoring of forest areas by timely and regular data acquisition, multi-spectral and multi-temporal analysis, non-invasive data collection, accessibility and cost-effectiveness. High-resolution satellite and airborne remote sensing technologies have supplied image data with rich spatial, color, and texture information. Nowadays, deep learning models are commonly utilized in image classification, object recognition, and semantic segmentation applications in remote sensing and forest monitoring as well. We, in this study, selected a popular {CNN} and object detection algorithm {YOLOv}8 variants for tree species classification from aerial images of {TreeSatAI} benchmark. Our results showed that {YOLOv}8-l outperformed benchmark’s initial release results, and other {YOLOv}8 variants with 71,55\% and 72,70\% for weighted and micro averaging scores, respectively.},
	pages = {471--476},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Bayrak, O. C. and Erdem, F. and Uzar, M.},
	urldate = {2023-08-31},
	date = {2023-08-15},
	note = {Conference Name: 39th International Symposium on Remote Sensing of Environment ({ISRSE}-39) “From Human needs to {SDGs}” - 24\&ndash;28 April 2023, Antalya, Türkiye
Publisher: Copernicus {GmbH}},
}

@article{xu_wood_2023,
	title = {Wood surface defects detection based on the improved {YOLOv}5-C3Ghost with {SimAm} module},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10214278/},
	doi = {10.1109/ACCESS.2023.3303890},
	abstract = {The detection of defects in wood is valuable for promoting the efficient exploitation of wood. So it is significant to further increase the accuracy of the detection of wood defects and enhance the real-time detection. In this paper, the {YOLOv}5 convolutional neural network is applied to wood defects detection, and the model is modified for both the {YOLOv}5n and {YOLOv}5m scales. The {SimAM} attention model was first incorporated into the network, and the learning rate decay strategy was replaced with {CosLR}, with Ghost convolution employed to minimize the model parameters. Finally, the modified network was tested for five types of wood defects, including live-knot, resin, dead-knot, knot-with-crack, and crack. It is demonstrated that the improvements resulted in a 1.5\% increase in {mAP}0.5:0.95 for {YOLOv}5n-C3Ghost and a 1.6\% increase in {mAP}0.5:0.95 for {YOLOv}5m-C3Ghost. In addition, there is a 51\% and 63\% difference in the number of model parameters, and a decrease in inference time and floating point operations respectively. The experiments indicate that our improved method not only enhances the accuracy of {YOLOv}5 in detecting wood defects, but also enables a reduction in the volume and computational cost of the model parameters.},
	pages = {1--1},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Xu, Jipan and Yang, Hong and Wan, Zihao and Mu, Hongbo and Qi, Dawei and Han, Shuxia},
	urldate = {2023-08-31},
	date = {2023},
	langid = {english},
}

@report{shao_wood_2023,
	title = {Wood Defect Detection and Grade Classification for Automatic Optimizing Cross-Cut Saw Based on Improved Yolov7 Model},
	url = {https://www.ssrn.com/abstract=4382834},
	abstract = {Defect detection and grade classification of a timber play an important role in the production of panel and profile. An algorithm based on improved {YOLOv}7 framework is developed in this paper to finish the automation of timber production. Defect detection is first performed to remove all defects based on information image, which is fused by texture image and depth image. According to an improved framework, a timber without any defects can be acquired. Then another improved {YOLOv}7 framework is used to finish the task of timber grade classification. Finally, the cutting list is calculated out based on our detection results and related optimization algorithm. Then Cross-cut task can be finished automatically. Based on our experiment, average accuracy of mean average precision for defect detection is 0.94. Compared with five classic detection algorithms, the proposed model is precise and with a high detection speed, including for defect detection and grade classification.},
	institution = {{SSRN}},
	type = {preprint},
	author = {Shao, Mingwei and Ma, Hailong},
	urldate = {2023-08-31},
	date = {2023},
	langid = {english},
	doi = {10.2139/ssrn.4382834},
}

@article{han_improved_2023,
	title = {An Improved {YOLOv}5 Algorithm for Wood Defect Detection Based on Attention},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10177807/},
	doi = {10.1109/ACCESS.2023.3293864},
	abstract = {Wood defect detection is a research hotspot in the field of forestry at present. However, existing studies on wood defect detection mainly focus on detecting a single type of defect or common defects, such as knots, insect pests, and cracks, which cannot meet the processing needs of high-quality wood. Moreover, there are problems, such as low recognition rates of small-target defects and poor recognition integrity of dense defects. To address these issues, we construct a large-scale dataset containing multiple types of wood surface defects through data augmentation techniques. We also introduce the Coordinate Attention module, Transformer Encoder module, and Swin Transformer module in the {YOLOv}5 network structure. The backbone network {CSP}-Darknet53 is optimized, and {BiFPN} is introduced in the neck part to achieve multi-scale weighted bidirectional feature fusion. In addition, we implement three new heads: Shead, Mhead, and Lhead in the prediction part. Comparison experiments show that {STC}-{YOLOv}5 outperforms some object detection algorithms. Ablation experiments show that each module effectively improves the detection performance. Compared to {YOLOv}5, {STC}-{YOLOv}5 proposed in this paper improve the {mAP} by 3.1\%. All types and scales of wood surface defects are detected better, with great potential for application in the forestry industry.},
	pages = {71800--71810},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Han, Siyu and Jiang, Xiangtao and Wu, Zhenyu},
	urldate = {2023-08-31},
	date = {2023},
	langid = {english},
}

@article{zhao_real-time_2021,
	title = {Real-time detection of particleboard surface defects based on improved {YOLOV}5 target detection},
	volume = {11},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-01084-x},
	doi = {10.1038/s41598-021-01084-x},
	abstract = {Abstract
            Particleboard surface defect detection technology is of great significance to the automation of particleboard detection, but the current detection technology has disadvantages such as low accuracy and poor real-time performance. Therefore, this paper proposes an improved lightweight detection method of You Only Live Once v5 ({YOLOv}5), namely {PB}-{YOLOv}5 (Particle Board-{YOLOv}5). Firstly, the gamma-ray transform method and the image difference method are combined to deal with the uneven illumination of the acquired images, so that the uneven illumination is well corrected. Secondly, Ghost Bottleneck lightweight deep convolution module is added to Backbone module and Neck module of {YOLOv}5 detection algorithm to reduce model volume. Thirdly, the {SELayer} module of attention mechanism is added into Backbone module. Finally, replace Conv in Neck module with depthwise convolution ({DWConv}) to compress network parameters. The experimental results show that the {PB}-{YOLOv}5 model proposed in this paper can accurately identify five types of defects on the particleboard surface: Bigshavings, {SandLeakage}, {GlueSpot}, Soft and {OliPollution}, and meet the real-time requirements. Specifically, recall, F1 score, {mAP}@.5, {mAP}@.5:.95 values of {pB}-Yolov5s model were 91.22\%, 94.5\%, 92.1\%, 92.8\% and 67.8\%, respectively. The results of Soft defects were 92.8\%, 97.9\%, 95.3\%, 99.0\% and 81.7\%, respectively. The detection of single image time of the model is only 0.031 s, and the weight size of the model is only 5.4 {MB}. Compared with the original {YOLOv}5s, {YOLOv}4, {YOLOv}3 and Faster {RCNN}, the {PB}-Yolov5s model has the fastest Detection of single image time. The Detection of single image time was accelerated by 34.0\%, 55.1\%, 64.4\% and 87.9\%, and the weight size of the model is compressed by 62.5\%, 97.7\%, 97.8\% and 98.9\%, respectively. The {mAP} value increased by 2.3\%, 4.69\%, 7.98\% and 13.05\%, respectively. The results show that the {PB}-{YOLOV}5 model proposed in this paper can realize the rapid and accurate detection of particleboard surface defects, and fully meet the requirements of lightweight embedded model.},
	pages = {21777},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Zhao, Ziyu and Yang, Xiaoxia and Zhou, Yucheng and Sun, Qinqian and Ge, Zhedong and Liu, Dongfang},
	urldate = {2023-08-31},
	date = {2021-11-05},
	langid = {english},
}

@article{cui_real-time_2023,
	title = {Real-time detection of wood defects based on {SPP}-improved {YOLO} algorithm},
	volume = {82},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-023-14588-7},
	doi = {10.1007/s11042-023-14588-7},
	abstract = {Wood processing is one of the most widely used in agriculture and industry. Low precision and high time delay of machine learning in wood defect detection are currently the main factors restricting the production efficiency and product quality of the wood processing industry. An {SPP}-improved deep learning method was proposed to detect wood defects based on the basic framework of the {YOLO} V3 network to improve accuracy and real-time performance. The extended dataset was firstly established by image data enhancement and preprocessing based on the limited samples of the wood defect dataset. Anchor box scale re-clustering of the wood defect dataset was carried out according to the defect features. The spatial pyramid pooling ({SPP}) network was applied to improve the feature pyramid ({FP}) network in {YOLO} V3. The validity and real-time performance of the proposed algorithm were verified by a randomly selected test set. The results show that the overall detection accuracy rate on the wood defect test dataset reaches 93.23\% while the detection time for each image is within 13 ms.},
	pages = {21031--21044},
	number = {14},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Cui, Yuming and Lu, Shuochen and Liu, Songyong},
	urldate = {2023-08-31},
	date = {2023-06},
	langid = {english},
}

@misc{cao_lightweight_2023,
	title = {Lightweight wood panel defect detection method incorporating attention mechanism and feature fusion network},
	url = {http://arxiv.org/abs/2306.12113},
	abstract = {In recent years, deep learning has made significant progress in wood panel defect detection. However, there are still challenges such as low detection , slow detection speed, and difficulties in deploying embedded devices on wood panel surfaces. To overcome these issues, we propose a lightweight wood panel defect detection method called {YOLOv}5-{LW}, which incorporates attention mechanisms and a feature fusion network. Firstly, to enhance the detection capability of acceptable defects, we introduce the Multi-scale Bi-directional Feature Pyramid Network ({MBiFPN}) as a feature fusion network. The {MBiFPN} reduces feature loss, enriches local and detailed features, and improves the model’s detection capability for acceptable defects. Secondly, to achieve a lightweight design, we reconstruct the {ShuffleNetv}2 network model as the backbone network. This reconstruction reduces the number of parameters and computational requirements while maintaining performance. We also introduce the Stem Block and Spatial Pyramid Pooling Fast ({SPPF}) models to compensate for any accuracy loss resulting from the lightweight design, ensuring the model’s detection capabilities remain intact while being computationally efficient. Thirdly, we enhance the backbone network by incorporating Efficient Channel Attention ({ECA}), which improves the network’s focus on key information relevant to defect detection. By attending to essential features, the model becomes more proficient in accurately identifying and localizing defects. We validate the proposed method using a selfdeveloped wood panel defect dataset. The experimental results demonstrate the effectiveness of the improved {YOLOv}5-{LW} method. Compared to the original model, our approach achieves a 92.8\% accuracy rate, reduces the number of parameters by 27.78\%, compresses computational volume by 41.25\%, improves detection inference speed by 10.16\%, and enhances the detection accuracy of two types of acceptable defects (dead knots and cracks) by 0.2\% and 1.3\% respectively.},
	number = {{arXiv}:2306.12113},
	publisher = {{arXiv}},
	author = {Cao, Yongxin and Liu, Fanghua and Jiang, Lai and Bao, Cheng and Miao, You and Chen, Yang},
	urldate = {2023-08-31},
	date = {2023-06-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.12113 [cs]},
}

@article{ma_complex_2022,
	title = {Complex Texture Contour Feature Extraction of Cracks in Timber Structures of Ancient Architecture Based on {YOLO} Algorithm},
	volume = {2022},
	issn = {1687-8094, 1687-8086},
	url = {https://www.hindawi.com/journals/ace/2022/7879302/},
	doi = {10.1155/2022/7879302},
	abstract = {Deep learning has achieved good results in the crack detection of roads and bridges. However, the timber structures of ancient architecture have strong orthotropic anisotropy and complex microscopic structures, and the law of cracks development is extremely complex. The image data has a large proportion of pixels, which is obviously different from the background gray value, and there is timber grain noise, thus the existing methods cannot accurately extract the complex texture contour feature of cracks. In previous studies, we have verified that {YOLO} v5s is effective in crack detection in timber structures of ancient architecture. However, there are many different versions of {YOLO} series models. In order to find a better algorithm, this paper mainly adopts three models including {YOLO} v3, {YOLO} v4s-mish, and {YOLO} v5s to detect cracks in the timber structures of ancient architecture, and compares and analyzes the advantages and disadvantages of the three models. In the comparing process, we mainly have discussed the index performance of the three models in terms of training time, loss function, recall rate, and {mAP} value. We have summarized and analyzed the advantages and disadvantages of the three models in cracks detection of the timber structures of ancient architecture, and concluded the comparing results of the three models in cracks detection based on experiments. We published the first picture data set of cracks in timber structures of ancient architecture, and applied {YOLO} model in the intelligent identification field of cracks in timber structures of ancient architecture for the first time, which opened up a new idea for the intelligent operation and maintenance of the timber structures of ancient architecture.},
	pages = {1--13},
	journaltitle = {Advances in Civil Engineering},
	shortjournal = {Advances in Civil Engineering},
	author = {Ma, Jian and Yan, Weidong and Liu, Guoqi and Xing, Shiyu and Niu, Siqi and Wei, Tong},
	editor = {Smarzewski, Piotr},
	urldate = {2023-08-31},
	date = {2022-08-23},
	langid = {english},
}

@article{hou_surface_2023,
	title = {Surface Defect Detection of Preform Based on Improved {YOLOv}5},
	volume = {13},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/13/7860},
	doi = {10.3390/app13137860},
	abstract = {This paper proposes a lightweight detection model based on machine vision, {YOLOv}5-{GC}, to improve the efﬁciency and accuracy of detecting and classifying surface defects in preforming materials. During this process, clear images of the entire surface are difﬁcult to obtain due to the stickiness, high reﬂectivity, and black resin of the thermosetting plain woven prepreg. To address this challenge, we built a machine vision platform equipped with a linescan camera and highintensity linear light source that captures surface images of the material during the preforming process. To solve the problem of defect detection in the case of extremely small and imbalanced samples, we adopt a transfer learning approach based on the {YOLOv}5 neural network for defect recognition and introduce a coordinate attention and Ghost Bottleneck module to improve recognition accuracy and speed. Experimental results demonstrate that the proposed approach achieves rapid and high-precision identiﬁcation of surface defects in preforming materials, outperforming other state-of-the-art methods. This work provides a promising solution for surface defect detection in preforming materials, contributing to the improvement of composite material quality.},
	pages = {7860},
	number = {13},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Hou, Jiatong and You, Bo and Xu, Jiazhong and Wang, Tao and Cao, Moran},
	urldate = {2023-08-31},
	date = {2023-07-04},
	langid = {english},
}

@article{wang_detection_2021,
	title = {Detection of wood surface defects based on improved {YOLOv}3 algorithm},
	volume = {16},
	issn = {19302126, 19302126},
	url = {https://bioresources.cnr.ncsu.edu/resources/detection-of-wood-surface-defects-based-on-improved-yolov3-algorithm/},
	doi = {10.15376/biores.16.4.6766-6780},
	abstract = {For the detection of wood surface defects, a convolutional neural network has a low detection efficiency and insufficient generalization ability, so it does not meet the requirements of online detection. Aiming to solve the above problems, the {YOLOv}3 baseline model, which has the advantage of multi-objective dynamic detection, was improved and applied to the online detection of wood surface defects. To solve the problem of the poor generalization ability of the network, {GridMask} was used to enhance the data and improve the robustness of the network. In order to solve the problem of the considerable amount of network parameter calculations and insufficient real-time performance, the residual block of the backbone network was changed to a Ghost block structure to achieve a lightweight model. Finally, the confidence loss function of the network was improved to reduce the influence of simple samples and negative samples on model convergence. The experimental results showed that, compared with the original network, the improved algorithm increased the mean average precision by 5.73\% and the detection speed was increased to 28 frames per second (an increase of 11), which met the requirements for real-time industrial detection.},
	pages = {6766--6780},
	number = {4},
	journaltitle = {{BioResources}},
	shortjournal = {{BioRes}},
	author = {Wang, Baogang and Yang, Chunmei and Ding, Yucheng and Qin, Guangyi},
	urldate = {2023-08-31},
	date = {2021-08-20},
	langid = {english},
}

@misc{gu_one-shot_2021,
	title = {A One-Shot Texture-Perceiving Generative Adversarial Network for Unsupervised Surface Inspection},
	url = {http://arxiv.org/abs/2106.06792},
	abstract = {Visual surface inspection is a challenging task owing to the highly diverse appearance of target surfaces and defective regions. Previous attempts heavily rely on vast quantities of training examples with manual annotation. However, in some practical cases, it is difﬁcult to obtain a large number of samples for inspection. To combat it, we propose a hierarchical texture-perceiving generative adversarial network ({HTPGAN}) that is learned from the one-shot normal image in an unsupervised scheme. Speciﬁcally, the {HTP}-{GAN} contains a pyramid of convolutional {GANs} that can capture the global structure and ﬁne-grained representation of an image simultaneously. This innovation helps distinguishing defective surface regions from normal ones. In addition, in the discriminator, a texture-perceiving module is devised to capture the spatially invariant representation of normal image via directional convolutions, making it more sensitive to defective areas. Experiments on a variety of datasets consistently demonstrate the effectiveness of our method.},
	number = {{arXiv}:2106.06792},
	publisher = {{arXiv}},
	author = {Gu, Lingyun and Zhang, Lin and Wang, Zhaokui},
	urldate = {2023-08-31},
	date = {2021-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.06792 [cs]},
}

@inreference{noauthor_imagej_2021,
	title = {{ImageJ}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://de.wikipedia.org/w/index.php?title=ImageJ&oldid=214543618},
	abstract = {{ImageJ} ist ein Bildbearbeitungs- und Bildverarbeitungsprogramm. Es wird vielfach für medizinische und wissenschaftliche Bildanalyse genutzt, zum Beispiel zum Vermessen von Strukturen auf Mikroskopaufnahmen. In der Druckvorstufe wird es für Farbraumanalysen verwendet.
Das Programm und der Quelltext sind gemeinfrei (public domain, Open Source) und dürfen daher frei kopiert und von jedermann verändert werden.Die Funktionalität des Programms kann durch Hunderte von Plug-ins erweitert werden.
Zusätzlich ist die {API} von {ImageJ} dabei so entworfen, dass {ImageJ} selbst ebenso als Bildverarbeitungsbibliothek in andere Programme eingebunden werden kann. {ImageJ} ist in Java geschrieben und damit plattformübergreifend verwendbar.},
	booktitle = {维基百科},
	urldate = {2023-05-24},
	date = {2021-08-06},
	langid = {pinyin},
	note = {Page Version {ID}: 214543618},
}

@article{_canny_2020,
	title = {基于Canny算子改进型的影像测量边缘检测},
	volume = {57},
	rights = {\&\#169; 2020 @中国光学期刊网},
	url = {https://www.opticsjournal.net/Articles/OJfe4c0144187d98a0/Abstract},
	doi = {10.3788/LOP57.241024},
	abstract = {针对采用高斯滤波器对图像进行滤波会导致图像边缘平滑,人为设定高、低阈值会导致阈值的自适应性差,采用双阈值法去除虚假边缘会导致去除效果不佳等问题,提出改进的Canny边缘检测算法并应用于影像测量领域。首先使用开关中值滤波代替高斯滤波,在去除噪声的同时保留非噪声像素点的灰度值不变,从而提高边缘定位精度;然后采用K-means聚类算法以得到高、低梯度值聚类中心,采用{OTSU算法以得到梯度阈值},将两个方法结合,可以实现高、低阈值的自适应;最后采用面积形态学的方法去除图像的干扰边缘。实验结果表明,改进的算法具有定位精度高、自适应性强以及干扰点去除效果好等优点。},
	pages = {241024},
	number = {24},
	journaltitle = {Laser \& Optoelectronics Progress},
	shortjournal = {激光与光电子学进展},
	author = {张加朋 and 于凤芹},
	urldate = {2023-05-22},
	date = {2020-12-01},
	langid = {pinyin},
	note = {Number: 24
Publisher: 中国激光杂志社},
}

@online{noauthor_20_nodate,
	title = {(20条消息) 数字图像处理(20): 边缘检测算子(Canny算子)\_数字图像处理边缘检测算子\_TechArtisan6的博客-{CSDN博客}},
	url = {https://blog.csdn.net/zaishuiyifangxym/article/details/90142702},
	urldate = {2023-05-22},
}

@online{noauthor_canny_nodate,
	title = {Canny算子进行图像边缘检测},
	url = {https://zhuanlan.zhihu.com/p/47017516},
	abstract = {“这是大三上学期计算机视觉课程的第二次作业，突然觉得，做完作业后记录下来也蛮好的” “我也是初学者，难免有错误，非常高兴可以有人指出错误”使用的语言：c++ 使用的库：{CImg}.h （课程要求） 初学者，并非完…},
	titleaddon = {知乎专栏},
	urldate = {2023-05-22},
	langid = {pinyin},
}

@online{noauthor_20_nodate-1,
	title = {(20条消息) {openCV}+python 采用滑动条实现动态全局阈值分割\_田土豆的博客-{CSDN博客}},
	url = {https://blog.csdn.net/weixin_42216109/article/details/89553383},
	urldate = {2023-05-22},
}

@online{noauthor_20_nodate-2,
	title = {(20条消息) python+{openCV} 自适应阈值分割\_python opencv 自适应阈值\_田土豆的博客-{CSDN博客}},
	url = {https://blog.csdn.net/weixin_42216109/article/details/89554532},
	urldate = {2023-05-22},
}

@online{noauthor_opencvpythoncanny--_nodate,
	title = {使用{OpenCV}+{Python进行Canny边缘检测}-腾讯云开发者社区-腾讯云},
	url = {https://cloud.tencent.com/developer/article/1942493},
	urldate = {2023-05-22},
}

@article{poux_automatic_2022,
	title = {Automatic region-growing system for the segmentation of large point clouds},
	volume = {138},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580522001236},
	doi = {10.1016/j.autcon.2022.104250},
	abstract = {This article describes a complete unsupervised system for the segmentation of massive 3D point clouds. Our system bridges the missing components that permit to go from 99\% automation to 100\% automation for the construction industry. It scales up to billions of 3D points and targets a generic low-level grouping of planar regions usable by a wide range of applications. Furthermore, we introduce a hierarchical multi-level segment definition to cope with potential variations in high-level object definitions. The approach first leverages planar predominance in scenes through a normal-based region growing. Then, for usability and simplicity, we designed an automatic heuristic to determine without user supervision three {RANSAC}-inspired parameters. These are the distance threshold for the region growing, the threshold for the minimum number of points needed to form a valid planar region, and the decision criterion for adding points to a region. Our experiments are conducted on 3D scans of complex buildings to test the robustness of the “one-click” method in varying scenarios. Labelled and instantiated point clouds from different sensors and platforms (depth sensor, terrestrial laser scanner, hand-held laser scanner, mobile mapping system), in different environments (indoor, outdoor, buildings) and with different objects of interests ({AEC}-related, {BIM}-related, navigation-related) are provided as a new extensive test-bench. The current implementation processes ten million points per minutes on a single thread {CPU} configuration. Moreover, the resulting segments are tested for the high-level task of semantic segmentation over 14 classes, to achieve an F1-score of 90+ averaged over all datasets while reducing the training phase to a fraction of state of the art point-based deep learning methods. We provide this baseline along with six new open-access datasets with 300+ million hand-labelled and instantiated 3D points at: https://www.graphics.rwth-aachen.de/project/ 45/.},
	pages = {104250},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Poux, F. and Mattes, C. and Selman, Z. and Kobbelt, L.},
	urldate = {2023-05-22},
	date = {2022-06},
	langid = {english},
}

@online{noauthor_20_nodate-3,
	title = {(20条消息) 区域生长算法原理及实现\_小武{\textasciitilde}{\textasciitilde}的博客-{CSDN博客}},
	url = {https://blog.csdn.net/weixin_40647819/article/details/90215872},
	urldate = {2023-05-20},
}

@online{noauthor_20_nodate-4,
	title = {(20条消息) C++实现区域生长算法（Region growing algorithm）\_c++ 区域分割算法\_A-Chin的博客-{CSDN博客}},
	url = {https://blog.csdn.net/L_J_Kin/article/details/102781148},
	urldate = {2023-05-20},
}

@online{alaa_week_nodate,
	title = {Week 6: Region Growing and Clustering Segmentation)},
	url = {https://sbme-tutorials.github.io/2019/cv/notes/6_week6.html},
	shorttitle = {Week 6},
	abstract = {These pages contain online teaching materials prepared by teaching assistants in the biomedical engineering department at Cairo University.},
	titleaddon = {Tutorials for {SBME} Students},
	author = {Alaa, Asem},
	urldate = {2023-05-16},
	langid = {english},
}

@article{boulch_deep_2016,
	title = {Deep Learning for Robust Normal Estimation in Unstructured Point Clouds},
	volume = {35},
	issn = {01677055},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.12983},
	doi = {10.1111/cgf.12983},
	abstract = {Normal estimation in point clouds is a crucial ﬁrst step for numerous algorithms, from surface reconstruction and scene understanding to rendering. A recurrent issue when estimating normals is to make appropriate decisions close to sharp features, not to smooth edges, or when the sampling density is not uniform, to prevent bias. Rather than resorting to manually-designed geometric priors, we propose to learn how to make these decisions, using ground-truth data made from synthetic scenes. For this, we project a discretized Hough space representing normal directions onto a structure amenable to deep learning. The resulting normal estimation method outperforms most of the time the state of the art regarding robustness to outliers, to noise and to point density variation, in the presence of sharp edges, while remaining fast, scaling up to millions of points.},
	pages = {281--290},
	number = {5},
	journaltitle = {Computer Graphics Forum},
	shortjournal = {Computer Graphics Forum},
	author = {Boulch, Alexandre and Marlet, Renaud},
	urldate = {2023-05-14},
	date = {2016-08},
	langid = {english},
}

@article{castillo_point_2023,
	title = {Point Cloud Segmentation via Constrained Nonlinear Least Squares Surface Normal Estimates},
	abstract = {We present a point cloud segmentation scheme based on estimated surface normals and local point connectivity, that operates on unstructured point cloud data. We can seg-ment a point cloud into disconnected components as well as piecewise smooth components as needed. Given that the performance of the segmentation routine depends on the quality of the surface normal approximation, we also propose an improved surface normal approximation method based on recasting the popular principal component analy-sis formulation as a constrained least squares problem. The new approach is robust to singularities in the data, such as corners and edges, and also incorporates data denoising in a manner similar to planar moving least squares.},
	author = {Castillo, Edward and Zhao, Hongkai},
	date = {2023-05-13},
}

@online{noauthor_figure_nodate,
	title = {Figure 1. {PCA} normal estimation for a cube. Notice the smeared normals...},
	url = {https://www.researchgate.net/figure/PCA-normal-estimation-for-a-cube-Notice-the-smeared-normals-along-the-edges-of-the-cube_fig1_260317575},
	abstract = {Download scientific diagram {\textbar} {PCA} normal estimation for a cube. Notice the smeared normals along the edges of the cube.  from publication: Point Cloud Segmentation via Constrained Nonlinear Least Squares Surface Normal Estimates {\textbar} We present a point cloud segmentation scheme based on estimated surface normals and local point connectivity, that operates on unstructured point cloud data. We can seg-ment a point cloud into disconnected components as well as piecewise smooth components as needed. Given... {\textbar} Point Clouds, Least-Squares Analysis and Segmentation {\textbar} {ResearchGate}, the professional network for scientists.},
	titleaddon = {{ResearchGate}},
	urldate = {2023-05-13},
	langid = {english},
}

@article{mitra_estimating_2004,
	title = {Estimating Surface Normals in Noisy Point Cloud Data},
	author = {Mitra, Niloy J},
	date = {2004},
	langid = {english},
}

@online{noauthor_19_nodate,
	title = {(19条消息) {CloudCompare}——计算点云的法向量\_cloudcompare法向量\_点云侠的博客-{CSDN博客}},
	url = {https://blog.csdn.net/qq_36686437/article/details/121717949?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-121717949-blog-117930176.235^v35^pc_relevant_increate_t0_download_v2&spm=1001.2101.3001.4242.1&utm_relevant_index=3},
	urldate = {2023-05-13},
}

@online{leonard_normals_nodate,
	title = {Normals: What are they and how they help us pick better},
	url = {https://blog.zivid.com/normals},
	shorttitle = {Normals},
	abstract = {Surface normals are the vectors we find on surfaces that help us understand the nature of that surface. How can they help in the context of 3D vision?},
	author = {Leonard, John},
	urldate = {2023-05-12},
	langid = {english},
}

@article{kazhdan_screened_2013,
	title = {Screened poisson surface reconstruction},
	volume = {32},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2487228.2487237},
	doi = {10.1145/2487228.2487237},
	abstract = {Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface reconstructions.},
	pages = {1--13},
	number = {3},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Kazhdan, Michael and Hoppe, Hugues},
	urldate = {2023-05-12},
	date = {2013-06},
	langid = {english},
}

@article{kazhdan_poisson_nodate,
	title = {Poisson Surface Reconstruction},
	abstract = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
	author = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
	langid = {english},
}

@article{tsuchida_comparison_2023,
	title = {Comparison of the accuracy of different handheld-type scanners in three-dimensional facial image recognition},
	volume = {67},
	issn = {1883-1958, 1883-9207},
	url = {https://www.jstage.jst.go.jp/article/jpr/67/2/67_JPR_D_22_00001/_article},
	doi = {10.2186/jpr.JPR_D_22_00001},
	abstract = {Purpose: Handheld-type scanners are widely used in clinical practice. This study examined the accuracy of handheldtype scanners using plaster statues to assess their performance in facial recognition.
Methods: Twelve 4-mm zirconia balls as measuring points were attached to the facial portions of three types of plaster statue. Six digital facial images of each plaster statue were obtained using one of the following five handheld-type scanners: Artec Eva, Artec Spider, Bellus 3D {FaceApp}, {SNAP}, and Vectra H1. Four-millimeter spherical objects were manually placed at the measurement points on the scanned data generated using computer-aided design software and coordinate positions were measured using a contact-type high-resolution three-dimensional measurement device. Consequently, the discrepancy between the distance measured using the contact-type device and that measured using the handheld-type scanner was calculated. The scanning time, processing time, and deviation of the distance between the measuring points were analyzed using two-way analysis of variance and t-test with Bonferroni correction.
Results: The scanning and processing times ranged from 15.2 to 42.2 s and 20.7 to 234.2 s, respectively. Overall, 97\% of all measured distances by Spider were within ±1.00\% deviation; 79\%, Vectra; 73\%, Eva; 70\%, Bellus; and 42\%, {SNAP}.
Conclusions: The performance of handheld-type scanners using plaster statues varied among the different scanners. The scanning time of Eva and the processing time of Bellus were significantly shorter than those of other scanners. Furthermore, Spider exhibited the best accuracy, followed by Eva, Vectra, Bellus, and {SNAP}.},
	pages = {222--230},
	number = {2},
	journaltitle = {Journal of Prosthodontic Research},
	shortjournal = {J Prosthodont Res},
	author = {Tsuchida, Yumi and Shiozawa, Maho and Handa, Kazuyuki and Takahashi, Hidekazu and Nikawa, Hiroki},
	urldate = {2023-05-11},
	date = {2023},
	langid = {english},
}

@incollection{doria_digital_2023,
	location = {Cham},
	title = {Digital Reconstruction for the Analysis of Conservation State: The Transmission of Historical Memory of St. George and the Dragon Tile in San Michele Basilica Facade},
	isbn = {978-3-031-15321-1},
	url = {https://doi.org/10.1007/978-3-031-15321-1_10},
	series = {Digital Innovations in Architecture, Engineering and Construction},
	shorttitle = {Digital Reconstruction for the Analysis of Conservation State},
	abstract = {The case study presented describes the experimentation of 3D digital reconstruction carried out on a portion of the facade of San Michele Basilica in Pavia. The facade is characterized by numerous sandstone bas-reliefs, that have been deteriorating throughout the years, composing narrative cycles of historical kings and myths of the Lombard Kingdom. Starting from acquisitions carried out on the field and through the study of the historical conservation of the decorative tiles, researchers worked to create a digital model representing the actual state of conservation of the St. George and the Dragon tile. Such model has been compared to the one based on historical images and surveys carried out during the past century. The goal is to obtain an information system for conservation management protocols and for dissemination of this disappearing heritage. The method applied to the pilot case consists of geometric and material survey, documentation of the state of conservation and analyses with non-invasive techniques, leading to the three-dimensional reconstruction of the tile. The result allows to perceive shapes of the tile that are now illegible and understand the volumes as they should have been. The different conservation status of the tile can be appreciated on the physical object or in virtual mode, through {VR} system and {FDM} 3D printing.},
	pages = {151--167},
	booktitle = {Digital Restoration and Virtual Reconstructions: Case Studies and Compared Experiences for Cultural Heritage},
	publisher = {Springer International Publishing},
	author = {Doria, Elisabetta and Fu, Hangjun and Picchio, Francesca},
	editor = {Trizio, Ilaria and Demetrescu, Emanuel and Ferdani, Daniele},
	urldate = {2023-05-11},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-15321-1_10},
}

@article{norin_application_2019,
	title = {Application of three-dimensional technologies in restoration of architectural decor elements},
	volume = {135},
	issn = {2267-1242},
	url = {https://www.e3s-conferences.org/10.1051/e3sconf/201913503007},
	doi = {10.1051/e3sconf/201913503007},
	abstract = {The article deals with issues of applying modern innovative three-dimensional techniques in production and restoration of facade decor elements. Methodology for producing an exact physical copy of a part under restoration is set forth as exemplified by the architectural rosette. It details a methodology of how to conduct virtual digital restoration of a rosette using one of the best software solutions for digital molding, Zbrush, which is a superb tool to create high-definition three-dimensional objects. Two methods of digital restoration are treated: copying and sculpting. The copying method produces the geometric shape of an article consisting of items with identical dimensions. Sculpting is performed using digital molding, i.e. working with digital clay – a more intuitive modeling method compared to traditional techniques. Possible methods of production of the restored physical copy of a damaged rosette, their merits, and demerits are described. The study provides a forecast for development of architectural monument restoration with the use of three-dimensional techniques.},
	pages = {03007},
	journaltitle = {E3S Web of Conferences},
	shortjournal = {E3S Web Conf.},
	author = {Norin, Veniamin and Golovina, Svetlana and Tikhonov, Yurii},
	editor = {Rudoy, D. and Murgul, V.},
	urldate = {2023-05-11},
	date = {2019},
	langid = {english},
}

@article{somogyi_analysis_2017,
	title = {Analysis of Gothic Architectural Details by Spatial Object Reconstruction Techniques},
	volume = {61},
	rights = {Copyright (c) 2017 Periodica Polytechnica Civil Engineering},
	issn = {1587-3773},
	url = {https://pp.bme.hu/ci/article/view/10418},
	doi = {10.3311/PPci.10418},
	abstract = {The paper focuses on the 3D data acquisition technologiesthat support capturing the geometry of medieval architecturalfragmented stones. High-resolution models of such fragmentsenable the analysis of profile shapes as well as the markinglines and curves left by the instruments of stonemasons, andtherefore, indirectly, identifying connections between severalmaster builders could become possible. Considering therequirements of historical analysis and the fact that the investigatedstones are under monument protection, the authorsdecided to use remote sensing technologies, such as structuredlight scanning, terrestrial laser scanning, depth camera andimage-based reconstruction.The paper evaluates the discussed technologies based on theaccuracy and geometric resolution of the obtained 3D models.Besides technical parameters, time and cost requirements alsohave been investigated. The paper gives an overview on theadvantages and shortcomings of the applied data acquisitiontechnologies and of the provided end-products.},
	pages = {640--651},
	number = {3},
	journaltitle = {Periodica Polytechnica Civil Engineering},
	author = {Somogyi, Árpád and Fehér, Krisztina and Lovas, Tamás and Halmos, Balázs and Barsi, Árpád},
	urldate = {2023-05-11},
	date = {2017-02-23},
	langid = {english},
	note = {Number: 3},
}

@article{inzerillo_high_2019,
	title = {{HIGH} {QUALITY} {TEXTURE} {MAPPING} {PROCESS} {AIMED} {AT} {THE} {OPTIMIZATION} {OF} 3D {STRUCTURED} {LIGHT} {MODELS}},
	volume = {{XLII}-2-W9},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLII-2-W9/389/2019/isprs-archives-XLII-2-W9-389-2019.html},
	doi = {10.5194/isprs-archives-XLII-2-W9-389-2019},
	abstract = {This article presents the evaluation of a pipeline to develop a high-quality texture mapping implementation which makes it possible to carry out a semantic high-quality 3D textured model. Due to geometric errors such as camera parameters or limited image resolution or varying environmental parameters, the calculation of a surface texture from 2D images could present several color errors. And, sometimes, it needs adjustments to the {RGB} or lightness information on a defined part of the texture. The texture mapping procedure is composed of mesh parameterization, mesh partitioning, mesh segmentation unwraps, {UV} map and projection of island, {UV} layout optimization, mesh packing and mesh baking. The study focuses attention to the mesh partitioning that essentially assigns a weight to each mesh, which reveals a mesh’s weight calculated by considering the flatness and distance of the mesh with respect to a chart. The 3D texture mapping has been developed in Blender and implemented in Python. In this paper we present a flowchart that resumes the procedure which aims to achieve a high-quality mesh and texture 3D model starting from the 3D Spider acquire, integrated with the {SfM} texture and using the texture mapping to reduce the color errors according to a semantic interpretation.},
	pages = {389--396},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Inzerillo, L. and Di Paola, F. and Alogna, Y.},
	urldate = {2023-05-11},
	date = {2019-01-31},
	note = {Conference Name: 8th International Workshop 3D-{ARCH} {\textless}q{\textgreater}3D Virtual Reconstruction and Visualization of Complex Architectures{\textless}/q{\textgreater} - 6\&ndash;8 February 2019, Bergamo, Italy
Publisher: Copernicus {GmbH}},
}

@article{chizhova_semantic_2018,
	title = {{SEMANTIC} {SEGMENTATION} {OF} {BUILDING} {ELEMENTS} {USING} {POINT} {CLOUD} {HASHING}},
	volume = {{XLII}-2},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLII-2/241/2018/isprs-archives-XLII-2-241-2018.html},
	doi = {10.5194/isprs-archives-XLII-2-241-2018},
	abstract = {For the interpretation of point clouds, the semantic definition of extracted segments from point clouds or images is a common problem. Usually, the semantic of geometrical pre-segmented point cloud elements are determined using probabilistic networks and scene databases. The proposed semantic segmentation method is based on the psychological human interpretation of geometric objects, especially on fundamental rules of primary comprehension. Starting from these rules the buildings could be quite well and simply classified by a human operator (e.g. architect) into different building types and structural elements (dome, nave, transept etc.), including particular building parts which are visually detected. The key part of the procedure is a novel method based on hashing where point cloud projections are transformed into binary pixel representations. A segmentation approach released on the example of classical Orthodox churches is suitable for other buildings and objects characterized through a particular typology in its construction (e.g. industrial objects in standardized enviroments with strict component design allowing clear semantic modelling).},
	pages = {241--250},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Chizhova, M. and Gurianov, A. and Hess, M. and Luhmann, T. and Brunn, A. and Stilla, U.},
	urldate = {2023-05-11},
	date = {2018-05-30},
	note = {Conference Name: {ISPRS} {TC} {II} Mid-term Symposium {\textless}q{\textgreater}Towards Photogrammetry 2020{\textless}/q{\textgreater} (Volume {XLII}-2) - 4\&ndash;7 June 2018, Riva del Garda, Italy
Publisher: Copernicus {GmbH}},
}

@article{gasparovic_increase_2012,
	title = {{INCREASE} {OF} {READABILITY} {AND} {ACCURACY} {OF} 3D {MODELS} {USING} {FUSION} {OF} {CLOSE} {RANGE} {PHOTOGRAMMETRY} {AND} {LASER} {SCANNING}},
	volume = {{XXXIX}-B5},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XXXIX-B5/93/2012/isprsarchives-XXXIX-B5-93-2012.html},
	doi = {10.5194/isprsarchives-XXXIX-B5-93-2012},
	abstract = {The development of laser scanning technology has opened a new page in geodesy and enabled an entirely new way of presenting data. Products obtained by the method of laser scanning are used in many sciences, as well as in archaeology. It should be noted that 3D models of archaeological artefacts obtained by laser scanning are fully measurable, written in 1:1 scale and have high accuracy. On the other hand, texture and {RGB} values of the surface of the object obtained by a laser scanner have lower resolution and poorer radiometric characteristics in relation to the textures captured with a digital camera. 

 Scientific research and the goal of this paper are to increase the accuracy and readability of the 3D model with textures obtained with a digital camera. Laser scanning was performed with triangulation scanner of high accuracy, Vivid 9i (Konica Minolta), while for photogrammetric recording digital camera Nikon D90 with a lens of fixed focal length 20 mm, was used. 

 It is important to stress that a posteriori accuracy score of the global registration of point clouds in the form of the standard deviation was ± 0.136 mm while the average distance was only ± 0.080 mm. Also research has proven that the quality projection texture model increases readability. 

 Recording of archaeological artefacts and making their photorealistic 3D model greatly contributes to archaeology as a science, accelerates processing and reconstruction of the findings. It also allows the presentation of findings to the general public, not just to the experts.},
	pages = {93--98},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Gašparović, M. and Malarić, I.},
	urldate = {2023-05-11},
	date = {2012-07-24},
	note = {Conference Name: {XXII} {ISPRS} Congress, Technical Commission V (Volume {XXXIX}-B5) - 25 August \&ndash; 01 September 2012, Melbourne, Australia
Publisher: Copernicus {GmbH}},
}

@article{baptista_documenting_2013,
	title = {{DOCUMENTING} A {COMPLEX} {MODERN} {HERITAGE} {BUILDING} {USING} {MULTI} {IMAGE} {CLOSE} {RANGE} {PHOTOGRAMMETRY} {AND} 3D {LASER} {SCANNED} {POINT} {CLOUDS}},
	volume = {{XL}-5-W2},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XL-5-W2/675/2013/isprsarchives-XL-5-W2-675-2013.html},
	doi = {10.5194/isprsarchives-XL-5-W2-675-2013},
	abstract = {Integrating different technologies and expertises help fill gaps when optimizing documentation of complex buildings. Described below is the process used in the first part of a restoration project, the architectural survey of Theatre Guaira Cultural Centre in Curitiba, Brazil. To diminish time on fieldwork, the two-person-field-survey team had to juggle, during three days, the continuous artistic activities and performers’ intense schedule. Both technologies (high definition laser scanning and close-range photogrammetry) were used to record all details in the least amount of time without disturbing the artists' rehearsals and performances. Laser Scanning was ideal to record the monumental stage structure with all of its existing platforms, light fixtures, scenery walls and curtains. Although scanned with high-definition, parts of the exterior façades were also recorded using Close Range Photogrammetry. Tiny cracks on the marble plaques and mosaic tiles, not visible in the point clouds, were then able to be precisely documented in order to create the exterior façades textures and damages mapping drawings. The combination of technologies and the expertise of service providers, knowing how and what to document, and what to deliver to the client, enabled maximum benefits to the following restoration project.},
	pages = {675--678},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Baptista, Vianna and L, M.},
	urldate = {2023-05-11},
	date = {2013-07-22},
	note = {Conference Name: {TC} V {\textless}br/{\textgreater} {XXIV} International {CIPA} Symposium (Volume {XL}-5/W2) - 2\&ndash;6 September 2013, Strasbourg, France
Publisher: Copernicus {GmbH}},
}

@article{orlik_3d_2021,
	title = {3D {MODELLING} {USING} {AERIAL} {OBLIQUE} {IMAGES} {WITH} {CLOSE} {RANGE} {UAV} {BASED} {DATA} {FOR} {SINGLE} {OBJECTS}},
	volume = {{XLIII}-B2-2021},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B2-2021/377/2021/isprs-archives-XLIII-B2-2021-377-2021.html},
	doi = {10.5194/isprs-archives-XLIII-B2-2021-377-2021},
	abstract = {The request for 3D Data for the use of 3D city-models is increasing rapidly. More and more tools are able to deal with data of several sensors, out of video-streams, oblique camera setups with huge overlaps as well as terrestrial data. To achieve high accuracy of the data and a fast processing pipeline, a smart workflow has to be defined and established. However, mixed data sources are still a challenge especially if different sensors with an extremely different {GSD} are used. This abstracts demonstrates such a workflow, the processing pipeline and the challenges in a mixed data processing. Special calibration and co-calibrating procedures have been applied to get model in model solution managed to solve the dual task of 3D city mapping and cultural heritage conservation. Especially the sensor setup directly influences the geometric accuracy of the product. To do missions for 2–5\&thinsp;cm {GSD}, metric systems are indispensable while for non-metric applications also simple and cheaper sensors do their job. Besides the different data-sources and sensors, the way of capturing and the related projection is a critical issue. While the classical oblique imaging is a standardized airborne application, captures with {UAVs} are more like close range photogrammetry on the facades. The combination requests specific pre-processing and definition and transformation steps.},
	pages = {377--382},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Orlik, T. and Shechter, E. B. and Kemper, G.},
	urldate = {2023-05-11},
	date = {2021-06-28},
	note = {Conference Name: {XXIV} {ISPRS} Congress {\textless}q{\textgreater}Imaging today, foreseeing tomorrow{\textless}/q{\textgreater}, Commission {II} - 2021 edition, 5\&ndash;9 July 2021
Publisher: Copernicus {GmbH}},
}

@article{antlej_combining_2011,
	title = {Combining 3D technologies in the ﬁeld of cultural heritage: three case studies},
	abstract = {The advantages of 3D technologies (3D digitisation, visualisation, 3D printing...) are recognised by various professions in the ﬁeld of cultural heritage ({CH}). Today these technologies have been technologically improved to the point that allows them to be merged for different purposes. The paper presents projects related to the successful combining of these technologies with regard to {CH}. In three case studies we discuss processes using 3D technologies for documenting and presenting artefacts, 3D collection by the Digital Library of Slovenia and directly using techology for the restoration of museum object. Although all the examples under discussion show how these tools and processes can be used for different purposes and applications in the area of {CH}.},
	author = {Antlej, K and Ericˇ, M and Šavnik, M and Županek, B and Slabe, J and Battestin, B},
	date = {2011},
	langid = {english},
}

@article{balletti_3d_2015,
	title = {3D integrated methodologies for the documentation and the virtual reconstruction of an archaeological site},
	volume = {{XL}-5/W4},
	doi = {10.5194/isprsarchives-XL-5-W4-215-2015},
	abstract = {Highly accurate documentation and 3D reconstructions are fundamental for analyses and further interpretations in archaeology. In the last years the integrated digital survey (ground-based survey methods and {UAV} photogrammetry) has confirmed its main role in the documentation and comprehension of excavation contexts, thanks to instrumental and methodological development concerning the on site data acquisition. The specific aim of the project, reported in this paper and realized by the Laboratory of Photogrammetry of the {IUAV} University of Venice, is to check different acquisition systems and their effectiveness test, considering each methodology individually or integrated. This research focuses on the awareness that the integration of different survey’s methodologies can as a matter of fact increase the representative efficacy of the final representations; these are based on a wider and verified set of georeferenced metric data. Particularly the methods’ integration allows reducing or neutralizing issues related to composite and complex objects’ survey, since the most appropriate tools and techniques can be chosen considering the characteristics of each part of an archaeological site (i.e. urban structures, architectural monuments, small findings). This paper describes the experience in several sites of the municipality of Sepino (Molise, Italy), where the 3d digital acquisition of cities and structure of monuments, sometimes hard to reach, was realized using active and passive techniques (rage-based and image based methods). This acquisition was planned in order to obtain not only the basic support for interpretation analysis, but also to achieve models of the actual state of conservation of the site on which some reconstructive hypotheses can be based on. Laser scanning data were merged with Structure from Motion techniques’ clouds into the same reference system, given by a topographical and {GPS} survey. These 3d models are not only the final results of the metric survey, but also the starting point for the whole reconstruction of the city and its urban context, from the research point of view. This reconstruction process will concern even some areas that have not yet been excavated, where the application of procedural modelling can offer an important support to the reconstructive hypothesis.},
	pages = {215--222},
	journaltitle = {{ISPRS} - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {{ISPRS} - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Balletti, Caterina and Guerra, Francesco and Scocca, V. and Gottardi, Caterina},
	date = {2015-02-18},
}

@article{croce_geomatics_nodate,
	title = {Geomatics for Cultural Heritage conservation: integrated survey and 3D modeling},
	abstract = {Purpose of this paper is to illustrate the contribution of geomatics studies in management and protection of Cultural Heritage assets. Architectural surveys, carried out by integrating traditional survey techniques with more innovative instruments, are fundamental to guide the different actors involved in conservation and restoration processes, as they allow them to get a complete and extensive knowledge of the survey object, at different levels of detail and scales of representation. The integration and elaboration of the different survey techniques are illustrated in this paper with reference to the case study of the Calci Charterhouse, unique for its historical, artistic and environmental value. Among the different studies that are performed based on integrated geomatics tools, an innovative application is here discussed, which refers to the generation of informative parametric models starting from point clouds acquired through survey, as the pipeline of a Scan-to-{HBIM} process.},
	author = {Croce, Valeria and Caroti, Gabriella and Piemonte, Andrea and Bevilacqua, Marco Giorgio},
	langid = {english},
}

@article{bodo_integrating_nodate,
	title = {Integrating point clouds to support heritage protection and {VR}/{AR} applications},
	abstract = {Current paper discusses the surveying of a {\textasciitilde}30 m high tower in a Hungarian castle; since it is a protected monument, its documentation is of national interest. Besides the surveying and data processing, the paper provides details on the procedure of sharing the collected data in virtual/augmented reality environment.},
	author = {Bodo, Gabor and Hadzijanisz, Konsztantinosz and Laki, Boglarka and Lovas, Reka and Surina, Dora and Szabo, Beatrix and Vari, Barnabas and Feher, Andras},
	langid = {english},
}

@article{jo_application_2019,
	title = {Application of Three‐dimensional Scanning, Haptic Modeling, and Printing Technologies for Restoring Damaged Artifacts},
	volume = {35},
	issn = {1225-5459, 2287-9781},
	url = {http://e-jcs.org/journal/view.php?doi=10.12654/JCS.2019.35.1.08},
	doi = {10.12654/JCS.2019.35.1.08},
	abstract = {This study examined the applicability of digital technologies based on three-dimensional(3D) scanning, modeling, and printing to the restoration of damaged artifacts. First, 3D close-range scanning was utilized to make a high-resolution polygon mesh model of a roof-end tile with a missing part, and a 3D virtual restoration of the missing part was conducted using a haptic interface. Furthermore, the virtual restoration model was printed out with a 3D printer using the material extrusion method and a {PLA} filament. Then, the additive structure of the printed output with a scanning electron microscope was observed and its shape accuracy was analyzed through 3D deviation analysis. It was discovered that the 3D printing output of the missing part has high dimensional accuracy and layer thickness, thus fitting extremely well with the fracture surface of the original roof-end tile. The convergence of digital virtual restoration based on 3D scanning and 3D printing technology has helped in minimizing contact with the artifact and broadening the choice of restoration materials significantly. In the future, if the efficiency of the virtual restoration modeling process is improved and the material stability of the printed output for the purpose of restoration is sufficiently verified, the usability of 3D digital technologies in cultural heritage restoration will increase.},
	pages = {71--80},
	number = {1},
	journaltitle = {Journal of Conservation Science},
	author = {Jo, Young Hoon and Hong, Seonghyuk},
	urldate = {2023-05-11},
	date = {2019-02-28},
	langid = {english},
}

@online{sahir_canny_2019,
	title = {Canny Edge Detection Step by Step in Python — Computer Vision},
	url = {https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123},
	abstract = {A easy to follow tutorial on how to build a Canny edge detector algorithm explained step by step.},
	titleaddon = {Medium},
	author = {Sahir, Sofiane},
	urldate = {2023-05-04},
	date = {2019-01-27},
	langid = {english},
	keywords = {Canny},
}

@book{confalone_3d_2023,
	edition = {1},
	title = {3D Scanning for Advanced Manufacturing, Design, and Construction},
	isbn = {978-1-119-75851-8 978-1-119-75853-2},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119758532},
	publisher = {Wiley},
	author = {Confalone, Gary and Smits, John and Kinnare, Thomas},
	urldate = {2023-04-16},
	date = {2023-03-07},
	langid = {english},
	doi = {10.1002/9781119758532},
}

@collection{pears_3d_2012,
	location = {London},
	title = {3D Imaging, Analysis and Applications},
	isbn = {978-1-4471-4062-7 978-1-4471-4063-4},
	url = {https://link.springer.com/10.1007/978-1-4471-4063-4},
	publisher = {Springer London},
	editor = {Pears, Nick and Liu, Yonghuai and Bunting, Peter},
	urldate = {2023-04-16},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-1-4471-4063-4},
}

@article{zhang_handbook_2013,
	title = {Handbook of 3D Machine Vision: Optical Metrology and Imaging},
	author = {Zhang, Song},
	date = {2013},
	langid = {english},
}

@book{giancola_survey_2018,
	location = {Cham},
	title = {A Survey on 3D Cameras: Metrological Comparison of Time-of-Flight, Structured-Light and Active Stereoscopy Technologies},
	isbn = {978-3-319-91760-3 978-3-319-91761-0},
	url = {http://link.springer.com/10.1007/978-3-319-91761-0},
	series = {{SpringerBriefs} in Computer Science},
	shorttitle = {A Survey on 3D Cameras},
	publisher = {Springer International Publishing},
	author = {Giancola, Silvio and Valenti, Matteo and Sala, Remo},
	urldate = {2023-04-16},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-91761-0},
}

@book{zanuttigh_time--flight_2016,
	location = {Cham},
	title = {Time-of-Flight and Structured Light Depth Cameras},
	isbn = {978-3-319-30971-2 978-3-319-30973-6},
	url = {http://link.springer.com/10.1007/978-3-319-30973-6},
	publisher = {Springer International Publishing},
	author = {Zanuttigh, Pietro and Marin, Giulio and Dal Mutto, Carlo and Dominio, Fabio and Minto, Ludovico and Cortelazzo, Guido Maria},
	urldate = {2023-04-16},
	date = {2016},
	langid = {italian},
	doi = {10.1007/978-3-319-30973-6},
}

@collection{andrews_structured_2008,
	location = {Amsterdam ; Boston},
	title = {Structured light and its applications: an introduction to phase-structured beams and nanoscale optical forces},
	isbn = {978-0-12-374027-4},
	shorttitle = {Structured light and its applications},
	pagetotal = {341},
	publisher = {Academic},
	editor = {Andrews, David L.},
	date = {2008},
	langid = {english},
	note = {{OCLC}: ocn191244950},
}

@book{wordemann_structured_2012,
	location = {Berlin, Heidelberg},
	title = {Structured Light Fields: Applications in Optical Trapping, Manipulation, and Organisation},
	isbn = {978-3-642-29322-1 978-3-642-29323-8},
	url = {https://link.springer.com/10.1007/978-3-642-29323-8},
	series = {Springer Theses},
	shorttitle = {Structured Light Fields},
	publisher = {Springer Berlin Heidelberg},
	author = {Wördemann, Mike},
	urldate = {2023-04-16},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-29323-8},
}

@collection{huri_anatomy_2021,
	location = {Cham},
	title = {Anatomy for Urologic Surgeons in the Digital Era: Scanning, Modelling and 3D Printing},
	isbn = {978-3-030-59478-7 978-3-030-59479-4},
	url = {https://link.springer.com/10.1007/978-3-030-59479-4},
	shorttitle = {Anatomy for Urologic Surgeons in the Digital Era},
	publisher = {Springer International Publishing},
	editor = {Huri, Emre and Veneziano, Domenico},
	urldate = {2023-04-16},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-59479-4},
}

@article{eva_marktfuhrerin_nodate,
	title = {{MARKTFÜHRERIN} {FÜR} {EINFACHES} {UND} {QUALITATIV} {HOCHWERTIGES} 3D-{SCANNEN}},
	author = {Eva, Artec},
	langid = {german},
	keywords = {Artec, Eva},
}

@article{hemmer_produktion_nodate,
	title = {{PRODUKTION}, {LOGISTIKA} {UND} {SERVICE} {CENTER}},
	author = {Hemmer, Rue Lou and Clara, Santa},
	langid = {german},
	keywords = {Artec, Spider},
}

@misc{kirillov_segment_2023,
	title = {Segment Anything},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything ({SA}) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model ({SAM}) and corresponding dataset ({SA}-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	number = {{arXiv}:2304.02643},
	publisher = {{arXiv}},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	urldate = {2023-04-06},
	date = {2023-04-05},
	eprinttype = {arxiv},
	eprint = {2304.02643 [cs]},
	keywords = {{AI}},
}

@online{noauthor_17_nodate,
	title = {(17条消息) Zotero文献管理软件使用指南——入门篇\_\_\_\_\_\_\_\_\_\_\_习惯的博客-{CSDN博客}},
	url = {https://blog.csdn.net/l903445981/article/details/119960684},
	urldate = {2023-04-06},
	keywords = {{GL}6\_Anwendungswoche\_Artec, {HiWi}\_Holzerkennung},
}
