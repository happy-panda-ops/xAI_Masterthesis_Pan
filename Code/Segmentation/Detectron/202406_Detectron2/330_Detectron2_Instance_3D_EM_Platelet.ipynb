{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/330_Detectron2_Instance_3D_EM_Platelet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QrSe6tbogs"
      },
      "source": [
        "https://youtu.be/cEgF0YknpZw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8l2Kg-mZ1Pb"
      },
      "source": [
        "# **Prepare environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTO_K23JaAxg"
      },
      "source": [
        "Create your own dataset by annotating for object detection using your favorite annotation software that can export annotations as COCO JSON format. I have used https://www.makesense.ai/ for my tutorial. I used the polygon tool to annotate objects and exported annotations as, \"Single file in COCO JSON format\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewiM3shDabuw"
      },
      "source": [
        "## Install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# python -m pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python3 -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "# python -m pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n",
        "\n",
        "\n",
        "import detectron2\n",
        "print(detectron2.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3RUzXAwDpmi"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import some common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQtGzZDam1Y"
      },
      "source": [
        "The default models are trained on natural images so let us go ahead and load a natural image to see if detectron is working. **We will run a pre-trained model on this image.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "Mie-EU7NDtzS",
        "outputId": "e0a24d9d-2e62-44b5-9b3c-7bddfc2218ca"
      },
      "outputs": [],
      "source": [
        "image_path = r\"B:\\01_Study\\Uni-Bamberg\\Work\\Holzprojects\\Datasets_local\\03_Domini_ip13\\Dominik-seg.v4i.coco-segmentation\\train\\Model_2_0023_jpeg.rf.89cd235857dc48c44ea58502277d0039.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "# cv2.imshow(\"image\", image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP7IeN5xbbUa"
      },
      "source": [
        "We create a detectron2 config and a detectron2 DefaultPredictor to run inference on this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JoHjpRMD8eb",
        "outputId": "359d6f5c-ed8d-4082-a140-dd0ed99a29ab"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo.  https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6wBmh_cEDw6",
        "outputId": "f41f44fb-93b3-4334-8add-93c90ec3433e"
      },
      "outputs": [],
      "source": [
        "# look at the outputs - tensors and bounding boxes.\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "d_E7J6N9EG7L",
        "outputId": "0b142c83-72c8-4ead-fc56-045e7e865eb5"
      },
      "outputs": [],
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2yUBzSPFPAS"
      },
      "source": [
        "# **Preparation Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Server*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader(server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FikRIR7S1uEO"
      },
      "source": [
        "Import the necessary function to register datasets in the COCO format. Let us register both the training and validation datasets. Please note that we are working with training (and validation) data that is is the coco format where we have a single JSON file that describes all the annotations from all training images. <p>\n",
        "Here, we are naming our training data as 'my_dataset_train' and the validation data as 'my_dataset_val'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssw3M-5HFQ3a"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wood_train\", {}, \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/train/_annotations.coco.json\", \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/train\")\n",
        "register_coco_instances(\"wood_val\", {}, \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/valid/_annotations.coco.json\", \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/valid\")\n",
        "\n",
        "train_metadata = MetadataCatalog.get(\"wood_train\")\n",
        "train_dataset_dicts = DatasetCatalog.get(\"wood_train\")\n",
        "\n",
        "val_metadata = MetadataCatalog.get(\"wood_val\")\n",
        "val_dataset_dicts = DatasetCatalog.get(\"wood_val\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check sample(server)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some random samples\n",
        "for d in random.sample(train_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])  # Read image in BGR format\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)  # Convert BGR to RGB\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    \n",
        "    vis_img = vis.get_image()  # Get the visualized image in RGB format\n",
        "    plt.imshow(vis_img)\n",
        "    plt.axis('off')  # Hide the axes for better display\n",
        "    plt.show()\n",
        "    \n",
        "# # Visualize some random samples\n",
        "# for d in random.sample(train_dataset_dicts, 2):\n",
        "#     img = cv2.imread(d[\"file_name\"])\n",
        "#     visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "#     vis = visualizer.draw_dataset_dict(d)\n",
        "#     plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Local*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader(local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wood_train\", {}, \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/train/_annotations.coco.json\", \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/train\")\n",
        "register_coco_instances(\"wood_val\", {}, \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/valid/_annotations.coco.json\", \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/valid\")\n",
        "\n",
        "train_metadata = MetadataCatalog.get(\"wood_train\")\n",
        "train_dataset_dicts = DatasetCatalog.get(\"wood_train\")\n",
        "\n",
        "val_metadata = MetadataCatalog.get(\"wood_val\")\n",
        "val_dataset_dicts = DatasetCatalog.get(\"wood_val\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check sample(local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some random samples\n",
        "for d in random.sample(train_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQNZNnWLpnc"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Train(server)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config .yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iPBoV69LrOE",
        "outputId": "7bcd42e7-4aa4-4ade-fd54-ca745406ee0d"
      },
      "outputs": [],
      "source": [
        "project_name=\"detectron2\"\n",
        "\n",
        "# # model rcnn r50 fpn 3x\n",
        "# task_name=\"test_r50_11_0617\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_3x\"\n",
        "# model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# model rcnn r101 fpn 3x\n",
        "task_name=\"r101_test_7_0618_server\"\n",
        "model_name = \"mask_rcnn_R_101_FPN_3x\"\n",
        "model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config .py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # model rcnn r 50 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"r50_400_test_1_0618_server\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_400ep_LSJ\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py\"\n",
        "\n",
        "# # model rcnn x101 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_101_FPN_400ep_LSJ.py\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_version = \"v1\"\n",
        "train_amount = 1113\n",
        "img_per_batch = 16\n",
        "max_iter = 10\n",
        "total_epoch =  math.ceil(max_iter / (train_amount // img_per_batch))\n",
        "base_lr = 0.00025\n",
        "\n",
        "test_record_csv = \"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2_test_record.csv\"\n",
        "\n",
        "# Save in csv\n",
        "\n",
        "import math\n",
        "\n",
        "new_row = [\n",
        "    task_name,\n",
        "    model_name,\n",
        "    Dataset_version,\n",
        "    train_amount,\n",
        "    img_per_batch,\n",
        "    max_iter,\n",
        "    total_epoch,\n",
        "    base_lr,\n",
        "    \"POLY_POWER = 1\",\n",
        "    \"none\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(test_record_csv, header=None)\n",
        "\n",
        "df.loc[len(df)] = new_row\n",
        "\n",
        "df.to_csv(test_record_csv, header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### config .yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg['OUTPUT_DIR'] = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg['OUTPUT_DIR']\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.DATASETS.TRAIN = (\"wood_train\",)\n",
        "cfg.DATASETS.TEST = (\"wood_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # number of data loading threads\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = img_per_batch # images per batch\n",
        "cfg.SOLVER.BASE_LR = base_lr # learning rate\n",
        "cfg.SOLVER.POLY_POWER = 1  # Set poly power to 1 to keep the learning rate nearly constant\n",
        "cfg.SOLVER.MAX_ITER = max_iter # iterations\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# save the configuration\n",
        "import yaml\n",
        "\n",
        "config_yaml_path = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config .py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg['OUTPUT_DIR'] = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg['OUTPUT_DIR']\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.DATASETS.TRAIN = (\"wood_train\",)\n",
        "cfg.DATASETS.TEST = (\"wood_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # number of data loading threads\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = img_per_batch # images per batch\n",
        "cfg.SOLVER.BASE_LR = base_lr # learning rate\n",
        "cfg.SOLVER.POLY_POWER = 1  # Set poly power to 1 to keep the learning rate nearly constant\n",
        "cfg.SOLVER.MAX_ITER = max_iter # iterations\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# save the configuration\n",
        "import yaml\n",
        "\n",
        "config_yaml_path = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clearml mac\n",
        "# %env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "# %env CLEARML_API_HOST=https://api.clear.ml\n",
        "# %env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "# %env CLEARML_API_ACCESS_KEY=QO1VBBX9J2S2VYILQTGI\n",
        "# %env CLEARML_API_SECRET_KEY=ERuc1S6o5SirQGugvYXDFjH9b9aNi0u8S3rpALzXMa8YPSLDMW\n",
        "\n",
        "# clearml win\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=Z4YDBU13VPOFHBHF8667\n",
        "%env CLEARML_API_SECRET_KEY=JMzvRXn76AT83WuFJfS0FBGCY8c5TccbH5XboTYztrWqwzDdyn\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "#Clear ML Initialization\n",
        "cl_task = Task.init(project_name=project_name,task_name=task_name)\n",
        "logger = cl_task.get_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNE3JZllGHB1"
      },
      "outputs": [],
      "source": [
        "trainer.train() #Start the training process\n",
        "cl_task.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Train(Local)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metadata training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "import math\n",
        "\n",
        "project_name=\"detectron2\"\n",
        "\n",
        "# # model rcnn r50 fpn 3x\n",
        "# task_name=\"test_r50_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_3x\"\n",
        "# model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# model rcnn r101 fpn 3x\n",
        "task_name=\"r101_test_1_0617_local\"\n",
        "model_name = \"mask_rcnn_R_101_FPN_3x\"\n",
        "model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "\n",
        "# # model rcnn r 50 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_400ep_LSJ\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py\"\n",
        "\n",
        "# # model rcnn x101 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_101_FPN_400ep_LSJ.py\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py.py\"\n",
        "\n",
        "\n",
        "Dataset_version = \"v1\"\n",
        "train_amount = 1113\n",
        "img_per_batch = 16\n",
        "max_iter = 70\n",
        "total_epoch =  math.ceil(max_iter / (train_amount // img_per_batch))\n",
        "base_lr = 0.00025\n",
        "\n",
        "test_record_csv = \"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2_test_record.csv\"\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg['OUTPUT_DIR'] = rf\"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg['OUTPUT_DIR']\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.DATASETS.TRAIN = (\"wood_train\",)\n",
        "cfg.DATASETS.TEST = (\"wood_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # number of data loading threads\n",
        "cfg.MODEL.DEVICE = \"cpu\"  # for macbook\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = img_per_batch # images per batch\n",
        "cfg.SOLVER.BASE_LR = base_lr # learning rate\n",
        "cfg.SOLVER.POLY_POWER = 1  # Set poly power to 1 to keep the learning rate nearly constant\n",
        "cfg.SOLVER.MAX_ITER = max_iter # iterations\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# save the configuration\n",
        "import yaml\n",
        "\n",
        "config_yaml_path = rf\"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)\n",
        "    \n",
        "# Save in csv\n",
        "\n",
        "import math\n",
        "\n",
        "new_row = [\n",
        "    task_name,\n",
        "    model_name,\n",
        "    Dataset_version,\n",
        "    train_amount,\n",
        "    img_per_batch,\n",
        "    max_iter,\n",
        "    total_epoch,\n",
        "    base_lr,\n",
        "    \"POLY_POWER = 1\",\n",
        "    \"none\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(test_record_csv, header=None)\n",
        "\n",
        "df.loc[len(df)] = new_row\n",
        "\n",
        "df.to_csv(test_record_csv, header=False, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clearml mac\n",
        "# %env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "# %env CLEARML_API_HOST=https://api.clear.ml\n",
        "# %env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "# %env CLEARML_API_ACCESS_KEY=QO1VBBX9J2S2VYILQTGI\n",
        "# %env CLEARML_API_SECRET_KEY=ERuc1S6o5SirQGugvYXDFjH9b9aNi0u8S3rpALzXMa8YPSLDMW\n",
        "\n",
        "# clearml win\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=Z4YDBU13VPOFHBHF8667\n",
        "%env CLEARML_API_SECRET_KEY=JMzvRXn76AT83WuFJfS0FBGCY8c5TccbH5XboTYztrWqwzDdyn\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "#Clear ML Initialization\n",
        "cl_task = Task.init(project_name=project_name,task_name=task_name)\n",
        "logger = cl_task.get_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train() #Start the training process\n",
        "cl_task.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgcPBalGMB4d"
      },
      "source": [
        "# Inference & evaluation using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyyRL4soMDdE",
        "outputId": "6f4e43aa-e0e8-4fd4-8c0f-b073be34b2a1"
      },
      "outputs": [],
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00pZTVD_WaRQ"
      },
      "source": [
        "Verify segmentation on random validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomly select one image from validation dataset and visualize predictions\n",
        "for d in random.sample(val_dataset_dicts, 1):\n",
        "    im = cv2.imread(d[\"file_name\"])  # Read image in BGR format\n",
        "    im_rgb = im[:, :, ::-1]  # Convert BGR to RGB\n",
        "    outputs = predictor(im_rgb)  # Get predictions from the model\n",
        "    \n",
        "    # Create a Visualizer instance\n",
        "    v = Visualizer(im_rgb,  # Use RGB image\n",
        "                   metadata=val_metadata,\n",
        "                   scale=0.5,\n",
        "                   # instance_mode=ColorMode.IMAGE_BW  # Optionally remove colors of unsegmented pixels\n",
        "    )\n",
        "    \n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    \n",
        "    # Get the visualized image in RGB format\n",
        "    out_img = out.get_image()\n",
        "    \n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(out_img)\n",
        "    plt.axis('off')  # Hide the axes for better display\n",
        "    plt.show()\n",
        "    \n",
        "    # Save the output image\n",
        "    original_file_name = os.path.basename(d[\"file_name\"])\n",
        "    new_file_name = f\"test_{original_file_name}\"\n",
        "    output_file_path = os.path.join(cfg.OUTPUT_DIR, new_file_name)\n",
        "    plt.imsave(output_file_path, out_img)  # Save as RGB format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_inference_on_custom_image(image_path, output_dir, predictor, val_metadata):\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    \n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=val_metadata,\n",
        "                   scale=0.5,\n",
        "                   instance_mode=ColorMode.IMAGE_BW  \n",
        "    )\n",
        "    \n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    \n",
        "    plt.imshow(out.get_image()[:, :, ::-1])\n",
        "    plt.axis('off')\n",
        "    \n",
        "    original_file_name = os.path.basename(image_path)\n",
        "    new_file_name = f\"test_{original_file_name}\"\n",
        "    output_file_path = os.path.join(output_dir, new_file_name)\n",
        "    plt.savefig(output_file_path, bbox_inches='tight', pad_inches=0)\n",
        "    # plt.close() \n",
        "\n",
        "custom_image_path = \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/test/Model_3_0020_jpeg.rf.b1ada19a12423f765881f57d7f2223d6.jpg\" \n",
        "output_dir = cfg.OUTPUT_DIR\n",
        "run_inference_on_custom_image(custom_image_path, output_dir, predictor, val_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on mac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F26poTkLPQzk"
      },
      "source": [
        "Check average precision and recall. (Need more validation data than just 2 images with handful of annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNSml38OuWV",
        "outputId": "ce72de09-f9d9-4257-dc2a-a60bc149316f"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wood_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"wood_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEDbRoL3Wytv"
      },
      "source": [
        "**Load a new image and segment it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "3s8VsDr-T-km",
        "outputId": "59bfdde9-d72b-4af0-d70d-5d503a075976"
      },
      "outputs": [],
      "source": [
        "new_im = cv2.imread(r\"B:\\01_Study\\Uni-Bamberg\\Work\\Holzprojects\\Datasets_local\\03_Domini_ip13\\Dominik-seg.v4i.coco-segmentation\\test\\Model_5_0016_jpeg.rf.04622e1c6175ec13b0be7f653916a02d.jpg\")\n",
        "outputs  = predictor(new_im)\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(new_im[:, :, ::-1], metadata=train_metadata)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Ph5_JgXFJu"
      },
      "source": [
        "**Process multiple images in a directory and save the results in an output directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSycv0yeT7IM",
        "outputId": "8bd2ed9e-59b1-4321-d55f-02b50c483003"
      },
      "outputs": [],
      "source": [
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the segmented images will be saved\n",
        "output_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results\"  # Replace this with the path to your desired output directory\n",
        "\n",
        "# Loop over the images in the input folder\n",
        "for image_filename in os.listdir(input_images_directory):\n",
        "    image_path = os.path.join(input_images_directory, image_filename)\n",
        "    new_im = cv2.imread(image_path)\n",
        "\n",
        "    # Perform prediction on the new image\n",
        "    outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "    # We can use `Visualizer` to draw the predictions on the image.\n",
        "    v = Visualizer(new_im[:, :, ::-1], metadata=train_metadata)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    # Create the output filename with _result extension\n",
        "    result_filename = os.path.splitext(image_filename)[0] + \"_result.png\"\n",
        "    output_path = os.path.join(output_directory, result_filename)\n",
        "\n",
        "    # Save the segmented image\n",
        "    cv2.imwrite(output_path, out.get_image()[:, :, ::-1])\n",
        "\n",
        "print(\"Segmentation of all images completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoouGMEDZr24"
      },
      "source": [
        "\n",
        "**Segment images and save object level information into a csv file.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_dnA1u6Zaz8",
        "outputId": "c030ebba-709d-4b69-d1c0-40496ea3f4fe"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from skimage.measure import regionprops, label\n",
        "\n",
        "\n",
        "# Assuming you have already defined the 'predictor' object and loaded the model.\n",
        "# Also, make sure 'metadata' is defined appropriately.\n",
        "\n",
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the CSV file will be saved\n",
        "output_csv_path = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results/output_objects.csv\"  # Replace this with the path to your desired output CSV file\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row in the CSV file\n",
        "    csvwriter.writerow([\"File Name\", \"Class Name\", \"Object Number\", \"Area\", \"Centroid\", \"BoundingBox\"])  # Add more columns as needed for other properties\n",
        "\n",
        "    # Loop over the images in the input folder\n",
        "    for image_filename in os.listdir(input_images_directory):\n",
        "        image_path = os.path.join(input_images_directory, image_filename)\n",
        "        new_im = cv2.imread(image_path)\n",
        "\n",
        "        # Perform prediction on the new image\n",
        "        outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "        # Convert the predicted mask to a binary mask\n",
        "        mask = outputs[\"instances\"].pred_masks.to(\"cpu\").numpy().astype(bool)\n",
        "\n",
        "        # Get the predicted class labels\n",
        "        class_labels = outputs[\"instances\"].pred_classes.to(\"cpu\").numpy()\n",
        "\n",
        "        # Debugging: print class_labels and metadata.thing_classes\n",
        "        #print(\"Class Labels:\", class_labels)\n",
        "        #print(\"Thing Classes:\", train_metadata.thing_classes)\n",
        "\n",
        "        # Use skimage.measure.regionprops to calculate object parameters\n",
        "        labeled_mask = label(mask)\n",
        "        props = regionprops(labeled_mask)\n",
        "\n",
        "        # Write the object-level information to the CSV file\n",
        "        for i, prop in enumerate(props):\n",
        "            object_number = i + 1  # Object number starts from 1\n",
        "            area = prop.area\n",
        "            centroid = prop.centroid\n",
        "            bounding_box = prop.bbox\n",
        "\n",
        "            # Check if the corresponding class label exists\n",
        "            if i < len(class_labels):\n",
        "                class_label = class_labels[i]\n",
        "                class_name = train_metadata.thing_classes[class_label]\n",
        "            else:\n",
        "                # If class label is not available (should not happen), use 'Unknown' as class name\n",
        "                class_name = 'Unknown'\n",
        "\n",
        "            # Write the object-level information to the CSV file\n",
        "            csvwriter.writerow([image_filename, class_name, object_number, area, centroid, bounding_box])  # Add more columns as needed for other properties\n",
        "\n",
        "print(\"Object-level information saved to CSV file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-RtuyQ42Rv"
      },
      "source": [
        "**Generate plots to understand the objects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h30LGnTfeIy8",
        "outputId": "c312a8a7-3bb2-4673-8055-0fb753251e66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the CSV file containing the object-level information\n",
        "csv_file_path = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results/output_objects.csv\"  # Update with your CSV file path\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Get class names from train_metadata.thing_classes\n",
        "class_names = train_metadata.thing_classes\n",
        "\n",
        "# Group the data by both \"File Name\" and \"Class Name\" and calculate the average number of objects per image for each class\n",
        "# first group the data by both \"File Name\" and \"Class Name\" and count the number of objects within each group.\n",
        "#Then, group the data by \"Class Name\" only and calculate the mean of the counts, which gives us the average number of objects per image for each class.\n",
        "avg_objects_per_class = df.groupby([\"File Name\", \"Class Name\"])[\"Object Number\"].count().reset_index()\n",
        "avg_objects_per_class = avg_objects_per_class.groupby(\"Class Name\")[\"Object Number\"].mean().reset_index()\n",
        "\n",
        "# Plot: Average number of objects per image for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"Class Name\", y=\"Object Number\", data=avg_objects_per_class, ci=None, order=class_names)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class Name\")\n",
        "plt.ylabel(\"Average Number of Objects per Image\")\n",
        "plt.title(\"Average Number of Objects per Image for Each Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Group the data by class and calculate the average area of objects for each class\n",
        "avg_area_per_class = df.groupby(\"Class Name\")[\"Area\"].mean().reset_index()\n",
        "\n",
        "# Plot: Average area of objects for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"Class Name\", y=\"Area\", data=avg_area_per_class, ci=None, order=class_names)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class Name\")\n",
        "plt.ylabel(\"Average Area of Objects\")\n",
        "plt.title(\"Average Area of Objects for Each Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3SJXvy8idl6"
      },
      "source": [
        "**Saving binary (actually multinary) images for each class for further processing.** Here, for each input image we will save n images corresponding to the number of classes. In our example, we will save 4 images for each image corresponding to the 4 classes. Each of these images will contain objects numbered 1, 2, 3, etc. - basically instance segmentation like images. These images can be used for further downstream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEayWBdnnku6",
        "outputId": "b9becf93-a66e-46df-ca09-07fcad157412"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the segmented images will be saved\n",
        "output_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results_instance\"  # Replace this with the path to your desired output directory\n",
        "\n",
        "# Loop over the images in the input folder\n",
        "for image_filename in os.listdir(input_images_directory):\n",
        "    image_path = os.path.join(input_images_directory, image_filename)\n",
        "    new_im = cv2.imread(image_path)\n",
        "\n",
        "    # Perform prediction on the new image\n",
        "    outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "    # Create a dictionary to store the mask for each class with unique integer labels\n",
        "    class_masks = {class_name: torch.zeros_like(outputs[\"instances\"].pred_masks[0], dtype=torch.uint8, device=torch.device(\"cuda:0\"))\n",
        "                   for class_name in train_metadata.thing_classes}\n",
        "\n",
        "    # Assign a unique integer label to each object in the mask\n",
        "    for i, pred_class in enumerate(outputs[\"instances\"].pred_classes):\n",
        "        class_name = train_metadata.thing_classes[pred_class]\n",
        "        class_masks[class_name] = torch.where(outputs[\"instances\"].pred_masks[i].to(device=torch.device(\"cuda:0\")),\n",
        "                                              i + 1,\n",
        "                                              class_masks[class_name])\n",
        "\n",
        "    # Save the masks for each class with unique integer labels\n",
        "    for class_name, class_mask in class_masks.items():\n",
        "        # Convert the tensor to a NumPy array and then to a regular (CPU) array\n",
        "        class_mask_np = class_mask.cpu().numpy()\n",
        "\n",
        "        # Create the output filename with _class_name_result.png extension\n",
        "        class_filename = os.path.splitext(image_filename)[0] + f\"_{class_name}_result.png\"\n",
        "        class_output_path = os.path.join(output_directory, class_filename)\n",
        "\n",
        "        # Save the image with unique integer labels\n",
        "        cv2.imwrite(class_output_path, class_mask_np.astype(np.uint8))\n",
        "\n",
        "print(\"Segmentation of all images completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8oBbRc7Xksw"
      },
      "source": [
        "# END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yZLzRZt1Vf8"
      },
      "source": [
        "**Interested in panoptic segmentation?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "eOOs4M1z1pXp",
        "outputId": "5c4dfc91-19c9-4b87-85fe-40d37e011204"
      },
      "outputs": [],
      "source": [
        "my_new_image = cv2.imread(\"/content/drive/MyDrive/ColabNotebooks/data/street_small.jpg\")\n",
        "cv2_imshow(my_new_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "tDAgk8fG21sP",
        "outputId": "d907761b-02d3-46ea-d97c-2f1c53bd025b"
      },
      "outputs": [],
      "source": [
        "# Inference with instance segmentation\n",
        "cfg_inst = get_cfg()\n",
        "cfg_inst.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg_inst.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo.  https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
        "cfg_inst.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg_inst)\n",
        "outputs = predictor(new_im)\n",
        "\n",
        "v = Visualizer(new_im[:, :, ::-1], MetadataCatalog.get(cfg_inst.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoTJ4VYx3zfn"
      },
      "source": [
        "**Panoptic segmentation = Instance segmentation + Semantic Segmentation**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "KBBjV3t01avh",
        "outputId": "f3f9411b-c772-4d2d-b82f-2623cbbd5e11"
      },
      "outputs": [],
      "source": [
        "# Inference with a panoptic segmentation model\n",
        "cfg_pan = get_cfg()\n",
        "cfg_pan.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg_pan.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg_pan)\n",
        "panoptic_seg, segments_info = predictor(new_im)[\"panoptic_seg\"]\n",
        "v = Visualizer(new_im[:, :, ::-1], MetadataCatalog.get(cfg_pan.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPhrr2e71uZZXW4xJXEF5QT",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "16uxXOT0U8KYhI02okV_sWhjgOy6Mbktv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
