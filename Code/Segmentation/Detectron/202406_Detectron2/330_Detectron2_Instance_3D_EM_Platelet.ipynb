{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/330_Detectron2_Instance_3D_EM_Platelet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QrSe6tbogs"
      },
      "source": [
        "https://youtu.be/cEgF0YknpZw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8l2Kg-mZ1Pb"
      },
      "source": [
        "# **Prepare environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTO_K23JaAxg"
      },
      "source": [
        "Create your own dataset by annotating for object detection using your favorite annotation software that can export annotations as COCO JSON format. I have used https://www.makesense.ai/ for my tutorial. I used the polygon tool to annotate objects and exported annotations as, \"Single file in COCO JSON format\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewiM3shDabuw"
      },
      "source": [
        "## Install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# python -m pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (5.1)\n",
            "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
            "Requirement already satisfied: Pillow>=7.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (10.3.0)\n",
            "Requirement already satisfied: matplotlib in /home/jpan/miniconda3/lib/python3.12/site-packages (3.8.4)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /home/jpan/miniconda3/lib/python3.12/site-packages (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /home/jpan/miniconda3/lib/python3.12/site-packages (0.1.8)\n",
            "Requirement already satisfied: tabulate in /home/jpan/miniconda3/lib/python3.12/site-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /home/jpan/miniconda3/lib/python3.12/site-packages (3.0.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (4.66.2)\n",
            "Requirement already satisfied: tensorboard in /home/jpan/miniconda3/lib/python3.12/site-packages (2.16.2)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /home/jpan/miniconda3/lib/python3.12/site-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /home/jpan/miniconda3/lib/python3.12/site-packages (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (1.3.2)\n",
            "Requirement already satisfied: black in /home/jpan/miniconda3/lib/python3.12/site-packages (24.4.2)\n",
            "Requirement already satisfied: packaging in /home/jpan/miniconda3/lib/python3.12/site-packages (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.21 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/jpan/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
            "Requirement already satisfied: PyYAML in /home/jpan/miniconda3/lib/python3.12/site-packages (from yacs>=0.1.8) (5.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (5.27.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (69.5.1)\n",
            "Requirement already satisfied: six>1.9 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: portalocker in /home/jpan/miniconda3/lib/python3.12/site-packages (from iopath<0.1.10,>=0.1.7) (2.8.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/jpan/miniconda3/lib/python3.12/site-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/jpan/miniconda3/lib/python3.12/site-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /home/jpan/miniconda3/lib/python3.12/site-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /home/jpan/miniconda3/lib/python3.12/site-packages (from black) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jpan/miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python3 -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "# python -m pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "torch:  2.3 ; cuda:  cu121\n",
            "detectron2: 0.6\n",
            "/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/__init__.py\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n",
        "\n",
        "\n",
        "import detectron2\n",
        "print(detectron2.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w3RUzXAwDpmi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "matplotlib data path: /home/jpan/miniconda3/lib/python3.12/site-packages/matplotlib/mpl-data\n",
            "CONFIGDIR=/home/jpan/.config/matplotlib\n",
            "interactive is False\n",
            "platform is linux\n",
            "CACHEDIR=/home/jpan/.cache/matplotlib\n",
            "Using fontManager instance from /home/jpan/.cache/matplotlib/fontlist-v330.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jpan/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import some common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQtGzZDam1Y"
      },
      "source": [
        "The default models are trained on natural images so let us go ahead and load a natural image to see if detectron is working. **We will run a pre-trained model on this image.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "Mie-EU7NDtzS",
        "outputId": "e0a24d9d-2e62-44b5-9b3c-7bddfc2218ca"
      },
      "outputs": [],
      "source": [
        "image_path = r\"B:\\01_Study\\Uni-Bamberg\\Work\\Holzprojects\\Datasets_local\\03_Domini_ip13\\Dominik-seg.v4i.coco-segmentation\\train\\Model_2_0023_jpeg.rf.89cd235857dc48c44ea58502277d0039.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "# cv2.imshow(\"image\", image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP7IeN5xbbUa"
      },
      "source": [
        "We create a detectron2 config and a detectron2 DefaultPredictor to run inference on this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JoHjpRMD8eb",
        "outputId": "359d6f5c-ed8d-4082-a140-dd0ed99a29ab"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo.  https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6wBmh_cEDw6",
        "outputId": "f41f44fb-93b3-4334-8add-93c90ec3433e"
      },
      "outputs": [],
      "source": [
        "# look at the outputs - tensors and bounding boxes.\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "d_E7J6N9EG7L",
        "outputId": "0b142c83-72c8-4ead-fc56-045e7e865eb5"
      },
      "outputs": [],
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2yUBzSPFPAS"
      },
      "source": [
        "# **Preparation Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Server*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader(server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FikRIR7S1uEO"
      },
      "source": [
        "Import the necessary function to register datasets in the COCO format. Let us register both the training and validation datasets. Please note that we are working with training (and validation) data that is is the coco format where we have a single JSON file that describes all the annotations from all training images. <p>\n",
        "Here, we are naming our training data as 'my_dataset_train' and the validation data as 'my_dataset_val'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ssw3M-5HFQ3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0mLoaded 1113 images in COCO format from /mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/train/_annotations.coco.json\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0mLoaded 107 images in COCO format from /mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/valid/_annotations.coco.json\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[06/19 16:22:49 d2.data.datasets.coco]: \u001b[0mLoaded 104 images in COCO format from /mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/test/_annotations.coco.json\n"
          ]
        }
      ],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wood_train\", {}, \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/train/_annotations.coco.json\", \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/train\")\n",
        "register_coco_instances(\"wood_val\", {}, \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/valid/_annotations.coco.json\", \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/valid\")\n",
        "register_coco_instances(\"wood_test\", {}, \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/test/_annotations.coco.json\", \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/test\")\n",
        "\n",
        "\n",
        "train_metadata = MetadataCatalog.get(\"wood_train\")\n",
        "train_dataset_dicts = DatasetCatalog.get(\"wood_train\")\n",
        "\n",
        "val_metadata = MetadataCatalog.get(\"wood_val\")\n",
        "val_dataset_dicts = DatasetCatalog.get(\"wood_val\")\n",
        "\n",
        "test_metadata = MetadataCatalog.get(\"wood_test\")\n",
        "test_dataset_dicts = DatasetCatalog.get(\"wood_test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check sample(server)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some random samples\n",
        "for d in random.sample(train_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])  # Read image in BGR format\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)  # Convert BGR to RGB\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    \n",
        "    vis_img = vis.get_image()  # Get the visualized image in RGB format\n",
        "    plt.imshow(vis_img)\n",
        "    plt.axis('off')  # Hide the axes for better display\n",
        "    plt.show()\n",
        "    \n",
        "# # Visualize some random samples\n",
        "# for d in random.sample(train_dataset_dicts, 2):\n",
        "#     img = cv2.imread(d[\"file_name\"])\n",
        "#     visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "#     vis = visualizer.draw_dataset_dict(d)\n",
        "#     plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Local*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader(local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wood_train\", {}, \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/train/_annotations.coco.json\", \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/train\")\n",
        "register_coco_instances(\"wood_val\", {}, \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/valid/_annotations.coco.json\", \"/Users/holmes/Documents/UNI-Bamberg/Arbeiten/Datensatz/Selfmade/3_iphone_domini_seg/Dominik-seg.v4i.coco-segmentation/valid\")\n",
        "\n",
        "train_metadata = MetadataCatalog.get(\"wood_train\")\n",
        "train_dataset_dicts = DatasetCatalog.get(\"wood_train\")\n",
        "\n",
        "val_metadata = MetadataCatalog.get(\"wood_val\")\n",
        "val_dataset_dicts = DatasetCatalog.get(\"wood_val\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check sample(local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some random samples\n",
        "for d in random.sample(train_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQNZNnWLpnc"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Train(server)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config .yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iPBoV69LrOE",
        "outputId": "7bcd42e7-4aa4-4ade-fd54-ca745406ee0d"
      },
      "outputs": [],
      "source": [
        "project_name=\"detectron2\"\n",
        "\n",
        "# # model rcnn r50 fpn 3x\n",
        "# task_name=\"test_r50_11_0617\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_3x\"\n",
        "# model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# # model rcnn r101 fpn 3x\n",
        "# task_name=\"r101_test_7_0618_server\"\n",
        "# model_name = \"mask_rcnn_R_101_FPN_3x\"\n",
        "# model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config .py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataloader': {'train': {'dataset': {'names': 'coco_2017_train', '_target_': <function get_detection_dataset_dicts at 0x7f1c91b168e0>}, 'mapper': {'is_train': True, 'augmentations': [{'min_scale': 0.1, 'max_scale': 2.0, 'target_height': 1024, 'target_width': 1024, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeScale'>}, {'crop_size': [1024, 1024], '_target_': <class 'detectron2.data.transforms.augmentation_impl.FixedSizeCrop'>}, {'horizontal': True, '_target_': <class 'detectron2.data.transforms.augmentation_impl.RandomFlip'>}], 'image_format': 'BGR', 'use_instance_mask': True, '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>, 'recompute_boxes': True}, 'total_batch_size': 64, 'num_workers': 4, '_target_': <function build_detection_train_loader at 0x7f1c91b17e20>}, 'test': {'dataset': {'names': 'coco_2017_val', 'filter_empty': False, '_target_': <function get_detection_dataset_dicts at 0x7f1c91b168e0>}, 'mapper': {'is_train': False, 'augmentations': [{'short_edge_length': 800, 'max_size': 1333, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeShortestEdge'>}], 'image_format': '${...train.mapper.image_format}', '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>}, 'num_workers': 4, '_target_': <function build_detection_test_loader at 0x7f1c91b28040>}, 'evaluator': {'dataset_name': '${..test.dataset.names}', '_target_': <class 'detectron2.evaluation.coco_evaluation.COCOEvaluator'>}}, 'lr_multiplier': {'scheduler': {'values': [1.0, 0.1, 0.01], 'milestones': [655556, 710184], 'num_updates': 737500, '_target_': <class 'fvcore.common.param_scheduler.MultiStepParamScheduler'>}, 'warmup_length': 0.002711864406779661, 'warmup_factor': 0.067, '_target_': <class 'detectron2.solver.lr_scheduler.WarmupParamScheduler'>}, 'model': {'backbone': {'bottom_up': {'stem': {'in_channels': 3, 'out_channels': 64, 'norm': 'SyncBN', '_target_': <class 'detectron2.modeling.backbone.resnet.BasicStem'>}, 'stages': {'depth': 50, 'stride_in_1x1': True, 'norm': 'SyncBN', '_target_': <function ResNet.make_default_stages at 0x7f1c91c2fa60>}, 'out_features': ['res2', 'res3', 'res4', 'res5'], '_target_': <class 'detectron2.modeling.backbone.resnet.ResNet'>, 'freeze_at': 0}, 'in_features': '${.bottom_up.out_features}', 'out_channels': 256, 'top_block': {'_target_': <class 'detectron2.modeling.backbone.fpn.LastLevelMaxPool'>}, '_target_': <class 'detectron2.modeling.backbone.fpn.FPN'>, 'norm': 'SyncBN'}, 'proposal_generator': {'in_features': ['p2', 'p3', 'p4', 'p5', 'p6'], 'head': {'in_channels': 256, 'num_anchors': 3, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.StandardRPNHead'>, 'conv_dims': [-1, -1]}, 'anchor_generator': {'sizes': [[32], [64], [128], [256], [512]], 'aspect_ratios': [0.5, 1.0, 2.0], 'strides': [4, 8, 16, 32, 64], 'offset': 0.0, '_target_': <class 'detectron2.modeling.anchor_generator.DefaultAnchorGenerator'>}, 'anchor_matcher': {'thresholds': [0.3, 0.7], 'labels': [0, -1, 1], 'allow_low_quality_matches': True, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box2box_transform': {'weights': [1.0, 1.0, 1.0, 1.0], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'batch_size_per_image': 256, 'positive_fraction': 0.5, 'pre_nms_topk': [2000, 1000], 'post_nms_topk': [1000, 1000], 'nms_thresh': 0.7, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.RPN'>}, 'roi_heads': {'num_classes': 80, 'batch_size_per_image': 512, 'positive_fraction': 0.25, 'proposal_matcher': {'thresholds': [0.5], 'labels': [0, 1], 'allow_low_quality_matches': False, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box_in_features': ['p2', 'p3', 'p4', 'p5'], 'box_pooler': {'output_size': 7, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'box_head': {'input_shape': {'channels': 256, 'height': 7, 'width': 7, 'stride': None}, 'conv_dims': [256, 256, 256, 256], 'fc_dims': [1024], '_target_': <class 'detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, 'box_predictor': {'input_shape': {'channels': 1024, 'height': None, 'width': None, 'stride': None}, 'test_score_thresh': 0.05, 'box2box_transform': {'weights': [10, 10, 5, 5], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'num_classes': '${..num_classes}', '_target_': <class 'detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers'>}, 'mask_in_features': ['p2', 'p3', 'p4', 'p5'], 'mask_pooler': {'output_size': 14, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'mask_head': {'input_shape': {'channels': 256, 'height': 14, 'width': 14, 'stride': None}, 'num_classes': '${..num_classes}', 'conv_dims': [256, 256, 256, 256, 256], '_target_': <class 'detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, '_target_': <class 'detectron2.modeling.roi_heads.roi_heads.StandardROIHeads'>}, 'pixel_mean': [103.53, 116.28, 123.675], 'pixel_std': [1.0, 1.0, 1.0], 'input_format': 'BGR', '_target_': <class 'detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN'>}, 'optimizer': {'params': {'weight_decay_norm': 0.0, '_target_': <function get_default_optimizer_params at 0x7f1c9198e8e0>}, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, '_target_': <class 'torch.optim.sgd.SGD'>}, 'train': {'output_dir': './output', 'init_checkpoint': '', 'max_iter': 737500, 'amp': {'enabled': True}, 'ddp': {'broadcast_buffers': False, 'find_unused_parameters': False, 'fp16_compression': True}, 'checkpointer': {'period': 5000, 'max_to_keep': 100}, 'eval_period': 5000, 'log_period': 20, 'device': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "from detectron2.model_zoo import get_config\n",
        "from detectron2.config import LazyConfig\n",
        "\n",
        "\n",
        "# model rcnn r 50 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "task_name=\"r50_400_test_1_0618_server\"\n",
        "model_name = \"mask_rcnn_R_50_FPN_400ep_LSJ\"\n",
        "model_config = \"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/configs/new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py\"\n",
        "\n",
        "# # model rcnn x101 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_101_FPN_400ep_LSJ.py\"\n",
        "# model_config = \"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/model_zoo/configs/new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py\"\n",
        "\n",
        "# cfg = LazyConfig.to_py(get_config(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py\"))\n",
        "\n",
        "# cfg = LazyConfig.load(model_config)\n",
        "from detectron2.model_zoo import get_config\n",
        "from detectron2.config import LazyConfig\n",
        "\n",
        "cfg = get_config(\"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py\")\n",
        "\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "Dataset_version = \"v1\"\n",
        "train_amount = 1113\n",
        "img_per_batch = 16\n",
        "max_iter = 10\n",
        "total_epoch =  math.ceil(max_iter / (train_amount // img_per_batch))\n",
        "base_lr = 0.00025\n",
        "poly_power = 1\n",
        "\n",
        "test_record_csv = \"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2_test_record.csv\"\n",
        "\n",
        "# Save in csv\n",
        "new_row = [\n",
        "    task_name,\n",
        "    model_name,\n",
        "    Dataset_version,\n",
        "    train_amount,\n",
        "    img_per_batch,\n",
        "    max_iter,\n",
        "    total_epoch,\n",
        "    base_lr,\n",
        "    poly_power,\n",
        "    \"none\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(test_record_csv, header=None)\n",
        "\n",
        "df.loc[len(df)] = new_row\n",
        "\n",
        "df.to_csv(test_record_csv, header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### config .yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg['OUTPUT_DIR'] = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg['OUTPUT_DIR']\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.DATASETS.TRAIN = (\"wood_train\",)\n",
        "cfg.DATASETS.TEST = (\"wood_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # number of data loading threads\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = img_per_batch # images per batch\n",
        "cfg.SOLVER.BASE_LR = base_lr # learning rate\n",
        "cfg.SOLVER.POLY_POWER = 1  # Set poly power to 1 to keep the learning rate nearly constant\n",
        "cfg.SOLVER.MAX_ITER = max_iter # iterations\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# save the configuration\n",
        "import yaml\n",
        "\n",
        "config_yaml_path = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config .py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataloader': {'train': {'dataset': {'names': 'wood_train', '_target_': 'wood_train'}, 'mapper': {'is_train': True, 'augmentations': [{'min_scale': 0.1, 'max_scale': 2.0, 'target_height': 1024, 'target_width': 1024, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeScale'>}, {'crop_size': [1024, 1024], '_target_': <class 'detectron2.data.transforms.augmentation_impl.FixedSizeCrop'>}, {'horizontal': True, '_target_': <class 'detectron2.data.transforms.augmentation_impl.RandomFlip'>}], 'image_format': 'BGR', 'use_instance_mask': True, '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>, 'recompute_boxes': True}, 'total_batch_size': 16, 'num_workers': 3, '_target_': 'wood_train'}, 'test': {'dataset': {'names': 'wood_val', 'filter_empty': False, '_target_': 'wood_val'}, 'mapper': {'is_train': False, 'augmentations': [{'short_edge_length': 800, 'max_size': 1333, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeShortestEdge'>}], 'image_format': '${...train.mapper.image_format}', '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>}, 'num_workers': 4, '_target_': 'wood_val'}, 'evaluator': {'dataset_name': 'wood_test', '_target_': <class 'detectron2.evaluation.coco_evaluation.COCOEvaluator'>}}, 'lr_multiplier': {'scheduler': {'values': [1.0, 0.1, 0.01], 'milestones': [655556, 710184], 'num_updates': 737500, '_target_': <class 'fvcore.common.param_scheduler.MultiStepParamScheduler'>}, 'warmup_length': 0.002711864406779661, 'warmup_factor': 0.067, '_target_': <class 'detectron2.solver.lr_scheduler.WarmupParamScheduler'>}, 'model': {'backbone': {'bottom_up': {'stem': {'in_channels': 3, 'out_channels': 64, 'norm': 'SyncBN', '_target_': <class 'detectron2.modeling.backbone.resnet.BasicStem'>}, 'stages': {'depth': 50, 'stride_in_1x1': True, 'norm': 'SyncBN', '_target_': <function ResNet.make_default_stages at 0x7f1c91c2fa60>}, 'out_features': ['res2', 'res3', 'res4', 'res5'], '_target_': <class 'detectron2.modeling.backbone.resnet.ResNet'>, 'freeze_at': 0}, 'in_features': '${.bottom_up.out_features}', 'out_channels': 256, 'top_block': {'_target_': <class 'detectron2.modeling.backbone.fpn.LastLevelMaxPool'>}, '_target_': <class 'detectron2.modeling.backbone.fpn.FPN'>, 'norm': 'SyncBN'}, 'proposal_generator': {'in_features': ['p2', 'p3', 'p4', 'p5', 'p6'], 'head': {'in_channels': 256, 'num_anchors': 3, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.StandardRPNHead'>, 'conv_dims': [-1, -1]}, 'anchor_generator': {'sizes': [[32], [64], [128], [256], [512]], 'aspect_ratios': [0.5, 1.0, 2.0], 'strides': [4, 8, 16, 32, 64], 'offset': 0.0, '_target_': <class 'detectron2.modeling.anchor_generator.DefaultAnchorGenerator'>}, 'anchor_matcher': {'thresholds': [0.3, 0.7], 'labels': [0, -1, 1], 'allow_low_quality_matches': True, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box2box_transform': {'weights': [1.0, 1.0, 1.0, 1.0], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'batch_size_per_image': 256, 'positive_fraction': 0.5, 'pre_nms_topk': [2000, 1000], 'post_nms_topk': [1000, 1000], 'nms_thresh': 0.7, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.RPN'>}, 'roi_heads': {'num_classes': 3, 'batch_size_per_image': 512, 'positive_fraction': 0.25, 'proposal_matcher': {'thresholds': [0.5], 'labels': [0, 1], 'allow_low_quality_matches': False, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box_in_features': ['p2', 'p3', 'p4', 'p5'], 'box_pooler': {'output_size': 7, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'box_head': {'input_shape': {'channels': 256, 'height': 7, 'width': 7, 'stride': None}, 'conv_dims': [256, 256, 256, 256], 'fc_dims': [1024], '_target_': <class 'detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, 'box_predictor': {'input_shape': {'channels': 1024, 'height': None, 'width': None, 'stride': None}, 'test_score_thresh': 0.05, 'box2box_transform': {'weights': [10, 10, 5, 5], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'num_classes': '${..num_classes}', '_target_': <class 'detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers'>}, 'mask_in_features': ['p2', 'p3', 'p4', 'p5'], 'mask_pooler': {'output_size': 14, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'mask_head': {'input_shape': {'channels': 256, 'height': 14, 'width': 14, 'stride': None}, 'num_classes': '${..num_classes}', 'conv_dims': [256, 256, 256, 256, 256], '_target_': <class 'detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, '_target_': <class 'detectron2.modeling.roi_heads.roi_heads.StandardROIHeads'>}, 'pixel_mean': [103.53, 116.28, 123.675], 'pixel_std': [1.0, 1.0, 1.0], 'input_format': 'BGR', '_target_': <class 'detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN'>}, 'optimizer': {'params': {'weight_decay_norm': 0.0, '_target_': <function get_default_optimizer_params at 0x7f1c9198e8e0>}, 'lr': 0.00025, 'momentum': 0.9, 'weight_decay': 4e-05, '_target_': <class 'torch.optim.sgd.SGD'>}, 'train': {'output_dir': '/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/r50_400_test_1_0618_server', 'init_checkpoint': '', 'max_iter': 10, 'amp': {'enabled': True}, 'ddp': {'broadcast_buffers': False, 'find_unused_parameters': False, 'fp16_compression': True}, 'checkpointer': {'period': 5000, 'max_to_keep': 100}, 'eval_period': 5000, 'log_period': 20, 'device': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg.train.output_dir = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg.train.output_dir\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.dataloader.train.dataset.names = \"wood_train\"\n",
        "cfg.dataloader.test.dataset.names = \"wood_val\"\n",
        "cfg.dataloader.evaluator.dataset_name = \"wood_test\"\n",
        "\n",
        "cfg.dataloader.train.dataset._target_ = \"wood_train\"\n",
        "cfg.dataloader.train._target_ = \"wood_train\"\n",
        "\n",
        "cfg.dataloader.test.dataset._target_ = \"wood_val\"\n",
        "cfg.dataloader.test._target_ = \"wood_val\"\n",
        "\n",
        "cfg.dataloader.train.total_batch_size = img_per_batch\n",
        "cfg.dataloader.train.num_workers = 3\n",
        "cfg.train.max_iter = max_iter\n",
        "cfg.optimizer.lr = base_lr\n",
        "# cfg.optimizer.weight_decay = None\n",
        "\n",
        "cfg.model.roi_heads.num_classes = 3\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.train.output_dir, exist_ok=True)\n",
        "\n",
        "# !\n",
        "# trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "# trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# # save the configuration\n",
        "# import yaml\n",
        "\n",
        "# config_yaml_path = rf\"/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "# directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "# if not os.path.exists(directory):\n",
        "#     os.makedirs(directory)\n",
        "\n",
        "# with open(config_yaml_path, 'w') as file:\n",
        "#     yaml.dump(cfg, file)\n",
        "\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cfg.dataloader.train.dataset.names)\n",
        "print(cfg.dataloader.test.dataset.names)\n",
        "print(cfg.dataloader.train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  --------------------------------------------------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2\n",
            "detectron2._C                    not built correctly: No module named 'detectron2._C'\n",
            "Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
            "CUDA compiler                    Cuda compilation tools, release 10.1, V10.1.243\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.3.1+cu121 @/home/jpan/miniconda3/lib/python3.12/site-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0,1                          NVIDIA RTX A5000 (arch=8.6)\n",
            "Driver version\n",
            "CUDA_HOME                        /usr\n",
            "Pillow                           10.3.0\n",
            "torchvision                      0.18.1+cu121 @/home/jpan/miniconda3/lib/python3.12/site-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.10.0\n",
            "-------------------------------  --------------------------------------------------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
            "\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  --------------------------------------------------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2\n",
            "detectron2._C                    not built correctly: No module named 'detectron2._C'\n",
            "Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
            "CUDA compiler                    Cuda compilation tools, release 10.1, V10.1.243\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.3.1+cu121 @/home/jpan/miniconda3/lib/python3.12/site-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0,1                          NVIDIA RTX A5000 (arch=8.6)\n",
            "Driver version\n",
            "CUDA_HOME                        /usr\n",
            "Pillow                           10.3.0\n",
            "torchvision                      0.18.1+cu121 @/home/jpan/miniconda3/lib/python3.12/site-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.10.0\n",
            "-------------------------------  --------------------------------------------------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
            "\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mCommand line arguments: <__main__.Args object at 0x7f1c91ca8350>\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mCommand line arguments: <__main__.Args object at 0x7f1c91ca8350>\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mContents of args.config_file=/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/configs/new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py:\n",
            "\u001b[38;5;204mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmask_rcnn_R_50_FPN_100ep_LSJ\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m(\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m)\u001b[39m\n",
            "\n",
            "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmax_iter\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m*\u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 100ep -> 400ep\u001b[39m\n",
            "\n",
            "\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmilestones\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mmilestone\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mfor\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmilestone\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204min\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmilestones\u001b[39m\n",
            "\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mnum_updates\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmax_iter\u001b[39m\n",
            "\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mContents of args.config_file=/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/configs/new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py:\n",
            "\u001b[38;5;204mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmask_rcnn_R_50_FPN_100ep_LSJ\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m(\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;15m,\u001b[39m\n",
            "\u001b[38;5;15m)\u001b[39m\n",
            "\n",
            "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmax_iter\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m*\u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 100ep -> 400ep\u001b[39m\n",
            "\n",
            "\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmilestones\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mmilestone\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mfor\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmilestone\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204min\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmilestones\u001b[39m\n",
            "\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mscheduler\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mnum_updates\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;204m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;204m.\u001b[39m\u001b[38;5;15mmax_iter\u001b[39m\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:25:03 d2.config.lazy]: \u001b[0mThe config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:25:03 d2.config.lazy]: \u001b[0mThe config contains objects that cannot serialize to a valid yaml. ./output/config.yaml is human-readable but cannot be loaded.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:25:03 d2.config.lazy]: \u001b[0mConfig is saved using cloudpickle at ./output/config.yaml.pkl.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/19 16:25:03 d2.config.lazy]: \u001b[0mConfig is saved using cloudpickle at ./output/config.yaml.pkl.\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[06/19 16:25:03 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[06/19 16:25:03 d2.utils.env]: \u001b[0mUsing a generated random seed 4558783\n",
            "\u001b[32m[06/19 16:25:03 d2.utils.env]: \u001b[0mUsing a generated random seed 4558783\n",
            "\u001b[32m[06/19 16:25:04 detectron2]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(\n",
            "      256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output2): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral3): Conv2d(\n",
            "      512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output3): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral4): Conv2d(\n",
            "      1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output4): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral5): Conv2d(\n",
            "      2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output5): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Sequential(\n",
            "        (conv0): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (conv1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[06/19 16:25:04 detectron2]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(\n",
            "      256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output2): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral3): Conv2d(\n",
            "      512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output3): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral4): Conv2d(\n",
            "      1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output4): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral5): Conv2d(\n",
            "      2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output5): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Sequential(\n",
            "        (conv0): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (conv1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): NaiveSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "model\n",
            "model2\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets/coco/annotations/instances_train2017.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#  Jupyter Notebook \u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 104\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(do_test(cfg, model))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mdo_train\u001b[0;34m(args, cfg)\u001b[0m\n\u001b[1;32m     49\u001b[0m optim \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m create_ddp_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mddp)\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/config/instantiate.py:67\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OmegaConf\u001b[38;5;241m.\u001b[39mto_object(cfg)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg, abc\u001b[38;5;241m.\u001b[39mMapping) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cfg:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# conceptually equivalent to hydra.utils.instantiate(cfg) with _convert_=all,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# but faster: https://github.com/facebookresearch/hydra/issues/1200\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m {k: \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m instantiate(\u001b[38;5;28mcls\u001b[39m)\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/config/instantiate.py:83\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not define a callable object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/data/build.py:253\u001b[0m, in \u001b[0;36mget_detection_dataset_dicts\u001b[0;34m(names, filter_empty, min_keypoints, proposal_files, check_consistency)\u001b[0m\n\u001b[1;32m    246\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    247\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following dataset names are not registered in the DatasetCatalog: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames_set\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable datasets are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m dataset_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43mDatasetCatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset_dicts[\u001b[38;5;241m0\u001b[39m], torchdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_dicts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;66;03m# ConcatDataset does not work for iterable style dataset.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m# We could support concat for iterable as well, but it's often\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# not a good idea to concat iterables anyway.\u001b[39;00m\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/data/catalog.py:58\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not registered! Available datasets are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     55\u001b[0m             name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/data/datasets/coco.py:510\u001b[0m, in \u001b[0;36mregister_coco_instances.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_root, (\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)), image_root\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# 1. register a function which returns dicts\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m DatasetCatalog\u001b[38;5;241m.\u001b[39mregister(name, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mload_coco_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# 2. Optionally, add metadata about this dataset,\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# since they might be useful in evaluation, visualization or logging\u001b[39;00m\n\u001b[1;32m    514\u001b[0m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m    515\u001b[0m     json_file\u001b[38;5;241m=\u001b[39mjson_file, image_root\u001b[38;5;241m=\u001b[39mimage_root, evaluator_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata\n\u001b[1;32m    516\u001b[0m )\n",
            "File \u001b[0;32m~/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2/detectron2/data/datasets/coco.py:74\u001b[0m, in \u001b[0;36mload_coco_json\u001b[0;34m(json_file, image_root, dataset_name, extra_annotation_keys)\u001b[0m\n\u001b[1;32m     72\u001b[0m json_file \u001b[38;5;241m=\u001b[39m PathManager\u001b[38;5;241m.\u001b[39mget_local_path(json_file)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n\u001b[0;32m---> 74\u001b[0m     coco_api \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timer\u001b[38;5;241m.\u001b[39mseconds() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m takes \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json_file, timer\u001b[38;5;241m.\u001b[39mseconds()))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_train2017.json'"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import LazyConfig, instantiate\n",
        "from detectron2.engine import (\n",
        "    AMPTrainer,\n",
        "    SimpleTrainer,\n",
        "    default_setup,\n",
        "    default_writers,\n",
        "    hooks,\n",
        "    launch,\n",
        ")\n",
        "from detectron2.engine.defaults import create_ddp_model\n",
        "from detectron2.evaluation import inference_on_dataset, print_csv_format\n",
        "from detectron2.utils import comm\n",
        "\n",
        "logger = logging.getLogger(\"detectron2\")\n",
        "\n",
        "class Args:\n",
        "    config_file = model_config\n",
        "    resume = False\n",
        "    eval_only = False\n",
        "    num_gpus = 1\n",
        "    num_machines = 1\n",
        "    machine_rank = 0\n",
        "    dist_url = \"tcp://127.0.0.1:50152\"\n",
        "    opts = []\n",
        "\n",
        "args = Args()\n",
        "\n",
        "def do_test(cfg, model):\n",
        "    if \"evaluator\" in cfg.dataloader:\n",
        "        ret = inference_on_dataset(\n",
        "            model,\n",
        "            instantiate(cfg.dataloader.test),\n",
        "            instantiate(cfg.dataloader.evaluator),\n",
        "        )\n",
        "        print_csv_format(ret)\n",
        "        return ret\n",
        "\n",
        "def do_train(args, cfg):\n",
        "    model = instantiate(cfg.model)\n",
        "    logger = logging.getLogger(\"detectron2\")\n",
        "    logger.info(\"Model:\\n{}\".format(model))\n",
        "    model.to(cfg.train.device)\n",
        "    print(\"model\")\n",
        "    \n",
        "    cfg.optimizer.params.model = model\n",
        "    optim = instantiate(cfg.optimizer)\n",
        "    print(\"model2\")\n",
        "\n",
        "\n",
        "    train_loader = instantiate(cfg.dataloader.train)\n",
        "    print(\"model3\")\n",
        "\n",
        "\n",
        "    model = create_ddp_model(model, **cfg.train.ddp)\n",
        "    trainer = (AMPTrainer if cfg.train.amp.enabled else SimpleTrainer)(model, train_loader, optim)\n",
        "    checkpointer = DetectionCheckpointer(\n",
        "        model,\n",
        "        cfg.train.output_dir,\n",
        "        trainer=trainer,\n",
        "    )\n",
        "    trainer.register_hooks(\n",
        "        [\n",
        "            hooks.IterationTimer(),\n",
        "            hooks.LRScheduler(scheduler=instantiate(cfg.lr_multiplier)),\n",
        "            (\n",
        "                hooks.PeriodicCheckpointer(checkpointer, **cfg.train.checkpointer)\n",
        "                if comm.is_main_process()\n",
        "                else None\n",
        "            ),\n",
        "            hooks.EvalHook(cfg.train.eval_period, lambda: do_test(cfg, model)),\n",
        "            (\n",
        "                hooks.PeriodicWriter(\n",
        "                    default_writers(cfg.train.output_dir, cfg.train.max_iter),\n",
        "                    period=cfg.train.log_period,\n",
        "                )\n",
        "                if comm.is_main_process()\n",
        "                else None\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    checkpointer.resume_or_load(cfg.train.init_checkpoint, resume=args.resume)\n",
        "    if args.resume and checkpointer.has_checkpoint():\n",
        "        start_iter = trainer.iter + 1\n",
        "    else:\n",
        "        start_iter = 0\n",
        "    trainer.train(start_iter, cfg.train.max_iter)\n",
        "\n",
        "def main(args):\n",
        "    cfg = LazyConfig.load(args.config_file)\n",
        "    cfg = LazyConfig.apply_overrides(cfg, args.opts)\n",
        "    default_setup(cfg, args)\n",
        "\n",
        "    if args.eval_only:\n",
        "        model = instantiate(cfg.model)\n",
        "        model.to(cfg.train.device)\n",
        "        model = create_ddp_model(model)\n",
        "        DetectionCheckpointer(model).load(cfg.train.init_checkpoint)\n",
        "        print(do_test(cfg, model))\n",
        "    else:\n",
        "        do_train(args, cfg)\n",
        "\n",
        "#  Jupyter Notebook \n",
        "if __name__ == \"__main__\":\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataloader': {'train': {'dataset': {'names': 'wood_train', '_target_': 'wood_train'}, 'mapper': {'is_train': True, 'augmentations': [{'min_scale': 0.1, 'max_scale': 2.0, 'target_height': 1024, 'target_width': 1024, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeScale'>}, {'crop_size': [1024, 1024], '_target_': <class 'detectron2.data.transforms.augmentation_impl.FixedSizeCrop'>}, {'horizontal': True, '_target_': <class 'detectron2.data.transforms.augmentation_impl.RandomFlip'>}], 'image_format': 'BGR', 'use_instance_mask': True, '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>, 'recompute_boxes': True}, 'total_batch_size': 16, 'num_workers': 3, '_target_': 'wood_train'}, 'test': {'dataset': {'names': 'wood_val', 'filter_empty': False, '_target_': 'wood_val'}, 'mapper': {'is_train': False, 'augmentations': [{'short_edge_length': 800, 'max_size': 1333, '_target_': <class 'detectron2.data.transforms.augmentation_impl.ResizeShortestEdge'>}], 'image_format': '${...train.mapper.image_format}', '_target_': <class 'detectron2.data.dataset_mapper.DatasetMapper'>}, 'num_workers': 4, '_target_': 'wood_val'}, 'evaluator': {'dataset_name': 'wood_test', '_target_': <class 'detectron2.evaluation.coco_evaluation.COCOEvaluator'>}}, 'lr_multiplier': {'scheduler': {'values': [1.0, 0.1, 0.01], 'milestones': [655556, 710184], 'num_updates': 737500, '_target_': <class 'fvcore.common.param_scheduler.MultiStepParamScheduler'>}, 'warmup_length': 0.002711864406779661, 'warmup_factor': 0.067, '_target_': <class 'detectron2.solver.lr_scheduler.WarmupParamScheduler'>}, 'model': {'backbone': {'bottom_up': {'stem': {'in_channels': 3, 'out_channels': 64, 'norm': 'SyncBN', '_target_': <class 'detectron2.modeling.backbone.resnet.BasicStem'>}, 'stages': {'depth': 50, 'stride_in_1x1': True, 'norm': 'SyncBN', '_target_': <function ResNet.make_default_stages at 0x7f1c91c2fa60>}, 'out_features': ['res2', 'res3', 'res4', 'res5'], '_target_': <class 'detectron2.modeling.backbone.resnet.ResNet'>, 'freeze_at': 0}, 'in_features': '${.bottom_up.out_features}', 'out_channels': 256, 'top_block': {'_target_': <class 'detectron2.modeling.backbone.fpn.LastLevelMaxPool'>}, '_target_': <class 'detectron2.modeling.backbone.fpn.FPN'>, 'norm': 'SyncBN'}, 'proposal_generator': {'in_features': ['p2', 'p3', 'p4', 'p5', 'p6'], 'head': {'in_channels': 256, 'num_anchors': 3, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.StandardRPNHead'>, 'conv_dims': [-1, -1]}, 'anchor_generator': {'sizes': [[32], [64], [128], [256], [512]], 'aspect_ratios': [0.5, 1.0, 2.0], 'strides': [4, 8, 16, 32, 64], 'offset': 0.0, '_target_': <class 'detectron2.modeling.anchor_generator.DefaultAnchorGenerator'>}, 'anchor_matcher': {'thresholds': [0.3, 0.7], 'labels': [0, -1, 1], 'allow_low_quality_matches': True, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box2box_transform': {'weights': [1.0, 1.0, 1.0, 1.0], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'batch_size_per_image': 256, 'positive_fraction': 0.5, 'pre_nms_topk': [2000, 1000], 'post_nms_topk': [1000, 1000], 'nms_thresh': 0.7, '_target_': <class 'detectron2.modeling.proposal_generator.rpn.RPN'>}, 'roi_heads': {'num_classes': 3, 'batch_size_per_image': 512, 'positive_fraction': 0.25, 'proposal_matcher': {'thresholds': [0.5], 'labels': [0, 1], 'allow_low_quality_matches': False, '_target_': <class 'detectron2.modeling.matcher.Matcher'>}, 'box_in_features': ['p2', 'p3', 'p4', 'p5'], 'box_pooler': {'output_size': 7, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'box_head': {'input_shape': {'channels': 256, 'height': 7, 'width': 7, 'stride': None}, 'conv_dims': [256, 256, 256, 256], 'fc_dims': [1024], '_target_': <class 'detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, 'box_predictor': {'input_shape': {'channels': 1024, 'height': None, 'width': None, 'stride': None}, 'test_score_thresh': 0.05, 'box2box_transform': {'weights': [10, 10, 5, 5], '_target_': <class 'detectron2.modeling.box_regression.Box2BoxTransform'>}, 'num_classes': '${..num_classes}', '_target_': <class 'detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers'>}, 'mask_in_features': ['p2', 'p3', 'p4', 'p5'], 'mask_pooler': {'output_size': 14, 'scales': [0.25, 0.125, 0.0625, 0.03125], 'sampling_ratio': 0, 'pooler_type': 'ROIAlignV2', '_target_': <class 'detectron2.modeling.poolers.ROIPooler'>}, 'mask_head': {'input_shape': {'channels': 256, 'height': 14, 'width': 14, 'stride': None}, 'num_classes': '${..num_classes}', 'conv_dims': [256, 256, 256, 256, 256], '_target_': <class 'detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead'>, 'conv_norm': <function <lambda> at 0x7f1c919ab380>}, '_target_': <class 'detectron2.modeling.roi_heads.roi_heads.StandardROIHeads'>}, 'pixel_mean': [103.53, 116.28, 123.675], 'pixel_std': [1.0, 1.0, 1.0], 'input_format': 'BGR', '_target_': <class 'detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN'>}, 'optimizer': {'params': {'weight_decay_norm': 0.0, '_target_': <function get_default_optimizer_params at 0x7f1c9198e8e0>}, 'lr': 0.00025, 'momentum': 0.9, 'weight_decay': 4e-05, '_target_': <class 'torch.optim.sgd.SGD'>}, 'train': {'output_dir': '/home/jpan/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/r50_400_test_1_0618_server', 'init_checkpoint': '', 'max_iter': 10, 'amp': {'enabled': True}, 'ddp': {'broadcast_buffers': False, 'find_unused_parameters': False, 'fp16_compression': True}, 'checkpointer': {'period': 5000, 'max_to_keep': 100}, 'eval_period': 5000, 'log_period': 20, 'device': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clearml mac\n",
        "# %env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "# %env CLEARML_API_HOST=https://api.clear.ml\n",
        "# %env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "# %env CLEARML_API_ACCESS_KEY=QO1VBBX9J2S2VYILQTGI\n",
        "# %env CLEARML_API_SECRET_KEY=ERuc1S6o5SirQGugvYXDFjH9b9aNi0u8S3rpALzXMa8YPSLDMW\n",
        "\n",
        "# clearml win\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=Z4YDBU13VPOFHBHF8667\n",
        "%env CLEARML_API_SECRET_KEY=JMzvRXn76AT83WuFJfS0FBGCY8c5TccbH5XboTYztrWqwzDdyn\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "#Clear ML Initialization\n",
        "cl_task = Task.init(project_name=project_name,task_name=task_name)\n",
        "logger = cl_task.get_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNE3JZllGHB1"
      },
      "outputs": [],
      "source": [
        "trainer.train() #Start the training process\n",
        "cl_task.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Train(Local)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metadata training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "import math\n",
        "\n",
        "project_name=\"detectron2\"\n",
        "\n",
        "# # model rcnn r50 fpn 3x\n",
        "# task_name=\"test_r50_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_3x\"\n",
        "# model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# model rcnn r101 fpn 3x\n",
        "task_name=\"r101_test_1_0617_local\"\n",
        "model_name = \"mask_rcnn_R_101_FPN_3x\"\n",
        "model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "\n",
        "# # model rcnn r 50 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_50_FPN_400ep_LSJ\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py\"\n",
        "\n",
        "# # model rcnn x101 fpn using Large-Scale Jitter and Longer Training Schedule\n",
        "# task_name=\"test_x101_11_0617_local\"\n",
        "# model_name = \"mask_rcnn_R_101_FPN_400ep_LSJ.py\"\n",
        "# model_config = \"new_baselines/mask_rcnn_R_101_FPN_400ep_LSJ.py.py\"\n",
        "\n",
        "\n",
        "Dataset_version = \"v1\"\n",
        "train_amount = 1113\n",
        "img_per_batch = 16\n",
        "max_iter = 70\n",
        "total_epoch =  math.ceil(max_iter / (train_amount // img_per_batch))\n",
        "base_lr = 0.00025\n",
        "\n",
        "test_record_csv = \"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/detectron2_test_record.csv\"\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg['OUTPUT_DIR'] = rf\"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}\"\n",
        "output_directory = cfg['OUTPUT_DIR']\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.DATASETS.TRAIN = (\"wood_train\",)\n",
        "cfg.DATASETS.TEST = (\"wood_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # number of data loading threads\n",
        "cfg.MODEL.DEVICE = \"cpu\"  # for macbook\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = img_per_batch # images per batch\n",
        "cfg.SOLVER.BASE_LR = base_lr # learning rate\n",
        "cfg.SOLVER.POLY_POWER = 1  # Set poly power to 1 to keep the learning rate nearly constant\n",
        "cfg.SOLVER.MAX_ITER = max_iter # iterations\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "# NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available\n",
        "\n",
        "# save the configuration\n",
        "import yaml\n",
        "\n",
        "config_yaml_path = rf\"/Users/holmes/Documents/UNI-Bamberg/4.Semester_MA/Masterthesis/xAI_Masterthesis_Pan/Code/Segmentation/Detectron/202406_Detectron2/{task_name}/config.yaml\"\n",
        "directory = os.path.dirname(config_yaml_path)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)\n",
        "    \n",
        "# Save in csv\n",
        "\n",
        "import math\n",
        "\n",
        "new_row = [\n",
        "    task_name,\n",
        "    model_name,\n",
        "    Dataset_version,\n",
        "    train_amount,\n",
        "    img_per_batch,\n",
        "    max_iter,\n",
        "    total_epoch,\n",
        "    base_lr,\n",
        "    \"POLY_POWER = 1\",\n",
        "    \"none\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(test_record_csv, header=None)\n",
        "\n",
        "df.loc[len(df)] = new_row\n",
        "\n",
        "df.to_csv(test_record_csv, header=False, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clearml mac\n",
        "# %env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "# %env CLEARML_API_HOST=https://api.clear.ml\n",
        "# %env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "# %env CLEARML_API_ACCESS_KEY=QO1VBBX9J2S2VYILQTGI\n",
        "# %env CLEARML_API_SECRET_KEY=ERuc1S6o5SirQGugvYXDFjH9b9aNi0u8S3rpALzXMa8YPSLDMW\n",
        "\n",
        "# clearml win\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=Z4YDBU13VPOFHBHF8667\n",
        "%env CLEARML_API_SECRET_KEY=JMzvRXn76AT83WuFJfS0FBGCY8c5TccbH5XboTYztrWqwzDdyn\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "#Clear ML Initialization\n",
        "cl_task = Task.init(project_name=project_name,task_name=task_name)\n",
        "logger = cl_task.get_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train() #Start the training process\n",
        "cl_task.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgcPBalGMB4d"
      },
      "source": [
        "# Inference & evaluation using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyyRL4soMDdE",
        "outputId": "6f4e43aa-e0e8-4fd4-8c0f-b073be34b2a1"
      },
      "outputs": [],
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00pZTVD_WaRQ"
      },
      "source": [
        "Verify segmentation on random validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomly select one image from validation dataset and visualize predictions\n",
        "for d in random.sample(val_dataset_dicts, 1):\n",
        "    im = cv2.imread(d[\"file_name\"])  # Read image in BGR format\n",
        "    im_rgb = im[:, :, ::-1]  # Convert BGR to RGB\n",
        "    outputs = predictor(im_rgb)  # Get predictions from the model\n",
        "    \n",
        "    # Create a Visualizer instance\n",
        "    v = Visualizer(im_rgb,  # Use RGB image\n",
        "                   metadata=val_metadata,\n",
        "                   scale=0.5,\n",
        "                   # instance_mode=ColorMode.IMAGE_BW  # Optionally remove colors of unsegmented pixels\n",
        "    )\n",
        "    \n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    \n",
        "    # Get the visualized image in RGB format\n",
        "    out_img = out.get_image()\n",
        "    \n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(out_img)\n",
        "    plt.axis('off')  # Hide the axes for better display\n",
        "    plt.show()\n",
        "    \n",
        "    # Save the output image\n",
        "    original_file_name = os.path.basename(d[\"file_name\"])\n",
        "    new_file_name = f\"test_{original_file_name}\"\n",
        "    output_file_path = os.path.join(cfg.OUTPUT_DIR, new_file_name)\n",
        "    plt.imsave(output_file_path, out_img)  # Save as RGB format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_inference_on_custom_image(image_path, output_dir, predictor, val_metadata):\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    \n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=val_metadata,\n",
        "                   scale=0.5,\n",
        "                   instance_mode=ColorMode.IMAGE_BW  \n",
        "    )\n",
        "    \n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    \n",
        "    plt.imshow(out.get_image()[:, :, ::-1])\n",
        "    plt.axis('off')\n",
        "    \n",
        "    original_file_name = os.path.basename(image_path)\n",
        "    new_file_name = f\"test_{original_file_name}\"\n",
        "    output_file_path = os.path.join(output_dir, new_file_name)\n",
        "    plt.savefig(output_file_path, bbox_inches='tight', pad_inches=0)\n",
        "    # plt.close() \n",
        "\n",
        "custom_image_path = \"/mnt/data/backup_pan/Dataset/Seg-dominik-iphone/Dominik-seg.v4i.coco-segmentation/test/Model_3_0020_jpeg.rf.b1ada19a12423f765881f57d7f2223d6.jpg\" \n",
        "output_dir = cfg.OUTPUT_DIR\n",
        "run_inference_on_custom_image(custom_image_path, output_dir, predictor, val_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on mac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F26poTkLPQzk"
      },
      "source": [
        "Check average precision and recall. (Need more validation data than just 2 images with handful of annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNSml38OuWV",
        "outputId": "ce72de09-f9d9-4257-dc2a-a60bc149316f"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wood_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"wood_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEDbRoL3Wytv"
      },
      "source": [
        "**Load a new image and segment it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "3s8VsDr-T-km",
        "outputId": "59bfdde9-d72b-4af0-d70d-5d503a075976"
      },
      "outputs": [],
      "source": [
        "new_im = cv2.imread(r\"B:\\01_Study\\Uni-Bamberg\\Work\\Holzprojects\\Datasets_local\\03_Domini_ip13\\Dominik-seg.v4i.coco-segmentation\\test\\Model_5_0016_jpeg.rf.04622e1c6175ec13b0be7f653916a02d.jpg\")\n",
        "outputs  = predictor(new_im)\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(new_im[:, :, ::-1], metadata=train_metadata)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Ph5_JgXFJu"
      },
      "source": [
        "**Process multiple images in a directory and save the results in an output directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSycv0yeT7IM",
        "outputId": "8bd2ed9e-59b1-4321-d55f-02b50c483003"
      },
      "outputs": [],
      "source": [
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the segmented images will be saved\n",
        "output_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results\"  # Replace this with the path to your desired output directory\n",
        "\n",
        "# Loop over the images in the input folder\n",
        "for image_filename in os.listdir(input_images_directory):\n",
        "    image_path = os.path.join(input_images_directory, image_filename)\n",
        "    new_im = cv2.imread(image_path)\n",
        "\n",
        "    # Perform prediction on the new image\n",
        "    outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "    # We can use `Visualizer` to draw the predictions on the image.\n",
        "    v = Visualizer(new_im[:, :, ::-1], metadata=train_metadata)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    # Create the output filename with _result extension\n",
        "    result_filename = os.path.splitext(image_filename)[0] + \"_result.png\"\n",
        "    output_path = os.path.join(output_directory, result_filename)\n",
        "\n",
        "    # Save the segmented image\n",
        "    cv2.imwrite(output_path, out.get_image()[:, :, ::-1])\n",
        "\n",
        "print(\"Segmentation of all images completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoouGMEDZr24"
      },
      "source": [
        "\n",
        "**Segment images and save object level information into a csv file.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_dnA1u6Zaz8",
        "outputId": "c030ebba-709d-4b69-d1c0-40496ea3f4fe"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from skimage.measure import regionprops, label\n",
        "\n",
        "\n",
        "# Assuming you have already defined the 'predictor' object and loaded the model.\n",
        "# Also, make sure 'metadata' is defined appropriately.\n",
        "\n",
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the CSV file will be saved\n",
        "output_csv_path = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results/output_objects.csv\"  # Replace this with the path to your desired output CSV file\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row in the CSV file\n",
        "    csvwriter.writerow([\"File Name\", \"Class Name\", \"Object Number\", \"Area\", \"Centroid\", \"BoundingBox\"])  # Add more columns as needed for other properties\n",
        "\n",
        "    # Loop over the images in the input folder\n",
        "    for image_filename in os.listdir(input_images_directory):\n",
        "        image_path = os.path.join(input_images_directory, image_filename)\n",
        "        new_im = cv2.imread(image_path)\n",
        "\n",
        "        # Perform prediction on the new image\n",
        "        outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "        # Convert the predicted mask to a binary mask\n",
        "        mask = outputs[\"instances\"].pred_masks.to(\"cpu\").numpy().astype(bool)\n",
        "\n",
        "        # Get the predicted class labels\n",
        "        class_labels = outputs[\"instances\"].pred_classes.to(\"cpu\").numpy()\n",
        "\n",
        "        # Debugging: print class_labels and metadata.thing_classes\n",
        "        #print(\"Class Labels:\", class_labels)\n",
        "        #print(\"Thing Classes:\", train_metadata.thing_classes)\n",
        "\n",
        "        # Use skimage.measure.regionprops to calculate object parameters\n",
        "        labeled_mask = label(mask)\n",
        "        props = regionprops(labeled_mask)\n",
        "\n",
        "        # Write the object-level information to the CSV file\n",
        "        for i, prop in enumerate(props):\n",
        "            object_number = i + 1  # Object number starts from 1\n",
        "            area = prop.area\n",
        "            centroid = prop.centroid\n",
        "            bounding_box = prop.bbox\n",
        "\n",
        "            # Check if the corresponding class label exists\n",
        "            if i < len(class_labels):\n",
        "                class_label = class_labels[i]\n",
        "                class_name = train_metadata.thing_classes[class_label]\n",
        "            else:\n",
        "                # If class label is not available (should not happen), use 'Unknown' as class name\n",
        "                class_name = 'Unknown'\n",
        "\n",
        "            # Write the object-level information to the CSV file\n",
        "            csvwriter.writerow([image_filename, class_name, object_number, area, centroid, bounding_box])  # Add more columns as needed for other properties\n",
        "\n",
        "print(\"Object-level information saved to CSV file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-RtuyQ42Rv"
      },
      "source": [
        "**Generate plots to understand the objects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h30LGnTfeIy8",
        "outputId": "c312a8a7-3bb2-4673-8055-0fb753251e66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the CSV file containing the object-level information\n",
        "csv_file_path = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results/output_objects.csv\"  # Update with your CSV file path\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Get class names from train_metadata.thing_classes\n",
        "class_names = train_metadata.thing_classes\n",
        "\n",
        "# Group the data by both \"File Name\" and \"Class Name\" and calculate the average number of objects per image for each class\n",
        "# first group the data by both \"File Name\" and \"Class Name\" and count the number of objects within each group.\n",
        "#Then, group the data by \"Class Name\" only and calculate the mean of the counts, which gives us the average number of objects per image for each class.\n",
        "avg_objects_per_class = df.groupby([\"File Name\", \"Class Name\"])[\"Object Number\"].count().reset_index()\n",
        "avg_objects_per_class = avg_objects_per_class.groupby(\"Class Name\")[\"Object Number\"].mean().reset_index()\n",
        "\n",
        "# Plot: Average number of objects per image for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"Class Name\", y=\"Object Number\", data=avg_objects_per_class, ci=None, order=class_names)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class Name\")\n",
        "plt.ylabel(\"Average Number of Objects per Image\")\n",
        "plt.title(\"Average Number of Objects per Image for Each Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Group the data by class and calculate the average area of objects for each class\n",
        "avg_area_per_class = df.groupby(\"Class Name\")[\"Area\"].mean().reset_index()\n",
        "\n",
        "# Plot: Average area of objects for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"Class Name\", y=\"Area\", data=avg_area_per_class, ci=None, order=class_names)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class Name\")\n",
        "plt.ylabel(\"Average Area of Objects\")\n",
        "plt.title(\"Average Area of Objects for Each Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3SJXvy8idl6"
      },
      "source": [
        "**Saving binary (actually multinary) images for each class for further processing.** Here, for each input image we will save n images corresponding to the number of classes. In our example, we will save 4 images for each image corresponding to the 4 classes. Each of these images will contain objects numbered 1, 2, 3, etc. - basically instance segmentation like images. These images can be used for further downstream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEayWBdnnku6",
        "outputId": "b9becf93-a66e-46df-ca09-07fcad157412"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# Directory path to the input images folder\n",
        "input_images_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test\"\n",
        "\n",
        "# Output directory where the segmented images will be saved\n",
        "output_directory = \"/content/drive/MyDrive/ColabNotebooks/data/3D-EM-Platelet/test_results_instance\"  # Replace this with the path to your desired output directory\n",
        "\n",
        "# Loop over the images in the input folder\n",
        "for image_filename in os.listdir(input_images_directory):\n",
        "    image_path = os.path.join(input_images_directory, image_filename)\n",
        "    new_im = cv2.imread(image_path)\n",
        "\n",
        "    # Perform prediction on the new image\n",
        "    outputs = predictor(new_im)  # Format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "\n",
        "    # Create a dictionary to store the mask for each class with unique integer labels\n",
        "    class_masks = {class_name: torch.zeros_like(outputs[\"instances\"].pred_masks[0], dtype=torch.uint8, device=torch.device(\"cuda:0\"))\n",
        "                   for class_name in train_metadata.thing_classes}\n",
        "\n",
        "    # Assign a unique integer label to each object in the mask\n",
        "    for i, pred_class in enumerate(outputs[\"instances\"].pred_classes):\n",
        "        class_name = train_metadata.thing_classes[pred_class]\n",
        "        class_masks[class_name] = torch.where(outputs[\"instances\"].pred_masks[i].to(device=torch.device(\"cuda:0\")),\n",
        "                                              i + 1,\n",
        "                                              class_masks[class_name])\n",
        "\n",
        "    # Save the masks for each class with unique integer labels\n",
        "    for class_name, class_mask in class_masks.items():\n",
        "        # Convert the tensor to a NumPy array and then to a regular (CPU) array\n",
        "        class_mask_np = class_mask.cpu().numpy()\n",
        "\n",
        "        # Create the output filename with _class_name_result.png extension\n",
        "        class_filename = os.path.splitext(image_filename)[0] + f\"_{class_name}_result.png\"\n",
        "        class_output_path = os.path.join(output_directory, class_filename)\n",
        "\n",
        "        # Save the image with unique integer labels\n",
        "        cv2.imwrite(class_output_path, class_mask_np.astype(np.uint8))\n",
        "\n",
        "print(\"Segmentation of all images completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8oBbRc7Xksw"
      },
      "source": [
        "# END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yZLzRZt1Vf8"
      },
      "source": [
        "**Interested in panoptic segmentation?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "eOOs4M1z1pXp",
        "outputId": "5c4dfc91-19c9-4b87-85fe-40d37e011204"
      },
      "outputs": [],
      "source": [
        "my_new_image = cv2.imread(\"/content/drive/MyDrive/ColabNotebooks/data/street_small.jpg\")\n",
        "cv2_imshow(my_new_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "tDAgk8fG21sP",
        "outputId": "d907761b-02d3-46ea-d97c-2f1c53bd025b"
      },
      "outputs": [],
      "source": [
        "# Inference with instance segmentation\n",
        "cfg_inst = get_cfg()\n",
        "cfg_inst.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg_inst.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo.  https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
        "cfg_inst.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg_inst)\n",
        "outputs = predictor(new_im)\n",
        "\n",
        "v = Visualizer(new_im[:, :, ::-1], MetadataCatalog.get(cfg_inst.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoTJ4VYx3zfn"
      },
      "source": [
        "**Panoptic segmentation = Instance segmentation + Semantic Segmentation**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "KBBjV3t01avh",
        "outputId": "f3f9411b-c772-4d2d-b82f-2623cbbd5e11"
      },
      "outputs": [],
      "source": [
        "# Inference with a panoptic segmentation model\n",
        "cfg_pan = get_cfg()\n",
        "cfg_pan.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg_pan.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg_pan)\n",
        "panoptic_seg, segments_info = predictor(new_im)[\"panoptic_seg\"]\n",
        "v = Visualizer(new_im[:, :, ::-1], MetadataCatalog.get(cfg_pan.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPhrr2e71uZZXW4xJXEF5QT",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "16uxXOT0U8KYhI02okV_sWhjgOy6Mbktv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
